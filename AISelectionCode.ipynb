{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55-KN8MU9NMl"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF\n",
        "\n",
        "# Install chromium and chromium-driver\n",
        "apt-get update\n",
        "apt-get install chromium chromium-driver\n",
        "\n",
        "# Install selenium\n",
        "pip install selenium\n",
        "!pip install fake_useragent\n",
        "from fake_useragent import UserAgent\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import selenium.webdriver.support.ui as ui\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzyHt5FA8HNN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyk1hUPq9etX"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "!pip install retry\n",
        "!pip install newspaper3k\n",
        "!pip install openai\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"key here\",timeout=10.0)\n",
        "#from newspaper import Article\n",
        "import nltk\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4.element import PageElement\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import os\n",
        "import openai\n",
        "openai.api_key = \"key here\"\n",
        "#openai.Model.list()\n",
        "import time\n",
        "import sys\n",
        "import argparse\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cd4V4DwWWRH"
      },
      "outputs": [],
      "source": [
        "samkey = \"key here\"\n",
        "melkey = \"key here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T6cM8GWDl1Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi3uDPsaU97y"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Install dependencies\n",
        "!apt-get update\n",
        "!apt install -y unzip xvfb libxi6 libgconf-2-4\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYp59_etTeQ"
      },
      "source": [
        "# Malay Mail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKzhCQk8n3sm"
      },
      "outputs": [],
      "source": [
        "urllist = []\n",
        "page = 0\n",
        "#malaymailcode\n",
        "# iterate through page numbers\n",
        "while True:\n",
        "    page += 1\n",
        "    print(page)\n",
        "    url = 'https://www.malaymail.com/search?query=malaysia%20refugee&pgno='\n",
        "    source = 'malaymail'\n",
        "    pageurl = url + str(page)\n",
        "    r = requests.get(pageurl)\n",
        "    r_soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    articles = r_soup.find_all('div',{'class':\"col-md-3 article-item\"})\n",
        "    print(len(articles),'num articles')\n",
        "    # check if there are articles on this page\n",
        "    if not articles:\n",
        "        break\n",
        "\n",
        "    for article in articles:\n",
        "        print(article)\n",
        "        date = article.find('span', {'class': 'article-date'}).text\n",
        "        print(date)\n",
        "        if int(date.split(' ')[-2]) > 2016:\n",
        "          aurl = article.find('h2', {'class': 'article-title'}).a['href']\n",
        "          print(aurl)\n",
        "          urllist.append(aurl)\n",
        "        else:\n",
        "            break\n",
        "with open('/content/drive/MyDrive/malaymailurls.txt', 'w') as file:\n",
        "    for url in urllist:\n",
        "        file.write(url + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45KtgiA-RkzO"
      },
      "source": [
        "# The Sun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saDfdqJRHHFr"
      },
      "outputs": [],
      "source": [
        "urllist = []\n",
        "page = 1\n",
        "#malaymailcode\n",
        "# iterate through page numbers\n",
        "while True:\n",
        "    print(page)\n",
        "    url = 'https://www.thesundaily.my/search-result/-/search/%22malaysia%22%20%22refugees%22/false/true/20170101/20230406/date/true/true/0/0/meta/0/0/0/'\n",
        "    source = 'The Sun'\n",
        "    pageurl = url + str(page)\n",
        "    r = requests.get(pageurl)\n",
        "    r_soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    articles = r_soup.find_all('li', {'class': \"element full-access norestricted\"})\n",
        "    print(len(articles), 'num articles')\n",
        "\n",
        "    # check if there are articles on this page\n",
        "    if not articles:\n",
        "        break\n",
        "\n",
        "    for article in articles:\n",
        "        print(article)\n",
        "        aurl = 'https://www.thesundaily.my' + article.find('div', {'class': 'headline'}).a['href']\n",
        "        print(aurl)\n",
        "        urllist.append(aurl)\n",
        "\n",
        "    page += 1\n",
        "\n",
        "with open('/content/drive/MyDrive/TheSunurls.txt', 'w') as file:\n",
        "    for url in urllist:\n",
        "        file.write(url + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMdQHXAwMBLk"
      },
      "outputs": [],
      "source": [
        "urllist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rco5gx7iFDK0"
      },
      "outputs": [],
      "source": [
        "stem =\n",
        "num = 1\n",
        "url = stem + str(num)\n",
        "\n",
        "r = requests.get(url)\n",
        "r_soup = BeautifulSoup(r.text, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMVjVFZxHYpR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcOqUF5kK_DQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E5jsmaQNgSc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMO_zl5QO6bn"
      },
      "outputs": [],
      "source": [
        "url_to_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCVuAuWveUva"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFevcAw7qxIG"
      },
      "outputs": [],
      "source": [
        "master_df.loc[master_df['source'] == 'malaymail', 'url'] = master_df[master_df['source'] == 'malaymail'].merge(df, on='title', how='left')[['url_y','title']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9u5VoZvsCMU"
      },
      "outputs": [],
      "source": [
        "master_df.loc[master_df['source'] == 'malaymail', 'url']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZRbkqwSrJ4p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjpCEH1yqz_p"
      },
      "outputs": [],
      "source": [
        "master_df.loc[:, ['url','title']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X52SdsNIoxQj"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waX7zi5ioR0-"
      },
      "outputs": [],
      "source": [
        "master_df[master_df.title.str.contains('ld release the 269 Rohingya refugees arrested and detained')][['text','fulltext']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEkj3M-ThZhW"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfJouXXXpz05"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEE3cmySqP49"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(master_df, df[['title', 'url']], on='title', how='left')\n",
        "\n",
        "# If you want to replace the 'url' column in the master_df with the new URLs\n",
        "merged_df['url_x'] = merged_df['url_x'].fillna(merged_df['url_y'])\n",
        "merged_df = merged_df.drop(columns='url_y')\n",
        "\n",
        "# Rename the columns to their original names\n",
        "merged_df = merged_df.rename(columns={'url_x': 'url'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dafiparrhZbx"
      },
      "outputs": [],
      "source": [
        "unassigned_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbqLS21Jhmyb"
      },
      "outputs": [],
      "source": [
        "unassigned_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHVDKpehGzpg"
      },
      "outputs": [],
      "source": [
        "urllist=pd.read_csv('/content/drive/MyDrive/TheSunurls.txt',header=None)\n",
        "#urllist = urllist.sample(frac=0.05, random_state=42).reset_index()\n",
        "urllist=list(urllist[0])\n",
        "urllist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rM5x3xT1Xmg"
      },
      "outputs": [],
      "source": [
        "urllist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkNwunQ41SXb"
      },
      "outputs": [],
      "source": [
        "for url in [x for x in urllist if x not in scraped_urls]:\n",
        "  print(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik19Nyx8eslS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from newspaper import Article, ArticleException\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "columns = ['url', 'title', 'date', 'text', 'keywords', 'source']\n",
        "source = 'The Sun'\n",
        "\n",
        "# Check if the partial file exists and load its data into a DataFrame\n",
        "if os.path.isfile('/content/drive/MyDrive/sun_partial.csv'):\n",
        "    partial_df = pd.read_csv('/content/drive/MyDrive/sun_partial.csv')\n",
        "    # Get a list of URLs that have already been scraped\n",
        "    scraped_urls = set(partial_df['url'].tolist())\n",
        "else:\n",
        "    partial_df = pd.DataFrame(columns=columns)\n",
        "    scraped_urls = set()\n",
        "\n",
        "rows = []  # list to hold rows of scraped data\n",
        "\n",
        "# Iterate through the list of URLs, skipping those that have already been scraped\n",
        "for url in [x for x in urllist if x not in scraped_urls]:\n",
        "    print(url)\n",
        "    try:\n",
        "        article = Article(url, request_timeout=15)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        title = article.title\n",
        "        soup = BeautifulSoup(article.html, 'html.parser')\n",
        "        # Search for the li element containing the date\n",
        "        try:\n",
        "          date_element = soup.find('li', {'class': 'date', 'itemprop': 'datePublished'})\n",
        "          date_str = date_element.text.strip()\n",
        "          date = datetime.strptime(date_str, \"%d-%m- %Y %I:%M %p\")\n",
        "          print(date)\n",
        "        except:\n",
        "          date_element = soup.find('div', {'class': 'datefrom'})\n",
        "          date_str = date_element.text.strip()\n",
        "          date = datetime.strptime(date_str, \"%d %b %Y / %H:%M H.\")\n",
        "          print(date)\n",
        "        text = article.text\n",
        "        article.nlp()\n",
        "        keywords = article.keywords\n",
        "        print(title)\n",
        "        # append the scraped data to the rows list\n",
        "        rows.append({'url': url, 'title': title, 'date': date, 'text': text, 'keywords': keywords,'source': source})\n",
        "    except (ArticleException, requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n",
        "        print(f\"An error occurred while scraping {url}: {e}. Saving the scraped data to a CSV file and continuing...\")\n",
        "        # create a DataFrame from the rows list and save it to a CSV file\n",
        "        partial_df = pd.DataFrame(rows)\n",
        "        partial_df.to_csv('/content/drive/MyDrive/sun_partial.csv', index=False)\n",
        "        continue\n",
        "\n",
        "# create a DataFrame from the rows list and split the text column into multiple rows\n",
        "articles = pd.concat([partial_df, pd.DataFrame(rows)])\n",
        "articles['fulltext']=articles['text']\n",
        "articles2 = articles['text'].str.split('\\n', expand=True).stack().reset_index(level=1, drop=True).to_frame('text')\n",
        "articles2 = articles.drop('text', axis=1).join(articles2).reset_index(drop=True)\n",
        "articles2=articles2[articles2['text']!='']\n",
        "\n",
        "# save the DataFrame to a CSV file\n",
        "articles2.to_csv('/content/drive/MyDrive/Sun.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bPWaCKLfJlV"
      },
      "outputs": [],
      "source": [
        "articles2 = pd.read_csv('/content/drive/MyDrive/Sun.csv')\n",
        "articles2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rb4Ajx4UFqp"
      },
      "source": [
        "# The Malaysian Insight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_2UV_wZIyHU"
      },
      "outputs": [],
      "source": [
        "'https://www.themalaysianinsight.com/t#gsc.tab=0&gsc.q=malaysia%20refugees%20&gsc.sort=date&gsc.page=1' #first search page\n",
        "<div class=\"gsc-webResult gsc-result\"><div class=\"gs-webResult gs-result\"><div class=\"gsc-thumbnail-inside\"><div class=\"gs-title\"><a class=\"gs-title\" href=\"https://www.google.com/url?client=internal-element-cse&amp;cx=017825756535751684928:zunkozct1m4&amp;q=https://www.themalaysianinsight.com/s/443086&amp;sa=U&amp;ved=2ahUKEwjM9-HLw_7-AhU2TWwGHUUvDdEQFnoECAMQAg&amp;usg=AOvVaw3bVIirGBkVpUQBV86Sfugg\" target=\"_self\" dir=\"ltr\" data-cturl=\"https://www.google.com/url?client=internal-element-cse&amp;cx=017825756535751684928:zunkozct1m4&amp;q=https://www.themalaysianinsight.com/s/443086&amp;sa=U&amp;ved=2ahUKEwjM9-HLw_7-AhU2TWwGHUUvDdEQFnoECAMQAg&amp;usg=AOvVaw3bVIirGBkVpUQBV86Sfugg\" data-ctorig=\"https://www.themalaysianinsight.com/s/443086\">UN says Sudan, <b>refugees</b> need over US$3 billion in aid | The ...</a></div></div><div class=\"gsc-url-top\"><div class=\"gs-bidi-start-align gs-visibleUrl gs-visibleUrl-short\" dir=\"ltr\">www.themalaysianinsight.com</div><div class=\"gs-bidi-start-align gs-visibleUrl gs-visibleUrl-long\" dir=\"ltr\" style=\"word-break:break-all;\">https://www.the<b>malaysian</b>insight.com/s/443086</div><div class=\"gs-bidi-start-align gs-visibleUrl gs-visibleUrl-breadcrumb\"><span>www.themalaysianinsight.com</span><span> › ...</span></div></div><div class=\"gsc-table-result\"><div class=\"gsc-table-cell-snippet-close\"><div class=\"gs-title gsc-table-cell-thumbnail gsc-thumbnail-left\"><a class=\"gs-title\" href=\"https://www.themalaysianinsight.com/s/443086\" target=\"_self\" dir=\"ltr\" data-cturl=\"https://www.google.com/url?client=internal-element-cse&amp;cx=017825756535751684928:zunkozct1m4&amp;q=https://www.themalaysianinsight.com/s/443086&amp;sa=U&amp;ved=2ahUKEwjM9-HLw_7-AhU2TWwGHUUvDdEQFnoECAMQAg&amp;usg=AOvVaw3bVIirGBkVpUQBV86Sfugg\" data-ctorig=\"https://www.themalaysianinsight.com/s/443086\">UN says Sudan, <b>refugees</b> need over US$3 billion in aid | The ...</a></div><div><span></span></div><div class=\"gs-bidi-start-align gs-snippet\" dir=\"ltr\">22 hours ago <b>...</b> 2023 The <b>Malaysian</b> Insight. All rights reserved. ×. The <b>Malaysian</b> Insight. The <b>Malaysian</b> Insight. FREE - In Google Play.</div><div class=\"gsc-url-bottom\"><div class=\"gs-bidi-start-align gs-visibleUrl gs-visibleUrl-short\" dir=\"ltr\">www.themalaysianinsight.com</div><div class=\"gs-bidi-start-align gs-visibleUrl gs-visibleUrl-long\" dir=\"ltr\" style=\"word-break:break-all;\">https://www.the<b>malaysian</b>insight.com/s/443086</div></div><div class=\"gs-richsnippet-box\" style=\"display: none;\"></div><div class=\"gs-per-result-labels\" url=\"https://www.themalaysianinsight.com/s/443086\"></div></div></div></div><div class=\"gs-watermark\"><a href=\"http://code.google.com/apis/ajaxsearch/faq.html\" class=\"gs-watermark\" target=\"_blank\">clipped from Google - 5/2023</a></div></div> # HTML context example\n",
        "\n",
        "<a class=\"gs-title\" href=\"https://www.google.com/url?client=internal-element-cse&amp;cx=017825756535751684928:zunkozct1m4&amp;q=https://www.themalaysianinsight.com/s/443086&amp;sa=U&amp;ved=2ahUKEwjul8-sxP7-AhX2S2wGHZAqAHMQFnoECAMQAg&amp;usg=AOvVaw0lPfz6lPViewK7-0boI4Ka\" target=\"_self\" dir=\"ltr\" data-cturl=\"https://www.google.com/url?client=internal-element-cse&amp;cx=017825756535751684928:zunkozct1m4&amp;q=https://www.themalaysianinsight.com/s/443086&amp;sa=U&amp;ved=2ahUKEwjul8-sxP7-AhX2S2wGHZAqAHMQFnoECAMQAg&amp;usg=AOvVaw0lPfz6lPViewK7-0boI4Ka\" data-ctorig=\"https://www.themalaysianinsight.com/s/443086\">UN says Sudan, <b>refugees</b> need over US$3 billion in aid | The ...</a> # HTML element example\n",
        "https://www.themalaysianinsight.com/s/443086 # desired URL example\n",
        "#note: no \"next\" button on this site, iterate through page= values in URL instead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCeRd1-cvpYB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from time import sleep\n",
        "from selenium import webdriver\n",
        "from pyvirtualdisplay import Display\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Set up a virtual display\n",
        "display = Display(visible=0, size=(1280, 800))\n",
        "display.start()\n",
        "\n",
        "# Configure Chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Create an instance of the Chrome WebDriver\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "\n",
        "# Initialize page number\n",
        "page_number = 1\n",
        "\n",
        "dates = []\n",
        "urls = []\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        # Navigate to a website\n",
        "        cururl=f'https://www.themalaysianinsight.com/t#gsc.tab=0&gsc.q=malaysia%20refugees%20&gsc.sort=date&gsc.page={page_number}'\n",
        "        print(cururl)\n",
        "        driver.get(cururl)\n",
        "        sleep(1)\n",
        "        # Wait for the search results to load\n",
        "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".gsc-webResult.gsc-result\")))\n",
        "\n",
        "        # Scrape the link URL for each search result on the current page\n",
        "        search_results = driver.find_elements(By.CSS_SELECTOR, \".gsc-webResult.gsc-result\")\n",
        "\n",
        "        for result in search_results:\n",
        "            link_element = result.find_element(By.CSS_SELECTOR, \".gs-title a.gs-title\")\n",
        "            link_url = link_element.get_attribute(\"data-ctorig\")\n",
        "            print([link_url])\n",
        "            urls.append(link_url)\n",
        "\n",
        "        # Go to the next page by incrementing the page_number\n",
        "        driver.save_screenshot(f'screenshot_page_{page_number}.png')\n",
        "        page_number += 1\n",
        "\n",
        "    except:\n",
        "        # An exception will occur when no more pages are found, so break out of the loop\n",
        "        break\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()\n",
        "\n",
        "# Stop the virtual display\n",
        "display.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbsG-tpX3cSY"
      },
      "outputs": [],
      "source": [
        "urllist = list({url for url in urls if all(substring not in url for substring in ['page=','/a/', '/t/', '/c/', 'bahasa', 'chinese'])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z9LN0n23_QG"
      },
      "outputs": [],
      "source": [
        "urllist =pd.DataFrame({'URL':urllist})\n",
        "urllist.to_csv(\"/content/drive/MyDrive/TMIurls.csv\")\n",
        "urllist = list({url for url in urls if all(substring not in url for substring in ['page=','/a/', '/t/', '/c/', 'bahasa', 'chinese'])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNMo8rYSjL6_"
      },
      "outputs": [],
      "source": [
        "urllist=pd.read_csv(\"/content/drive/MyDrive/TMIurls.csv\")['URL']\n",
        "urllist = list({url for url in urllist if all(substring not in url for substring in ['page=', '/a/','/t/', '/c/', 'bahasa', 'chinese'])})\n",
        "urllist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEU6maVCjacI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Set up a virtual display\n",
        "display = Display(visible=0, size=(1280, 800))\n",
        "display.start()\n",
        "\n",
        "# Configure Chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Create an instance of the Chrome WebDriver\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "\n",
        "# Log in\n",
        "driver.get(\"https://www.themalaysianinsight.com/login\")\n",
        "try:\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"email\"))).send_keys(\"samf1986@gmail.com\")\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"password\"))).send_keys(\"NqudNid@2AXEwq\")\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//input[@value='Log In']\"))).click()\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    driver.quit()\n",
        "time.sleep(5)  # adjust as necessary\n",
        "try:\n",
        "    # Wait up to 10 seconds for the page to load\n",
        "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    driver.quit()\n",
        "\n",
        "html = driver.page_source\n",
        "article = newspaper.Article(url = ' ')\n",
        "article.set_html(html)\n",
        "# Close the browser\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9Xg4XKZoQwO"
      },
      "outputs": [],
      "source": [
        "article = Article(url, request_timeout=15)\n",
        "article.download()\n",
        "article.parse()\n",
        "title = article.title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmKc_zsvmEwQ"
      },
      "outputs": [],
      "source": [
        "driver.get(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdzOLQE7oZwc"
      },
      "outputs": [],
      "source": [
        "import newspaper\n",
        "#with open(\"file.html\", 'rb') as fh:\n",
        "#    ht = fh.read()\n",
        "\n",
        "\n",
        "article.parse()\n",
        "article.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keDKnBrEjos2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from pyvirtualdisplay import Display\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Set up a virtual display\n",
        "display = Display(visible=0, size=(1280, 800))\n",
        "display.start()\n",
        "\n",
        "# Configure Chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Create an instance of the Chrome WebDriver\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "\n",
        "# Navigate to a website\n",
        "driver.get('https://www.malaysiakini.com/en/search?keywords=malaysia+refugees&category=columns&startDate=2017-01-01&endDate=&sort=desc&page=0')\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6VoTWEiNvk2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from selenium import webdriver\n",
        "from pyvirtualdisplay import Display\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from retry import retry\n",
        "import requests\n",
        "from newspaper import Article, ArticleException\n",
        "# Configure Chrome options\n",
        "driver.quit()\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Create an instance of the Chrome WebDriver\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "# Log in\n",
        "driver.get(\"https://www.themalaysianinsight.com/login\")\n",
        "try:\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"email\"))).send_keys(\"samf1986@gmail.com\")\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"password\"))).send_keys(\"NqudNid@2AXEwq\")\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//input[@value='Log In']\"))).click()\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    driver.quit()\n",
        "time.sleep(5)  # adjust as necessary\n",
        "\n",
        "columns = ['url', 'title', 'date', 'text', 'keywords', 'source']\n",
        "articles = pd.DataFrame(columns=columns)\n",
        "source = 'The Malaysian Insight'\n",
        "\n",
        "rows = []  # list to hold rows of scraped data\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def gethtml(url):\n",
        "    driver.get(url)\n",
        "    WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "    return driver.page_source\n",
        "for url in urllist:\n",
        "    print(url)\n",
        "    try:\n",
        "        article = Article(url = ' ')\n",
        "        article.set_html(gethtml(url))\n",
        "        article.parse()\n",
        "        title = article.title\n",
        "        soup = BeautifulSoup(article.html, 'html.parser')\n",
        "        # Search for the li element containing the date\n",
        "        date_element = soup.find('span', {'class': 'byline-time'})\n",
        "        try:\n",
        "          date_text = date_element.get_text(strip=True)\n",
        "          date = date_text.split(\"Published on \")[1].split(\" ·\")[0]\n",
        "          #date = article.publish_date\n",
        "          print(date)\n",
        "        except:\n",
        "          print('no date')\n",
        "        text = article.text\n",
        "        article.nlp()\n",
        "        keywords = article.keywords\n",
        "        print(title)\n",
        "        # append the scraped data to the rows list\n",
        "        rows.append({'url': url, 'title': title, 'date': date, 'text': text, 'keywords': keywords, 'source': source})\n",
        "    except (ArticleException, requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n",
        "        print(f\"An error occurred while scraping {url}: {e}. Saving the scraped data to a CSV file and continuing...\")\n",
        "        # create a DataFrame from the rows list and save it to a CSV file\n",
        "        articles = pd.DataFrame(rows)\n",
        "        articles.to_csv('/content/drive/MyDriveTMIunfinished.csv', index=False)\n",
        "        driver.quit()\n",
        "        exit()\n",
        "driver.quit()\n",
        "# create a DataFrame from the rows list and split the text column into multiple rows\n",
        "articles = pd.DataFrame(rows)\n",
        "articles['fulltext']=articles['text']\n",
        "articles2 = articles['text'].str.split('\\n', expand=True).stack().reset_index(level=1, drop=True).to_frame('text')\n",
        "articles2 = articles.drop('text', axis=1).join(articles2).reset_index(drop=True)\n",
        "\n",
        "articles2['coding']=''\n",
        "articles2 = articles2[articles2.text.str.len()>50]\n",
        "articles2=articles2[articles2.text!=''].reset_index()\n",
        "# save the DataFrame to a CSV file\n",
        "articles2.to_csv('/content/drive/MyDrive/TMI.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ejUsS4Isz_"
      },
      "source": [
        "# Malaysiakini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P0mYMwuIOYf"
      },
      "outputs": [],
      "source": [
        "News and Columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5vQJ6c7RozE"
      },
      "outputs": [],
      "source": [
        "https://www.malaysiakini.com/en/search?keywords=malaysia+refugees&category=news&startDate=2017-01-01&endDate=&sort=desc&page=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkCVdn5uIbDj"
      },
      "outputs": [],
      "source": [
        "https://www.malaysiakini.com/en/search?keywords=malaysia+refugees&category=columns&startDate=2017-01-01&endDate=&sort=desc&page=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87e5vO5LIoyW"
      },
      "outputs": [],
      "source": [
        "<div class=\"cursor-pointer overflow-hidden shadow-lg text-coolGray-600 group relative h-full mb-4\"><a href=\"/columns/664418\"><div class=\"jsx-2653088069 w-full landscape-padding relative bg-coolGray-100 print-screen\"><span style=\"box-sizing: border-box; display: block; overflow: hidden; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px; position: absolute; inset: 0px;\"><img alt=\"story image\" src=\"https://i.ncdn.xyz/publisher-c1a3f893382d2b2f8a9aa22a654d9c97/2021/11/cccc918aff448a01340d907ddaa7ebfd.jpg=s500\" decoding=\"async\" data-nimg=\"fill\" class=\"w-full h-full print-screen\" style=\"position: absolute; inset: 0px; box-sizing: border-box; padding: 0px; border: none; margin: auto; display: block; width: 0px; height: 0px; min-width: 100%; max-width: 100%; min-height: 100%; max-height: 100%; object-fit: cover; object-position: center center;\"></span></div><div class=\"px-4 pt-4 flex flex-col pb-4\"><div class=\"text-xl leading-tight group-hover:text-orange-500 transition duration-300 pt-2 font-semibold\">ADUN SPEAKS | Govt taking right step in recognising UNHCR</div><div class=\"mt-2 text-sm mb-8 text-coolGray-600 opacity-70\">This will assist the govt in managing refugees in meaningful ways.</div><div class=\"flex flex-wrap items-center text-xs leading-none opacity-70 absolute bottom-0 left-0 right-0 px-4 py-4\"><div class=\"font-medium flex items-center\"><div>P Ramasamy</div><div><div class=\"text-lg leading-none mx-1\">⋅</div></div></div><div class=\"flex items-center opacity-70\"><div class=\"flex items-center flex-1 whitespace-nowrap\"><div>09-05-2023</div></div><div class=\"text-lg leading-none mx-1\">⋅</div><div class=\"flex items-center\"><div class=\"mr-1\"><svg xmlns=\"http://www.w3.org/2000/svg\" fill=\"none\" viewBox=\"0 0 24 24\" stroke=\"currentColor\" class=\"w-3\"><path stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z\"></path></svg></div><div></div></div></div></div></div></a></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r50cNwGIzkB"
      },
      "outputs": [],
      "source": [
        "'https://www.malaysiakini.com/' + <a href=\"/columns/664418\">\n",
        "1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP9GEiDRjgZA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7tsXGN8ht9z"
      },
      "outputs": [],
      "source": [
        "# Set up and use Chrome WebDriver\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from pyvirtualdisplay import Display\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Set up a virtual display\n",
        "display = Display(visible=0, size=(1280, 800))\n",
        "display.start()\n",
        "\n",
        "# Configure Chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Create an instance of the Chrome WebDriver\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "\n",
        "# Navigate to a website\n",
        "driver.get('https://www.malaysiakini.com/en/search?keywords=malaysia+refugees&category=columns&startDate=2017-01-01&endDate=&sort=desc&page=0')\n",
        "\n",
        "dates = []\n",
        "urls = []\n",
        "base_url = ''\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        # Wait for the search results to load\n",
        "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".cursor-pointer.overflow-hidden.shadow-lg.text-coolGray-600.group.relative.h-full.mb-4\")))\n",
        "\n",
        "        # Scrape the date and link URL for each search result on the current page\n",
        "        search_results = driver.find_elements(By.CSS_SELECTOR, \".cursor-pointer.overflow-hidden.shadow-lg.text-coolGray-600.group.relative.h-full.mb-4\")\n",
        "\n",
        "        for result in search_results:\n",
        "            date = result.find_element(By.CSS_SELECTOR, \".flex.items-center.flex-1.whitespace-nowrap\").text\n",
        "            link_url = base_url + result.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
        "            print([date, link_url])\n",
        "            dates.append(date)\n",
        "            urls.append(link_url)\n",
        "\n",
        "        # Click the \"next page\" button and wait for the next page to load\n",
        "        next_page_button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//button[contains(., 'NEXT')]\")))\n",
        "        driver.execute_script(\"arguments[0].scrollIntoView();\", next_page_button)\n",
        "        next_page_button.click()\n",
        "        WebDriverWait(driver, 10).until(EC.staleness_of(search_results[0]))\n",
        "\n",
        "    except:\n",
        "        # No more \"next page\" button found, so break out of the loop\n",
        "        break\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()\n",
        "\n",
        "# Stop the virtual display\n",
        "display.stop()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM4YlW0ehypV"
      },
      "outputs": [],
      "source": [
        "urls2=urls.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM6EBTZGtdlb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGDHuYvEpPeM"
      },
      "outputs": [],
      "source": [
        "urllist =pd.DataFrame({'URL': urls2+urls})\n",
        "urllist.to_csv(\"/content/drive/MyDrive/kiniurls.csv\")\n",
        "urllist = urls2+urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7-HLr_06XwB"
      },
      "outputs": [],
      "source": [
        "urllist=pd.read_csv(\"/content/drive/MyDrive/kiniurls.csv\")\n",
        "urllist = urllist['URL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W2OmBtqRvzb"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from newspaper import Article, ArticleException\n",
        "import requests\n",
        "columns = ['url', 'title', 'date', 'text', 'keywords', 'source']\n",
        "source = 'The Star'\n",
        "article = Article('https://www.themalaysianinsight.com/s/443494')\n",
        "article.download()\n",
        "article.parse()\n",
        "title = article.title\n",
        "date = article.publish_date\n",
        "text = article.text\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BimpAFwF6o9C"
      },
      "outputs": [],
      "source": [
        "'https://membership.malaysiakini.com/auth/local?redirectUrl=https://www.malaysiakini.com/&flow=normal&lang=en' #login URL\n",
        "\n",
        "<form class=\"jsx-267149412\"><div class=\"jsx-544239716 input-icon-tooltip error-false\"><input type=\"text\" value=\"melati88@gmail.com\" placeholder=\"Email or username\" name=\"email\" required=\"\" autocorrect=\"off\" autocapitalize=\"off\" class=\"jsx-544239716 error-false\"></div><div class=\"jsx-544239716 input-icon-tooltip error-false\"><input type=\"password\" value=\"vegetable\" placeholder=\"Password\" name=\"password\" required=\"\" autocorrect=\"off\" autocapitalize=\"off\" class=\"jsx-544239716 error-false\"><div class=\"jsx-544239716 passwordToggle\"><img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj4KICA8cGF0aCBkPSJNOTkuNCA0Ny40Yy01LjYtOC4yLTEzLTE0LjgtMjEuNi0xOS4zTDg5IDE3LjQgODMuOCAxMiA3MC40IDI0LjljLTYuMi0yLjItMTIuOS0zLjQtMTkuNy0zLjVoLTJDMjkuMiAyMS45IDExLjMgMzEuNy42IDQ4Yy0uOCAxLjItLjggMi43IDAgMy45IDUuNiA4LjIgMTMgMTQuOCAyMS42IDE5LjJsLTEyIDExLjYgNS4yIDUuNCAxNC4zLTEzLjdjNi4xIDIuMiAxMi42IDMuNCAxOS4zIDMuNWgyLjdjMTkuMy0uNiAzNy4xLTEwLjUgNDcuNy0yNi43LjgtMS4xLjgtMi43IDAtMy44em0tMjguMiAyLjJjMCAxMS43LTkuNSAyMS4yLTIxLjIgMjEuMi00LjcgMC05LTEuNS0xMi41LTQuMWw3LTYuN2MxLjYuOSAzLjUgMS40IDUuNSAxLjQgNi41IDAgMTEuNy01LjIgMTEuNy0xMS43IDAtMS44LS40LTMuNS0xLjEtNWw3LTYuN2MyLjIgMy4yIDMuNiA3LjMgMy42IDExLjZ6bS00Mi40IDBjMC0xMS43IDkuNS0yMS4yIDIxLjItMjEuMiA0LjYgMCA4LjkgMS41IDEyLjQgNGwtNyA2LjdjLTEuNi0uOC0zLjQtMS4zLTUuNC0xLjMtNi41IDAtMTEuNyA1LjItMTEuNyAxMS43IDAgMS44LjQgMy40IDEuMSA0LjlsLTcgNi43Yy0yLjMtMy4yLTMuNi03LjItMy42LTExLjV6bS0yMSAuM2M0LjktNi44IDExLjQtMTIuMiAxOC43LTE1LjktMyA0LjUtNC44IDkuOS00LjggMTUuNyAwIDUuOSAxLjggMTEuNCA0LjkgMTUuOS03LjMtMy43LTEzLjgtOS0xOC44LTE1Ljd6bTY1LjcgMTUuNGMzLTQuNSA0LjgtOS45IDQuOC0xNS43IDAtNS45LTEuOC0xMS40LTQuOS0xNS45IDcuNCAzLjYgMTMuOSA4LjkgMTguOSAxNS43LTUgNi44LTExLjQgMTIuMi0xOC44IDE1Ljl6IiBmaWxsPSIjYmRiZGJkIi8+Cjwvc3ZnPg==\" width=\"20px\" class=\"jsx-544239716\"></div></div><div class=\"jsx-267149412 spacer-40\"></div><div class=\"jsx-1504941063 floating-action\"><div class=\"jsx-1504941063 label\"><span class=\"jsx-1504941063\">Sign In</span></div><div class=\"jsx-2309431947 floating-button\"><button type=\"submit\" class=\"jsx-2309431947\"><img src=\"data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDIyLjAuMSwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPgo8c3ZnIHZlcnNpb249IjEuMSIgaWQ9IkxheWVyXzEiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgeG1sbnM6eGxpbms9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkveGxpbmsiIHg9IjBweCIgeT0iMHB4IgoJIHZpZXdCb3g9IjAgMCAxMDAgMTAwIiBzdHlsZT0iZW5hYmxlLWJhY2tncm91bmQ6bmV3IDAgMCAxMDAgMTAwOyIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+CjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+Cgkuc3Qwe2ZpbGw6bm9uZTtzdHJva2U6I0ZGRkZGRjtzdHJva2Utd2lkdGg6ODtzdHJva2UtbWl0ZXJsaW1pdDoxMDt9Cjwvc3R5bGU+CjxnPgoJPGc+CgkJPGc+CgkJCTxwb2x5bGluZSBjbGFzcz0ic3QwIiBwb2ludHM9IjQwLDk1IDk0LjMsNTAgNDAsNSAJCQkiLz4KCQk8L2c+CgkJPGc+CgkJCTxsaW5lIGNsYXNzPSJzdDAiIHgxPSI5NC4zIiB5MT0iNTAiIHgyPSI1LjciIHkyPSI1MCIvPgoJCTwvZz4KCTwvZz4KPC9nPgo8L3N2Zz4K\" alt=\"next\" class=\"jsx-2309431947\"></button></div></div></form>#HTML for the box containing the username and password fields and \"next\" button"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Val13P77yof"
      },
      "outputs": [],
      "source": [
        "driver.quit()\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Create an instance of the Chrome WebDriver\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "login_url = 'https://membership.malaysiakini.com/auth/local?redirectUrl=https://www.malaysiakini.com/&flow=normal&lang=en'\n",
        "# Log in\n",
        "driver.get(login_url)\n",
        "\n",
        "try:\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.NAME, \"email\"))).send_keys(\"melati88@gmail.com\")\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.NAME, \"password\"))).send_keys(\"vegetable\")\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[@type='submit']\"))).click()\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print('failed')\n",
        "    driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjXmoMye9t1w"
      },
      "outputs": [],
      "source": [
        "\n",
        "url = 'https://www.malaysiakini.com/news/666048'\n",
        "article = Article(url = ' ')\n",
        "article.set_html(gethtml(url))\n",
        "article.parse()\n",
        "article.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdF9qUc1GGvL"
      },
      "outputs": [],
      "source": [
        "print(len(article.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p9HcU8LFGOf"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from pyvirtualdisplay import Display\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from retry import retry\n",
        "import requests\n",
        "from newspaper import Article, ArticleException\n",
        "# Configure Chrome options\n",
        "driver.quit()\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Create an instance of the Chrome WebDriver\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "login_url = 'https://membership.malaysiakini.com/auth/local?redirectUrl=https://www.malaysiakini.com/&flow=normal&lang=en'\n",
        "# Log in\n",
        "driver.get(login_url)\n",
        "\n",
        "try:\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.NAME, \"email\"))).send_keys(\"melati88@gmail.com\")\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.NAME, \"password\"))).send_keys(\"vegetable\")\n",
        "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[@type='submit']\"))).click()\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print('failed')\n",
        "    driver.quit()\n",
        "time.sleep(5)  # adjust as necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u3afv2hE6VGm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "columns = ['url', 'title', 'date', 'text', 'keywords', 'source']\n",
        "articles = pd.DataFrame(columns=columns)\n",
        "source = 'Malaysiakini'\n",
        "\n",
        "rows = []  # list to hold rows of scraped data\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def gethtml(url):\n",
        "    driver.get(url)\n",
        "    WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "    return driver.page_source\n",
        "for url in urllist:\n",
        "    print(url)\n",
        "    try:\n",
        "        article = Article(url = ' ')\n",
        "        article.set_html(gethtml(url))\n",
        "        article.parse()\n",
        "        title = article.title\n",
        "        date = article.publish_date\n",
        "        text = article.text\n",
        "        print('characters', len(text))\n",
        "        article.nlp()\n",
        "        keywords = article.keywords\n",
        "        print(title)\n",
        "        # append the scraped data to the rows list\n",
        "        rows.append({'url': url, 'title': title, 'date': date, 'text': text, 'keywords': keywords, 'source': source})\n",
        "    except (ArticleException, requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n",
        "        print(f\"An error occurred while scraping {url}: {e}. Saving the scraped data to a CSV file and continuing...\")\n",
        "        # create a DataFrame from the rows list and save it to a CSV file\n",
        "        articles = pd.DataFrame(rows)\n",
        "        articles.to_csv('/content/drive/MyDrive/kiniunfinished.csv', index=False)\n",
        "        driver.quit()\n",
        "        exit()\n",
        "driver.quit()\n",
        "# create a DataFrame from the rows list and split the text column into multiple rows\n",
        "articles = pd.DataFrame(rows)\n",
        "articles['fulltext']=articles['text']\n",
        "articles2 = articles['text'].str.split('\\n', expand=True).stack().reset_index(level=1, drop=True).to_frame('text')\n",
        "articles2 = articles.drop('text', axis=1).join(articles2).reset_index(drop=True)\n",
        "\n",
        "articles2['coding']=''\n",
        "articles2 = articles2[articles2.text.str.len()>50]\n",
        "articles2=articles2[articles2.text!=''].reset_index()\n",
        "# save the DataFrame to a CSV file\n",
        "articles2.to_csv('/content/drive/MyDrive/kini.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX0U9h1EtsYZ"
      },
      "source": [
        "# The Star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uDBL2kzwhY6"
      },
      "outputs": [],
      "source": [
        "# Set up and use Chrome WebDriver\n",
        "import pandas as pd\n",
        "from IPython.display import Image\n",
        "from IPython.display import display as disp\n",
        "from selenium import webdriver\n",
        "from pyvirtualdisplay import Display\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "# Set up a virtual display\n",
        "display = Display(visible=0, size=(1280, 800))\n",
        "display.start()\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "# Configure Chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Create an instance of the Chrome WebDriver\n",
        "\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "# Navigate to a website\n",
        "driver.get('https://www.thestar.com.my/search?query=malaysia+refugee')\n",
        "dates = []\n",
        "urls = []\n",
        "while True:\n",
        "    try:\n",
        "        # Wait for the search results to load\n",
        "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".queryly_item_container\")))\n",
        "\n",
        "        # Scrape the date and link URL for each search result on the current page\n",
        "        search_results = driver.find_elements(By.CSS_SELECTOR, \".queryly_item_container\")\n",
        "        for result in search_results:\n",
        "            date = result.find_element(By.CSS_SELECTOR, \".timestamp\").text.split('|')[0].strip()\n",
        "            link_url = result.find_element(By.CSS_SELECTOR, \"h2 a\").get_attribute(\"href\")\n",
        "            print([date, link_url])\n",
        "            dates.append(date)\n",
        "            urls.append(link_url)\n",
        "        # Click the \"next page\" button and wait for the next page to load\n",
        "        next_page_button = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".next_btn\")))\n",
        "        driver.execute_script(\"arguments[0].scrollIntoView();\", next_page_button)\n",
        "        next_page_button.click()\n",
        "        WebDriverWait(driver, 10).until(EC.staleness_of(search_results[0]))\n",
        "\n",
        "    except:\n",
        "        # No more \"next page\" button found, so break out of the loop\n",
        "        break\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()\n",
        "\n",
        "# Stop the virtual display\n",
        "display.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t26ZvifL8Tbf"
      },
      "outputs": [],
      "source": [
        "\n",
        "#urllist = urllist.URL\n",
        "urllist=pd.read_csv(\"/content/drive/MyDrive/TMIurls.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y0WsfhD10XU"
      },
      "outputs": [],
      "source": [
        "\n",
        "urllist=pd.read_csv('/content/drive/MyDrive/Starthemes.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lnEaGRMMp86"
      },
      "outputs": [],
      "source": [
        "urllist[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGHDfCFH2r8m"
      },
      "outputs": [],
      "source": [
        " urllist=list(urllist['URL'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srpqz4BAlZGw"
      },
      "outputs": [],
      "source": [
        "urllist.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVR6ZnRf0ltw"
      },
      "outputs": [],
      "source": [
        "from newspaper import Article, ArticleException"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiRyhRGB06ZC"
      },
      "outputs": [],
      "source": [
        "url = 'https://www.thesundaily.my/archive/myanmar-migrants-abandoned-smuggler-thai-south-police-CTARCH443829'\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "article.publish_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLHEa7kPjS32"
      },
      "outputs": [],
      "source": [
        "date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EUnyA9ugOwQ"
      },
      "outputs": [],
      "source": [
        "datetime.strptime(date_str, \"%m-%d- %Y %I:%M %p\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyje3HhJ88nO"
      },
      "outputs": [],
      "source": [
        "date_element = soup.find('div', {'class': 'datefrom'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liOHOkHKwSz0"
      },
      "outputs": [],
      "source": [
        "article = Article(urllist[1], request_timeout=15)\n",
        "article.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTbfMs3kwauq"
      },
      "outputs": [],
      "source": [
        "article.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP7RDbdGSTno"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from newspaper import Article, ArticleException\n",
        "import requests\n",
        "import dateutil.parser\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "def extract_date_using_bs4(html):\n",
        "    date_element = BeautifulSoup(html, \"html.parser\").find(\"li\", {\"class\": \"date\", \"itemprop\": \"datePublished\"})\n",
        "    if date_element:\n",
        "        date_string = date_element.text.strip()\n",
        "        return datetime.strptime(date_string, \"%d-%m- %Y %I:%M %p\")\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "columns = ['url', 'title', 'date', 'text', 'keywords', 'source']\n",
        "source = 'The Sun'\n",
        "\n",
        "# Check if the partial file exists and load its data into a DataFrame\n",
        "if os.path.isfile('/content/drive/MyDrive/suntest_partial.csv'):\n",
        "    partial_df = pd.read_csv('/content/drive/MyDrive/suntest_partial.csv')\n",
        "    # Get a list of URLs that have already been scraped\n",
        "    scraped_urls = set(partial_df['url'].tolist())\n",
        "else:\n",
        "    partial_df = pd.DataFrame(columns=columns)\n",
        "    scraped_urls = set()\n",
        "\n",
        "rows = []  # list to hold rows of scraped data\n",
        "\n",
        "# Iterate through the list of URLs, skipping those that have already been scraped\n",
        "for url in [x for x in urllist if x not in scraped_urls]:\n",
        "    print(url)\n",
        "    try:\n",
        "        article = Article(url, request_timeout=15)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        title = article.title\n",
        "        # Search for the li element containing the date\n",
        "        date = article.publish_date or extract_date_using_bs4(article.html)\n",
        "        print(date)\n",
        "        text = article.text\n",
        "        article.nlp()\n",
        "        keywords = article.keywords\n",
        "        print(title)\n",
        "        # append the scraped data to the rows list\n",
        "        rows.append({'url': url, 'title': title, 'date': date, 'text': text, 'keywords': keywords,'source': source})\n",
        "    except (ArticleException, requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n",
        "        print(f\"An error occurred while scraping {url}: {e}. Saving the scraped data to a CSV file and continuing...\")\n",
        "        # create a DataFrame from the rows list and save it to a CSV file\n",
        "        partial_df = pd.DataFrame(rows)\n",
        "        partial_df.to_csv('/content/drive/MyDrive/suntest_partial.csv', index=False)\n",
        "        continue\n",
        "\n",
        "# create a DataFrame from the rows list and split the text column into multiple rows\n",
        "articles = pd.concat([partial_df, pd.DataFrame(rows)])\n",
        "\n",
        "articles['fulltext']=articles['text']\n",
        "articles2 = articles['text'].str.split('\\n', expand=True).stack().reset_index(level=1, drop=True).to_frame('text')\n",
        "articles2 = articles.drop('text', axis=1).join(articles2).reset_index(drop=True)\n",
        "articles2=articles2[articles2['text']!='']\n",
        "# save the DataFrame to a CSV file\n",
        "articles2.to_csv('/content/drive/MyDrive/Suntest.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq2R9fY02Rrn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX4y1rgszKyV"
      },
      "outputs": [],
      "source": [
        "articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSW7-MfG3WMN"
      },
      "outputs": [],
      "source": [
        "testarticles=articles2.sample(frac=0.01, random_state=42).reset_index()\n",
        "testarticles.to_csv('/content/summarytest.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWEsTWypJ1ML"
      },
      "outputs": [],
      "source": [
        "with open(\"file.html\", 'rb') as fh:\n",
        "    ht = fh.read()#replace with just beautifulsoup html.\n",
        "article = newspaper.Article(url = ' ')\n",
        "article.set_html(ht)\n",
        "article.parse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxiwr731yBLh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from newspaper import Article, ArticleException\n",
        "columns = ['url', 'title', 'date', 'text', 'keywords', 'source']\n",
        "articles = pd.DataFrame(columns=columns)\n",
        "source = 'The Malaysian Insight'\n",
        "\n",
        "rows = []  # list to hold rows of scraped data\n",
        "\n",
        "for url in urllist:\n",
        "    print(url)\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        title = article.title\n",
        "        soup = BeautifulSoup(article.html, 'html.parser')\n",
        "        # Search for the li element containing the date\n",
        "        date_element = soup.find('span', {'class': 'byline-time'})\n",
        "        date_text = date_element.get_text(strip=True)\n",
        "        date = date_text.split(\"Published on \")[1].split(\" ·\")[0]\n",
        "        #date = article.publish_date\n",
        "        print(date)\n",
        "        text = article.text\n",
        "        article.nlp()\n",
        "        keywords = article.keywords\n",
        "        print(title)\n",
        "        # append the scraped data to the rows list\n",
        "        rows.append({'url': url, 'title': title, 'date': date, 'text': text, 'keywords': keywords, 'source': source})\n",
        "    except (ArticleException, requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n",
        "        print(f\"An error occurred while scraping {url}: {e}. Saving the scraped data to a CSV file and continuing...\")\n",
        "        # create a DataFrame from the rows list and save it to a CSV file\n",
        "        articles = pd.DataFrame(rows)\n",
        "        articles.to_csv('/content/drive/MyDriveTMIunfinished.csv', index=False)\n",
        "        exit()\n",
        "\n",
        "# create a DataFrame from the rows list and split the text column into multiple rows\n",
        "articles = pd.DataFrame(rows)\n",
        "articles['fulltext']=articles['text']\n",
        "articles2 = articles['text'].str.split('\\n', expand=True).stack().reset_index(level=1, drop=True).to_frame('text')\n",
        "articles2 = articles.drop('text', axis=1).join(articles2).reset_index(drop=True)\n",
        "\n",
        "articles2['coding']=''\n",
        "articles2 = articles2[articles2.text.str.len()>50]\n",
        "articles2=articles2[articles2.text!=''].reset_index()\n",
        "# save the DataFrame to a CSV file\n",
        "articles2.to_csv('/content/drive/MyDrive/TMI.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wawT308_-TCy"
      },
      "outputs": [],
      "source": [
        "articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXFi4KQXdPOY"
      },
      "outputs": [],
      "source": [
        "#only for old datasets!!! Malay Mail and Star\n",
        "articles2[\"text\"] = articles2[\"text\"].astype(str)\n",
        "# Group the dataframe by title and concatenate all text values\n",
        "grouped = articles2.groupby(\"title\")[\"text\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "# Create a new dataframe with the grouped results\n",
        "result_df = pd.DataFrame({\"title\": grouped.index, \"fulltext\": grouped.values})\n",
        "\n",
        "# Merge the new dataframe with the original dataframe\n",
        "merged_df = pd.merge(articles2, result_df, on=\"title\")\n",
        "articles2 = merged_df\n",
        "articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL0jtGx97CrX"
      },
      "outputs": [],
      "source": [
        "articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a-SYXMqCGxe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "articles2=pd.read_csv('/content/drive/MyDrive/malaymail.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EByY-zRR6uOC"
      },
      "outputs": [],
      "source": [
        "articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiUJSxRH0qTt"
      },
      "outputs": [],
      "source": [
        "articles.to_csv(\"/content/drive/MyDrive/malaymail.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSbSOsROf7R-"
      },
      "outputs": [],
      "source": [
        "articles3=pd.read_csv('/content/drive/MyDrive/sunpartialsumm.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ2idnwbf6MI"
      },
      "outputs": [],
      "source": [
        "articles2.to_csv('/content/drive/MyDrive/sunsumm.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZhC5LG4g1C8"
      },
      "outputs": [],
      "source": [
        "grouped_df = articles2.groupby('fulltext')['title'].first().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-q4XvhrQlVj"
      },
      "outputs": [],
      "source": [
        "test = articles2.groupby('fulltext')['title'].first().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9utOAUoUOxx"
      },
      "outputs": [],
      "source": [
        "articles2.to_csv('/content/drive/MyDrive/kini.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smZwZsIasjXa"
      },
      "outputs": [],
      "source": [
        "articles3.update(grouped_df[['summary']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U37pYMn4gzoz"
      },
      "outputs": [],
      "source": [
        "df = grouped_df.iloc[:100,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PWITGZEDoan"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYYNHVdvowNC"
      },
      "outputs": [],
      "source": [
        "segments[18]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJD2RhKOnV67"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhiBONDssqhR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.1\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are coding excerpts of news articles for a thematic analysis of attitudes towards refugees in news media in Malaysia.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,summary):\n",
        "    #print(excerpt)\n",
        "    try:\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article summarized here: ### '  + str(summary) + ' ### passage: ### ' + str(excerpt)+ ' ### In 12 words or less, give the theme of this specific passage as it embodies, relates to or reflects attitudes towards refugees in Malaysia, or return \"Irrelevant\"'  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_0AVs51cggn"
      },
      "outputs": [],
      "source": [
        "review[:80]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etxAOgH8NFJn"
      },
      "outputs": [],
      "source": [
        "grouped_df=review[:80]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivRjUJKXdsBu"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "from retry import retry\n",
        "\n",
        "# define the function to process each excerpt\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt):\n",
        "    excerpt = ' '.join(excerpt.split()[:400])\n",
        "    try:\n",
        "      api_key = random.choice([samkey])\n",
        "      openai.api_key = api_key\n",
        "      completion = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\", temperature=0.1,\n",
        "          messages=[\n",
        "              {\"role\": \"system\", \"content\": \"You are summarizing news articles for a thematic analysis of attitudes towards refugees in news media.\"},\n",
        "              {\"role\": \"user\", \"content\": \"Return a one paragraph summary of the following article: \" + excerpt}\n",
        "          ]\n",
        "      )\n",
        "      print('done')\n",
        "      print(excerpt[:15])\n",
        "      print(completion.choices[0].message['content'])\n",
        "      return completion.choices[0].message['content']\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing row {idx}: {e}\")\n",
        "      return None\n",
        "\n",
        "# split the dataframe into segments\n",
        "num_segments = 2\n",
        "if 'segments' not in globals():\n",
        "  print('newsegs')\n",
        "  grouped_df['summary']=None\n",
        "  segment_size = len(grouped_df) // num_segments\n",
        "  segments = [grouped_df.iloc[i:i+segment_size] for i in range(0, len(grouped_df), segment_size)]\n",
        "\n",
        "# define a function to process each segment in parallel\n",
        "def process_segment(segment):\n",
        "    segment.loc[segment['summary'].isna(), 'summary'] = segment.loc[segment['summary'].isna(), 'fulltext'].apply(process_excerpt)\n",
        "    return segment\n",
        "# process each segment in parallel using concurrent.futures\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    # apply the process_segment function to each segment in parallel\n",
        "    results = list(executor.map(process_segment, segments))\n",
        "\n",
        "# merge the results and update the original dataframe\n",
        "grouped_df = pd.concat(results).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quDGPhfxgQwg"
      },
      "outputs": [],
      "source": [
        "grouped_df['summary']=''\n",
        "for index, row in grouped_df .iterrows():\n",
        "  print(index)\n",
        "  excerpt = grouped_df.loc[index,'fulltext']\n",
        "  completion = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\", temperature=0.1,\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are summarizing news articles for a thematic analysis of attitudes towards refugees in news media.\"},\n",
        "      {\"role\": \"user\", \"content\": \"Return a one paragraph summary of the following article: \" + excerpt}\n",
        "    ]\n",
        "  )\n",
        "  grouped_df.loc[index,'summary']=completion.choices[0].message['content']\n",
        "\n",
        "  print(completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c5J-HIopA1M"
      },
      "outputs": [],
      "source": [
        "grouped_df = pd.concat(results).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLkJ0RP31TrA"
      },
      "outputs": [],
      "source": [
        "articles2 = pd.read_csv(\"/content/drive/MyDrive/mmsumm.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTyQyG4WRgqW"
      },
      "outputs": [],
      "source": [
        "grouped_df['summary']=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXq5x5TnnRPO"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMxVVIYlMnvF"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShIR9LAEUMaY"
      },
      "outputs": [],
      "source": [
        "segments[gro]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LHLt7EULRUd_"
      },
      "outputs": [],
      "source": [
        "#@title OLD?\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.1\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are summarizing news articles for a thematic analysis of attitudes towards refugees in news media.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "def process_excerpt(excerpt):\n",
        "    print('done')\n",
        "    return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": \"Return a one paragraph summary of the following article: \" + str(excerpt)}]).choices[0].message['content']\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(grouped_df, 2)]\n",
        "    for segment in segments:\n",
        "      segment['summary'] = None\n",
        "\n",
        "for segment in segments:\n",
        "\n",
        "    if not segment['summary'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor\n",
        "        futures = [executor.submit(process_excerpt, row['fulltext']) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'summary'] = result\n",
        "\n",
        "# Merge the segments\n",
        "#grouped_df=pd.concat(segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuMeQH3Iea6-"
      },
      "outputs": [],
      "source": [
        " articles2 = grouped_df.merge(articles2, on=['title', 'fulltext'], how='outer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvYiAkR8O-HQ"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Htg5xMfkM0As"
      },
      "outputs": [],
      "source": [
        "articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iun9wWNW3bij"
      },
      "outputs": [],
      "source": [
        "articles2.to_csv(\"/content/drive/MyDrive/kinisumm.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANeYXAktgHOY"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE68SDErz4x3"
      },
      "outputs": [],
      "source": [
        "Describe how the following passage reflects attitudes towards refugees in Malaysia in one sentence , taking into account the content of the text that precede and follow the following passage in this entire news article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1nNa-CY0T6J"
      },
      "outputs": [],
      "source": [
        "'You will read a passage from a news article summarized as follows ### '  + str(summary) + ' ### Now, describe how the following passage reflects attitudes towards refugees in Malaysia in one sentence ### '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpyeEss5rKaR"
      },
      "outputs": [],
      "source": [
        "summary= 'Myanmars navy has detained nearly 50 Rohingya Muslims at sea, including men, women, and children, as well as five traffickers, according to local officials. The group was likely aiming for Malaysia or Indonesia, countries with large Rohingya diasporas. Thousands of Rohingya have attempted to escape refugee camps in Bangladesh and oppressive conditions in Rakhine by taking to the sea in high-risk attempts. The group detained at sea this week is the latest in a series of arrests in recent months as seasonal calmer waters tempt more Rohingya to put their lives in the hands of traffickers.'\n",
        "excerpt = 'Hundreds of thousands more remain in Rakhine, living under tight restrictions with little access to healthcare, education or livelihoods in conditions that Amnesty International brands as \"apartheid\".'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhEDnba2swvA"
      },
      "outputs": [],
      "source": [
        "testarticles=articles2[:200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZvHzxZvyJuh"
      },
      "outputs": [],
      "source": [
        "articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0idS-Vg5yJQN"
      },
      "outputs": [],
      "source": [
        "segments = [df.reset_index(drop=True) for df in np.array_split(articles2, 10)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thv46t4VHFFd"
      },
      "outputs": [],
      "source": [
        "segments[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ag_ws7GuE6E"
      },
      "outputs": [],
      "source": [
        "testarticles.to_csv(\"/content/testprompts.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_XvvkPfrInc"
      },
      "outputs": [],
      "source": [
        "openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'You will read a passage from a news article summarized as follows ### '  + str(summary) + ' ### Now, describe how the following passage reflects attitudes towards refugees in Malaysia in one sentence ### ' + str(excerpt)}]).choices[0].message['content']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcWZLrlGsxzq"
      },
      "outputs": [],
      "source": [
        "articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwU4bpryf62z"
      },
      "outputs": [],
      "source": [
        "\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.1\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are coding excerpts of news articles for a thematic analysis of attitudes towards refugees in news media in Malaysia.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,summary):\n",
        "    #print(excerpt)\n",
        "    try:\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article summarized here: ### '  + str(summary) + ' ### passage: ### ' + str(excerpt)+ ' ### In 12 words or less, give the theme of this specific passage as it embodies, relates to or reflects attitudes towards refugees in Malaysia, or return \"Irrelevant\"'  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(articles2, 10)]\n",
        "    for segment in segments:\n",
        "      segment['coding'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['coding'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','summary']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'coding'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).drop(['index'], axis=1).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['coding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    grouped_df.at[idx, 'coding'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in grouped_df[grouped_df['coding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    grouped_df.at[idx, 'coding'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "grouped_df.to_csv(\"/content/drive/MyDrive/kinithemes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hs_fXcKvLfA"
      },
      "outputs": [],
      "source": [
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).drop(['index'], axis=1).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['coding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    grouped_df.at[idx, 'coding'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "#grouped_df.to_csv(\"/content/drive/MyDrive/sunthemes.csv\")\n",
        "for idx, row in grouped_df[grouped_df['coding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    grouped_df.at[idx, 'coding'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y49tONaZJZS8"
      },
      "outputs": [],
      "source": [
        "#grouped_df=pd.concat(segments).drop(['index'], axis=1).reset_index().drop('index', axis=1)\n",
        "#grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['coding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    grouped_df.at[idx, 'coding'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9-KZyNoK2it"
      },
      "outputs": [],
      "source": [
        "grouped_df=grouped_df.sample(frac=0.02, random_state=42).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkRPykR2lVp8"
      },
      "outputs": [],
      "source": [
        "grouped_df=pd.concat(segments).drop(['level_0','index'], axis=1).reset_index().drop('index', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uppviHhLLLcU"
      },
      "outputs": [],
      "source": [
        "grouped_dfold=grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cCY6CjwCcow"
      },
      "outputs": [],
      "source": [
        "grouped_df[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-24fWTvm9r3"
      },
      "outputs": [],
      "source": [
        "grouped_df.to_csv(\"/content/drive/MyDrive/TMIthemes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCIvxQUyiDN0"
      },
      "outputs": [],
      "source": [
        "grouped_df=pd.read_csv(\"/content/drive/MyDrive/starthemes.csv\").iloc[:,3:]\n",
        "grouped_df['index'] = grouped_df.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QDWoq-ApIuE"
      },
      "outputs": [],
      "source": [
        "grouped_df=pd.read_csv(\"/content/drive/MyDrive/starthemes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKsnt7RhuDuo"
      },
      "outputs": [],
      "source": [
        "grouped_df.rename(columns={'coding2': 'coding'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4zZ2AqDQ1Fo"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WowEAZTHRrr4"
      },
      "outputs": [],
      "source": [
        "grouped_df.loc[~grouped_df['coding'].str.contains('rrelevant')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJQwuX76XFvq"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u57G1jFnW9F9"
      },
      "outputs": [],
      "source": [
        "grouped_df.loc[~grouped_df['coding'].astype(str).str.contains('rrelevant')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spXjQAN4a2si"
      },
      "outputs": [],
      "source": [
        "test.loc[test['coding2'].str.contains('rue')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0TBMKYGkR_S"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPMqLyfxJskX"
      },
      "outputs": [],
      "source": [
        "\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "test = grouped_df.loc[~grouped_df['coding'].astype(str).str.contains('rrelevant')]\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.0\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are reviewing codings for excerpts of news articles for a thematic analysis of attitudes towards refugees in news media in Malaysia.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt):\n",
        "    #print(excerpt)\n",
        "    try:\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Is this code : ### ' + str(excerpt)+ ' ### too vague (True) to provide useful information about the theme of the passage, or is it adequate (False)? Good codes will identify specific themes relevant to attitudes towards refugees and the valence of those attitudes, if relevant. Example: \"Attitudes towards refugees in Malaysia\" is too vague as a code. Respond in one word, with a Python Boolean: True/False'  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(test, 10)]\n",
        "    for segment in segments:\n",
        "      segment['coding2'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['coding2'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding2' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['coding']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'coding2'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "test=pd.concat(segments)\n",
        "for idx, row in test[test['coding2'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['coding'])\n",
        "    print(result)\n",
        "    test.at[idx, 'coding2'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in test[test['coding2'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['coding'])\n",
        "    print(result)\n",
        "    test.at[idx, 'coding2'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "test.to_csv(\"/content/drive/MyDrive/starvague.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf55hA3KMNjn"
      },
      "outputs": [],
      "source": [
        "for idx, row in test[test['coding2'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['coding'])\n",
        "    print(result)\n",
        "    test.at[idx, 'coding2'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "    test.to_csv(\"/content/drive/MyDrive/starvague.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP4jXDBM40MJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNaOHyaAMO0Q"
      },
      "outputs": [],
      "source": [
        "test=pd.read_csv(\"/content/drive/MyDrive/starvague.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT0YAQMEKRrq"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxKEMX20tT8K"
      },
      "outputs": [],
      "source": [
        "segments[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4P5_MSeCRtv"
      },
      "outputs": [],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BHdL8UxdyOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "test = test.loc[test['coding2'].str.contains('rue')]\n",
        "test['codingold']=test['coding']\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.1\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are coding excerpts of news articles for a thematic analysis of attitudes towards refugees in news media in Malaysia.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,summary):\n",
        "    #print(excerpt)\n",
        "    try:\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article summarized here: ### '  + str(summary) + ' ### passage: ### ' + str(excerpt)+ ' ### In 18 words or less, give a theme of this specific passage as it embodies, relates to or reflects attitudes towards refugees in Malaysia. Include the valence of the attitudes and their target, if relevant.'  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(test, 10)]\n",
        "    for segment in segments:\n",
        "      segment['coding'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['coding'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','summary']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'coding'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "group2=pd.concat(segments).iloc[:,1:].reset_index()\n",
        "group2.set_index('index', inplace=True)\n",
        "for idx, row in group2[group2['coding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    group2.at[idx, 'coding'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "grouped_df.loc[group2.index, 'coding']=group2['coding']\n",
        "grouped_df.to_csv(\"/content/drive/MyDrive/starthemes2.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugUJPkyIFdJx"
      },
      "outputs": [],
      "source": [
        "for idx, row in group2[group2['coding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    group2.at[idx, 'coding'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "grouped_df.loc[group2.index, 'coding']=group2['coding']\n",
        "grouped_df.to_csv(\"/content/drive/MyDrive/starthemes2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMVPDJRFGQKZ"
      },
      "outputs": [],
      "source": [
        "test.loc[test.index==80,['coding','coding2']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPoAytXPHQt4"
      },
      "outputs": [],
      "source": [
        "test[:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b1maXtPGjqW"
      },
      "outputs": [],
      "source": [
        "grouped_df[140:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owkFMRadD_B-"
      },
      "outputs": [],
      "source": [
        "grouped_df.loc[grouped_df['coding'].str.contains('ttitudes'),['date','coding']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ee3YlJycrNf"
      },
      "outputs": [],
      "source": [
        "group2=pd.concat(segments).iloc[:,1:].reset_index()\n",
        "group2[['coding','codingold']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnJGJcTfuojL"
      },
      "outputs": [],
      "source": [
        "grouped_df.to_csv(\"/content/drive/MyDrive/valencetest.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4SW2t8cMvB-"
      },
      "outputs": [],
      "source": [
        "\n",
        "test[test['url'].str.contains('https://www.thestar.com.my/aseanplus')].url[104]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xweqlc1gw3mX"
      },
      "outputs": [],
      "source": [
        "group2.loc[group2.index, 'codingold']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07dbiDUBnzBi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqbdq8JBvlXI"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4z5xpE08JZn"
      },
      "outputs": [],
      "source": [
        "grouped_df.to_csv(\"/content/drive/MyDrive/TMIthemes2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuANroaN8Q4j"
      },
      "outputs": [],
      "source": [
        "grouped_df=pd.read_csv(\"/content/drive/MyDrive/kinithemes2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oClHq3jfHh_D"
      },
      "outputs": [],
      "source": [
        "grouped_df.to_csv(\"codingtest.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN_aLf6mbdUI"
      },
      "outputs": [],
      "source": [
        "segments[399].coding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brl-SuKyv4nY"
      },
      "outputs": [],
      "source": [
        "articles2=grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq5PXeTSvh8W"
      },
      "outputs": [],
      "source": [
        "articles2['prompt8']=grouped_df['coding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I5KrrIdz7r6"
      },
      "outputs": [],
      "source": [
        "testarticles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGp1CN2JqT2C"
      },
      "outputs": [],
      "source": [
        "articles2['summary'].str.len().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JCcQBECjChp"
      },
      "outputs": [],
      "source": [
        "segments[0].loc[104,['coding']].str.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOO5IRwGiV4E"
      },
      "outputs": [],
      "source": [
        "articles2=pd.concat(segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biAXV7U2xvAi"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import openai\n",
        "\n",
        "articles2['coding'] = ''\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.1\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are coding text excerpts for a thematic analysis of attitudes towards refugees in news media.\"}]\n",
        "\n",
        "def process_excerpt(excerpt):\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=model, temperature=temperature,\n",
        "        messages=messages + [{\"role\": \"user\", \"content\": \"Return the theme of the following passage: \" + excerpt}]\n",
        "    )\n",
        "    print( completion.choices[0].message['content'])\n",
        "    return completion.choices[0].message['content']\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
        "    results = list(executor.map(process_excerpt, articles2['text']))\n",
        "\n",
        "articles2['coding'] = results\n",
        "articles2.to_csv(\"/content/drive/MyDrive/starthemes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX7Wa4JRfFaj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "articles2=pd.read_csv('/content/drive/MyDrive/malaymailthemes2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAVcrnvlZZIB"
      },
      "outputs": [],
      "source": [
        "articles2.fulltext[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGWRzRsDolms"
      },
      "outputs": [],
      "source": [
        "articles3=pd.read_csv('/content/drive/MyDrive/kinithemesclean.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xzx-WQ8Ft4v"
      },
      "outputs": [],
      "source": [
        "articles3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFtoe1doZnKw"
      },
      "outputs": [],
      "source": [
        "articles3 = articles2.copy()\n",
        "articles3['coding'] = articles3['coding'].str.split('heme:').str[-1]\n",
        "#articles3['coding'] = articles3['coding'].str.split('The theme of the passage is').str[-1]\n",
        "#articles3['coding'] = articles3['coding'].str.split('The theme of this passage is').str[-1]\n",
        "#articles3['coding'] = articles3['coding'].str.split('The theme of this passage is').str[-1]\n",
        "import re\n",
        "pattern = r'(?<!\\\\)\\\"(?=[^\"]*(?<!\\\\)\\\")|(?<!\\\\)\\\"(?=[^\"]*$)'\n",
        "\n",
        "# Apply regular expression to remove nested quotes\n",
        "articles3['coding'] = articles3['coding'].apply(lambda x: re.sub(pattern, '', x))\n",
        "#articles3 = articles3.loc[~articles3['coding'].str.contains('passage')]\n",
        "articles3.to_csv(\"/content/drive/MyDrive/kinithemesclean.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI0vIK0P2QaM"
      },
      "outputs": [],
      "source": [
        "articles3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luIWhY1L1cup"
      },
      "outputs": [],
      "source": [
        "articles3full = articles3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3coIBy6r3O63"
      },
      "outputs": [],
      "source": [
        "articles3 =articles3full.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz8oyFL9kA0E"
      },
      "outputs": [],
      "source": [
        "articles3.to_csv(\"/content/drive/MyDrive/starthemesembed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPQeNHIigjYu"
      },
      "outputs": [],
      "source": [
        "openai.Embedding.create(input=['hello'], model=\"text-embedding-ada-002\")['data'][0]['embedding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InqVwlJkumf5"
      },
      "outputs": [],
      "source": [
        "articles3.loc[1,'ada_embedding']=irel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iRoEQncr7fG"
      },
      "outputs": [],
      "source": [
        "articles3['ada_embedding'] = articles3.apply(lambda row: irel, axis=1)\n",
        "articles3.loc[~articles3['coding'].str.contains('rrelevant'),'ada_embedding'] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkVXB0tawlf3"
      },
      "outputs": [],
      "source": [
        "articles3[articles3['ada_embedding'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr8h-IJCwU6Q"
      },
      "outputs": [],
      "source": [
        "articles3.loc[~articles3['coding'].str.contains('rrelevant'),'ada_embedding'] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA45kCduzJzt"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Qr9rqyW22m"
      },
      "outputs": [],
      "source": [
        "articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFmv1V-9xzt0"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from retry import retry\n",
        "\n",
        "model=\"text-embedding-ada-002\"\n",
        "irel=openai.Embedding.create(input=['Irrelevant.'], model=model)['data'][0]['embedding']\n",
        "articles3['ada_embedding'] = articles3.apply(lambda row: irel, axis=1)\n",
        "articles3.loc[~articles3['coding'].str.contains('rrelevant'),'ada_embedding'] = None\n",
        "\n",
        "# define the function to apply to each row\n",
        "subarts=articles3[articles3['ada_embedding'].isnull()]\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(text):\n",
        "    try:\n",
        "      text = text.replace(\"\\n\", \" \")\n",
        "      if random.random() < 0.01:\n",
        "        print(iter)\n",
        "      return openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(subarts, 25)]\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['ada_embedding'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['coding']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'ada_embedding'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).reset_index()\n",
        "for idx, row in grouped_df[grouped_df['ada_embedding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['coding'])\n",
        "    grouped_df.at[idx, 'ada_embedding'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "articles3.loc[articles3['ada_embedding'].isnull(), 'ada_embedding'] = grouped_df.ada_embedding.values\n",
        "articles3.to_csv(\"/content/drive/MyDrive/kiniembed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_2RrVamHUfu"
      },
      "outputs": [],
      "source": [
        "articles3=pd.read_csv(\"/content/drive/MyDrive/TMIembed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2U_vONFjTe-"
      },
      "outputs": [],
      "source": [
        "articles3[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0TBGiFzi4i9"
      },
      "outputs": [],
      "source": [
        "I have a list of 5 pandas dataframes saved as CSVs. These dataframes share a set of columns, but may have additional, irrelevant columns interspersed. I want to generate a master dataframe containing the rows of all 5 dataframes for the relevant columns. Provide a script to do this, using the following details###\n",
        "[malaymail, star, sun, kini, TMI]#5 prefixes to prepend to filenames\n",
        "'embed.csv' #filename stem\n",
        "fulltext,\ttitle,\tsummary,\turl,\tdate,\tkeywords,\tsource,\ttext,\tcoding,\tindex,\tada_embedding #names of the columns to use from each df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uQEwtxGk6UX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of file prefixes\n",
        "file_prefixes = ['malaymail', 'star', 'sun', 'kini', 'TMI']\n",
        "\n",
        "# Columns to use from each dataframe\n",
        "usecols = ['fulltext', 'title', 'summary', 'url', 'date', 'keywords', 'source', 'text', 'coding', 'index', 'ada_embedding']\n",
        "\n",
        "# Empty list to hold dataframes\n",
        "df_list = []\n",
        "\n",
        "# Read each CSV and append the dataframe to df_list\n",
        "for prefix in file_prefixes:\n",
        "    df = pd.read_csv(f'/content/drive/MyDrive/{prefix}embed.csv', usecols=usecols)\n",
        "    df_list.append(df)\n",
        "# Concatenate all the dataframes in df_list\n",
        "master_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Save the master dataframe to a new CSV file\n",
        "master_df.to_csv('/content/drive/MyDrive/master_embed.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smS9fT66lZPc"
      },
      "outputs": [],
      "source": [
        "master_df = pd.read_csv('/content/drive/MyDrive/master_embed.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTbfH5jnVHhd"
      },
      "outputs": [],
      "source": [
        "master_df2= master_df.loc[~master_df['coding'].str.contains('rrelevant')].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "decKJpQTVK6V"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXZu8kt_2KXn"
      },
      "outputs": [],
      "source": [
        "articles3.to_csv(\"/content/drive/MyDrive/starthemesembed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ts1_yT22r5z"
      },
      "outputs": [],
      "source": [
        "articles3=pd.read_csv('/content/drive/MyDrive/malaymailthemesembed_combined.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFI70v72kxLN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3DxdL3pfZ8F"
      },
      "outputs": [],
      "source": [
        "articles3star=pd.read_csv('/content/drive/MyDrive/starthemesembed.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWwzfBu0kLuy"
      },
      "outputs": [],
      "source": [
        "articles3.to_csv(\"/content/drive/MyDrive/malaymailthemesembed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKQqQX96kpC9"
      },
      "outputs": [],
      "source": [
        "articles3 = pd.concat([articles3star.iloc[:,1:], articles3.iloc[:,4:]], axis=0, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhPQutospm51"
      },
      "outputs": [],
      "source": [
        "articles3 = pd.concat([articles3.iloc[:,:-1], pd.DataFrame(embedding, columns=['embedding1', 'embedding2'])], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bZh4eu6o6Rx"
      },
      "outputs": [],
      "source": [
        "articles3star[['coding','ada_embedding']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkiWtJ_nhrya"
      },
      "outputs": [],
      "source": [
        "sampled_df = articles3.sample(frac=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL4OmdAskC9U"
      },
      "outputs": [],
      "source": [
        "\n",
        "articles3.to_csv(\"starmalaymail.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUM1ulyjhx7G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtWv-1fNfKLV"
      },
      "outputs": [],
      "source": [
        "articles3star=articles3.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEyVG_f1O1RF"
      },
      "outputs": [],
      "source": [
        "pd.csv(\"/content/drive/MyDrive/malaymailthemesembed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FaOVRiw6afG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "matrix = np.vstack(articles3.ada_embedding.values)\n",
        "n_clusters = 20\n",
        "\n",
        "kmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\n",
        "kmeans.fit(matrix)\n",
        "articles3['Cluster'] = kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-3tWrTEUfZC"
      },
      "outputs": [],
      "source": [
        "articles3.types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNo2slRhQZ0B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "def convert_str_to_list(str_list):\n",
        "    return ast.literal_eval(str_list)\n",
        "\n",
        "# Apply the function to the ada_embedding column\n",
        "\n",
        "articles3['ada_embedding'] = articles3['ada_embedding'].map(convert_str_to_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfOYupolTLIQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "# Assuming `articles3.ada_embedding.values` is a numpy array of strings\n",
        "str_array = master_df2.ada_embedding.values\n",
        "\n",
        "# Use ast.literal_eval to safely evaluate each string into a list\n",
        "list_array = [ast.literal_eval(s) for s in str_array]\n",
        "\n",
        "# Convert the list of lists to a 2D numpy array\n",
        "data = np.array(list_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBPXuPThSUgt"
      },
      "outputs": [],
      "source": [
        "master_df.ada_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eG7li59TkVc"
      },
      "outputs": [],
      "source": [
        "data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPytH_d2AEmX"
      },
      "outputs": [],
      "source": [
        "data = [np.array(x) for x in master_df2.ada_embedding2.values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzaDPrGo-vTt"
      },
      "outputs": [],
      "source": [
        "!pip install hdbscan\n",
        "\n",
        "import hdbscan\n",
        "hdbscan = hdbscan.HDBSCAN(min_cluster_size=20, min_samples = 1)\n",
        "labels = hdbscan.fit_predict(embedding)\n",
        "hdbscan.condensed_tree_.plot(select_clusters=True)\n",
        "np.unique(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3S4aoe25NMG"
      },
      "outputs": [],
      "source": [
        "master_df=articles3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d188XKX5CAHP"
      },
      "outputs": [],
      "source": [
        "master_df['cluster']= labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heBfAx1SYA4m"
      },
      "outputs": [],
      "source": [
        "master_df.loc[master_df['cluster']==-1,['text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs83yBPRWc7Z"
      },
      "outputs": [],
      "source": [
        "master_df.loc[master_df['cluster']==100,['text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEjUyuXo3xKz"
      },
      "outputs": [],
      "source": [
        "master_df.to_csv('/content/drive/MyDrive/master_embed.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGFMRuec2eE7"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8LkUCgB9oO2"
      },
      "outputs": [],
      "source": [
        "articles3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm9eksZX7-Zf"
      },
      "outputs": [],
      "source": [
        "len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9tq5-MinDSd"
      },
      "outputs": [],
      "source": [
        "articles3[articles3.Cluster.isin([-1,1])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLkgPrA34zTn"
      },
      "outputs": [],
      "source": [
        "# Apply the sampling for each cluster\n",
        "master_df=master_df[master_df.text.str.len()>50]\n",
        "sampled_df = master_df.groupby('cluster').apply(lambda x: x.sample(n=6, random_state=1))\n",
        "\n",
        "# Sample 20 more rows where cluster == -1\n",
        "sampled_minus1 = master_df[master_df['cluster'] == -1].sample(n=24, random_state=1)\n",
        "\n",
        "# Sample 25 rows where cluster == NaN\n",
        "sampled_nan = master_df[master_df['cluster'].isna()].sample(n=30, random_state=1)\n",
        "\n",
        "# Append the sampled dfs\n",
        "sampled_df = sampled_df.append([sampled_minus1, sampled_nan], ignore_index=True).sort_values('cluster')\n",
        "\n",
        "sampled_df\n",
        "sampled_df.loc[:,['fulltext','url','title','summary','text','cluster','coding']].to_csv(\"/content/drive/MyDrive/rawcodes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J8SsYs0uYxR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRBHpyYBucpD"
      },
      "outputs": [],
      "source": [
        "review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yUbTravu3lh"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fFHS4ZrvlfY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul6lPZL2vokQ"
      },
      "outputs": [],
      "source": [
        "merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN_MTCbsA4FU"
      },
      "outputs": [],
      "source": [
        "sampled_df[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpT1Z-AnBIBE"
      },
      "outputs": [],
      "source": [
        "test = master_df.loc[master_df.source==\"The Sun\",['fulltext','url','title','summary','text','cluster','coding']].sample(n=1000, random_state=1)\n",
        "test['text_length'] = test['fulltext'].str.len()\n",
        "test.to_csv('/content/drive/MyDrive/articlelengthtest.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw_QAUIX-wpg"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_colwidth = 1000\n",
        "test.loc[test.text_length>2000,'url']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyqFjf7BpIA3"
      },
      "outputs": [],
      "source": [
        "sampled_df.loc[:,['fulltext','url','title','summary','text','cluster','coding']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXsHcnR3mKcO"
      },
      "outputs": [],
      "source": [
        "articles3.loc[articles3['Cluster']==-1,['text','coding']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e1Jil7KkX1K"
      },
      "outputs": [],
      "source": [
        "data[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5tYBD2h8ZL3"
      },
      "outputs": [],
      "source": [
        "articles4=articles3[articles3.Cluster.isin([-1,1])].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLmuiA3oosNX"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn\n",
        "import umap\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDB-ArA8QcDc"
      },
      "outputs": [],
      "source": [
        "!pip install hdbscan\n",
        "\n",
        "import hdbscan\n",
        "hdbscan = hdbscan.HDBSCAN(min_cluster_size=20, min_samples = 1)\n",
        "labels = hdbscan.fit_predict(articles3[['embedding1', 'embedding2', 'embedding3', 'embedding4']] )\n",
        "hdbscan.condensed_tree_.plot(select_clusters=True)\n",
        "np.unique(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgA1SvoaPkSu"
      },
      "outputs": [],
      "source": [
        "master_df=master_df[master_df.text.str.len()>100]\n",
        "data = master_df.ada_embedding2.to_list()\n",
        "reducer = umap.UMAP(n_components=4,metric='cosine',n_neighbors=100, min_dist = 0.0001)\n",
        "embedding = reducer.fit_transform(data)\n",
        "embedding.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usziCyfrVkCK"
      },
      "outputs": [],
      "source": [
        "!pip install hdbscan\n",
        "\n",
        "import hdbscan\n",
        "hdbscan = hdbscan.HDBSCAN(min_cluster_size=20, min_samples = 20)\n",
        "labels = hdbscan.fit_predict(embedding)\n",
        "hdbscan.condensed_tree_.plot(select_clusters=True)\n",
        "np.unique(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh6h1V-lPXvK"
      },
      "outputs": [],
      "source": [
        "master_df.Cluster=labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PSXpe9xCEAj"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM_eKdxhDukL"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "#size of the figure. I made it very large to accommodate country annotations.\n",
        "fig.set_size_inches(20, 20)\n",
        "ax.scatter(embedding[:, 0], embedding[:, 1],s=0.1)\n",
        "#script below adds annotations from the \"Country\" field of the datasets\n",
        "for i, txt in enumerate(countries.Country):\n",
        "    ax.annotate(txt, (embedding[:, 0][i], embedding[:, 1][i]),fontsize=8)\n",
        "#title of the figure\n",
        "plt.title('UMAP projection of Country Clustering by DFS Indicators', fontsize=24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGGkzuw75lEi"
      },
      "outputs": [],
      "source": [
        "master_df[['embedding1', 'embedding2', 'embedding3', 'embedding4']] = embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSpFm5uXlqZF"
      },
      "outputs": [],
      "source": [
        "articles3.loc[articles3['coding']=='Irrelevant.',['embedding1', 'embedding2']]=[21,4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTW1a3YBAKjq"
      },
      "outputs": [],
      "source": [
        "articles3.loc[articles3['coding']=='Irrelevant.',['embedding1', 'embedding2', 'embedding3']]=[-13,7,7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyLeEAu-EYyZ"
      },
      "outputs": [],
      "source": [
        "articles3[['embedding1', 'embedding2', 'embedding3']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9AcRRBq5Nq_"
      },
      "outputs": [],
      "source": [
        "!pip install plotly\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Assuming your dataframe is called 'df'\n",
        "\n",
        "# Create the scatterplot using plotly\n",
        "fig = px.scatter(articles3[articles3.embedding1<10], x='embedding1', y='embedding2', color='source', opacity=0.3, hover_data=['coding'])\n",
        "\n",
        "# Add title and axis labels\n",
        "fig.update_layout(title='Scatterplot of embeddings', xaxis_title='embedding1', yaxis_title='embedding2')\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHPXCtDFDCfk"
      },
      "outputs": [],
      "source": [
        "fig.write_html(\"scatterplot.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mtjERlKMEP8"
      },
      "outputs": [],
      "source": [
        "reducer = umap.UMAP(metric='euclidean',n_neighbors=10, min_dist = 0.002)\n",
        "embedding = reducer.fit_transform(data)\n",
        "embedding.shape\n",
        "fig, ax = plt.subplots()\n",
        "#size of the figure. I made it very large to accommodate country annotations.\n",
        "fig.set_size_inches(20, 20)\n",
        "ax.scatter(embedding[:, 0], embedding[:, 1], s=0.1)\n",
        "#script below adds annotations from the \"Country\" field of the datasets\n",
        "for i, txt in enumerate(countries.Country):\n",
        "    ax.annotate(txt, (embedding[:, 0][i], embedding[:, 1][i]),fontsize=8)\n",
        "#title of the figure\n",
        "plt.title('UMAP projection of Country Clustering by DFS Indicators', fontsize=24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz1I9PMeBpRC"
      },
      "source": [
        "# Fix missing Summary rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "327w8qdlTXJU"
      },
      "outputs": [],
      "source": [
        "master_df=pd.read_csv('/content/drive/MyDrive/master_embed.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hau0YggiFMBc"
      },
      "outputs": [],
      "source": [
        "master_df.iloc[:,10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EEjvFj8o2sd"
      },
      "outputs": [],
      "source": [
        "sampled_df.sample(n=11, random_state=1)[['summary','text','coding','codingd']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNQuJ-Z-CJTb"
      },
      "outputs": [],
      "source": [
        "articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cN__qeqFsu3"
      },
      "outputs": [],
      "source": [
        "articles2 = master_df[master_df.summary.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28YdYq5HFzsS"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykjfj8Y_FqlZ"
      },
      "outputs": [],
      "source": [
        "grouped_df = articles2.groupby('fulltext')['title'].first().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtXIptQWnoz_"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJRJSlmDv8D9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k4Uf-wUB7a5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "from retry import retry\n",
        "\n",
        "# define the function to process each excerpt\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt):\n",
        "    excerpt = ' '.join(excerpt.split()[:1500])\n",
        "    try:\n",
        "      completion = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\", temperature=0.1,\n",
        "          messages=[\n",
        "              {\"role\": \"system\", \"content\": \"You are summarizing news articles for a thematic analysis of attitudes towards refugees in news media.\"},\n",
        "              {\"role\": \"user\", \"content\": \"Return a one paragraph summary of the following article: \" + excerpt}\n",
        "          ]\n",
        "      )\n",
        "      print('done')\n",
        "      print(excerpt[:15])\n",
        "      print(completion.choices[0].message['content'])\n",
        "      return completion.choices[0].message['content']\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing row {idx}: {e}\")\n",
        "      return None\n",
        "\n",
        "# split the dataframe into segments\n",
        "num_segments = 2\n",
        "if 'segments' not in globals():\n",
        "  print('newsegs')\n",
        "  grouped_df['summary']=None\n",
        "  segment_size = len(grouped_df) // num_segments\n",
        "  segments = [grouped_df.iloc[i:i+segment_size] for i in range(0, len(grouped_df), segment_size)]\n",
        "\n",
        "# define a function to process each segment in parallel\n",
        "def process_segment(segment):\n",
        "    segment.loc[segment['summary'].isna(), 'summary'] = segment.loc[segment['summary'].isna(), 'fulltext'].apply(process_excerpt)\n",
        "    return segment\n",
        "# process each segment in parallel using concurrent.futures\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    # apply the process_segment function to each segment in parallel\n",
        "    results = list(executor.map(process_segment, segments))\n",
        "\n",
        "# merge the results and update the original dataframe\n",
        "grouped_df = pd.concat(results).sort_index()\n",
        "for idx, row in grouped_df[grouped_df['summary'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['fulltext'])\n",
        "    grouped_df.at[idx, 'summary'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQieD0FWHQR3"
      },
      "outputs": [],
      "source": [
        " articles2.drop('summary', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvyZrYYAIWWL"
      },
      "outputs": [],
      "source": [
        " articles2.drop('coding', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyJw0Y3sG7hD"
      },
      "outputs": [],
      "source": [
        " articles2 = grouped_df.merge(articles2, on=['title', 'fulltext'], how='outer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-67bvIq4IYRP"
      },
      "outputs": [],
      "source": [
        " articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yH0zm2ctG8w9"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv_6eMP-ITB5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.1\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are coding excerpts of news articles for a thematic analysis of attitudes towards refugees in news media in Malaysia.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,summary):\n",
        "    #print(excerpt)\n",
        "    try:\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article summarized here: ### '  + str(summary) + ' ### passage: ### ' + str(excerpt)+ ' ### In 12 words or less, give the theme of this specific passage as it embodies, relates to or reflects attitudes towards refugees in Malaysia, or return \"Irrelevant\"'  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(articles2, 10)]\n",
        "    for segment in segments:\n",
        "      segment['coding'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['coding'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','summary']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'coding'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).drop(['index'], axis=1).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['coding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    grouped_df.at[idx, 'coding'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in grouped_df[grouped_df['coding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    grouped_df.at[idx, 'coding'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7Z9uJo-KWox"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDDPK-zfKX8V"
      },
      "outputs": [],
      "source": [
        "\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "test = grouped_df.loc[~grouped_df['coding'].astype(str).str.contains('rrelevant')]\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.0\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are reviewing codings for excerpts of news articles for a thematic analysis of attitudes towards refugees in news media in Malaysia.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt):\n",
        "    #print(excerpt)\n",
        "    try:\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Is this code : ### ' + str(excerpt)+ ' ### too vague (True) to provide useful information about the theme of the passage, or is it adequate (False)? Good codes will identify specific themes relevant to attitudes towards refugees and the valence of those attitudes, if relevant. Example: \"Attitudes towards refugees in Malaysia\" is too vague as a code. Respond in one word, with a Python Boolean: True/False'  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(test, 10)]\n",
        "    for segment in segments:\n",
        "      segment['coding2'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['coding2'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding2' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['coding']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'coding2'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "test=pd.concat(segments)\n",
        "for idx, row in test[test['coding2'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['coding'])\n",
        "    print(result)\n",
        "    test.at[idx, 'coding2'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in test[test['coding2'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['coding'])\n",
        "    print(result)\n",
        "    test.at[idx, 'coding2'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOMN5b0gL-_X"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Semk7ejPMAmY"
      },
      "outputs": [],
      "source": [
        "\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "test = test.loc[test['coding2'].str.contains('rue')]\n",
        "test['codingold']=test['coding']\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.1\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are coding excerpts of news articles for a thematic analysis of attitudes towards refugees in news media in Malaysia.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,summary):\n",
        "    #print(excerpt)\n",
        "    try:\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article summarized here: ### '  + str(summary) + ' ### passage: ### ' + str(excerpt)+ ' ### In 18 words or less, give a theme of this specific passage as it embodies, relates to or reflects attitudes towards refugees in Malaysia. Include the valence of the attitudes and their target, if relevant.'  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(test, 10)]\n",
        "    for segment in segments:\n",
        "      segment['coding'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['coding'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','summary']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'coding'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "group2=pd.concat(segments).iloc[:,1:].reset_index()\n",
        "group2.set_index('index', inplace=True)\n",
        "for idx, row in group2[group2['coding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    group2.at[idx, 'coding'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "grouped_df.loc[group2.index, 'coding']=group2['coding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF420XMtO2Sz"
      },
      "outputs": [],
      "source": [
        "articles2['coding']=grouped_df['coding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8BD3kxxUhxI"
      },
      "outputs": [],
      "source": [
        "articles2['ada_embedding']=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpRMZeO9UZHy"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from retry import retry\n",
        "\n",
        "model=\"text-embedding-ada-002\"\n",
        "irel=openai.Embedding.create(input=['Irrelevant.'], model=model)['data'][0]['embedding']\n",
        "articles2['ada_embedding'] = articles2.apply(lambda row: irel, axis=1)\n",
        "articles2.loc[~articles2['coding'].str.contains('rrelevant'),'ada_embedding'] = None\n",
        "\n",
        "# define the function to apply to each row\n",
        "subarts=articles2[articles2['ada_embedding'].isnull()]\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(text):\n",
        "    try:\n",
        "      text = text.replace(\"\\n\", \" \")\n",
        "      if random.random() < 0.01:\n",
        "        print(iter)\n",
        "      return openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(subarts, 3)]\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['ada_embedding'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['coding']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'ada_embedding'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).reset_index()\n",
        "for idx, row in grouped_df[grouped_df['ada_embedding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['coding'])\n",
        "    grouped_df.at[idx, 'ada_embedding'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "articles2.loc[articles2['ada_embedding'].isnull(), 'ada_embedding'] = grouped_df.ada_embedding.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNMhTyWwNtTP"
      },
      "outputs": [],
      "source": [
        "articles2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti81YmkyPjHf"
      },
      "outputs": [],
      "source": [
        "master_df2 = master_df.merge(articles2, on=['title', 'fulltext','url','index','source','date','text','keywords','ada_embedding',\t'cluster',\t'dateold',\t'theme'], how='outer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olcVUEXcQfLN"
      },
      "outputs": [],
      "source": [
        "merged_df = master_df.merge(articles2, how='outer', on=['fulltext', 'text'], suffixes=('', '_y'))\n",
        "\n",
        "for col in ['summary', 'coding', 'ada_embedding']:\n",
        "    merged_df[col].fillna(merged_df[col+'_y'], inplace=True)\n",
        "\n",
        "merged_df = merged_df[merged_df.columns.drop(list(merged_df.filter(regex='_y')))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGJN4a6dVzZe"
      },
      "outputs": [],
      "source": [
        "merged_df[master_df.summary.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qq5LWTwUPLFR"
      },
      "outputs": [],
      "source": [
        "master_df.loc[master_df.summary.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ9dAH_7XsLy"
      },
      "outputs": [],
      "source": [
        "master_df=merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7OyW1RkXw1s"
      },
      "outputs": [],
      "source": [
        "master_df.to_csv('/content/drive/MyDrive/master_embed.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objcz_vmnrEr"
      },
      "source": [
        "# Theme summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIqPQxbMNL4W"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "#size of the figure. I made it very large to accommodate country annotations.\n",
        "fig.set_size_inches(20, 20)\n",
        "ax.scatter(embedding[:, 0], embedding[:, 1], s=0.1)\n",
        "#script below adds annotations from the \"Country\" field of the datasets\n",
        "for i, txt in enumerate(countries.Country):\n",
        "    ax.annotate(txt, (embedding[:, 0][i], embedding[:, 1][i]),fontsize=8)\n",
        "#title of the figure\n",
        "plt.title('UMAP projection of Country Clustering by DFS Indicators', fontsize=24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQXv_N5x6ZUe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRFvCjVAn0xN"
      },
      "outputs": [],
      "source": [
        "sampled_df = master_df.groupby('cluster').apply(lambda x: x.sample(n=8, random_state=1))\n",
        "sampled_df=sampled_df[sampled_df.cluster!=-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uef31RHToSOw"
      },
      "outputs": [],
      "source": [
        "master_df.groupby('Cluster').apply(lambda x: x.sample(n=5, random_state=1)).reset_index(drop=True)[['Cluster','text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewW-1zn9KnFN"
      },
      "outputs": [],
      "source": [
        "def combine_columns(row):\n",
        "    return f'Coded passage: {row[\"text\"]}. Coding: {row[\"codingb\"]}'\n",
        "    #return f'Coded passage: {row[\"text\"]}. Coding: {row[\"coding\"]}'\n",
        "\n",
        "sampled_df['combined'] = sampled_df.apply(combine_columns, axis=1)\n",
        "\n",
        "codelists = sampled_df.groupby(level='cluster')['combined'].apply(', '.join).to_frame()\n",
        "codelists['theme'] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sycCUqcEwSnS"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mszWditsoWJh"
      },
      "outputs": [],
      "source": [
        "\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.1\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are coding excerpts of news articles for a thematic analysis of attitudes towards refugees in news media in Malaysia.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(codelist):\n",
        "    #print(excerpt)\n",
        "    try:\n",
        "                return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article ### ' + str(excerpt)+ ' ### Step by step, answer the following questions: 1. Does the passage explicitly discuss refugees? 2. Is there an explicit Malaysia context? 3. Now, in 12 words or less, give the theme of this passage as it embodies, relates to or reflects attitudes towards refugees in Malaysia, or return “Irrelevant.” Now respond in the following Python dictionary format: {\"1. Refugees?\":True/False, \"Malaysia?\":True/False, \"Theme\":string}'  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(codelists, 1)]\n",
        "    for segment in segments:\n",
        "      segment['theme'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['theme'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[[0]]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'theme'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "codelists2=pd.concat(segments)\n",
        "for idx, row in codelists2[codelists2['theme'].isnull()].iterrows():\n",
        "    result = process_excerpt(row[0])\n",
        "    codelists2.at[idx, 'theme'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDukFmCR7zuo"
      },
      "outputs": [],
      "source": [
        "for idx, row in codelists2[codelists2['theme'].isnull()].iterrows():\n",
        "    result = process_excerpt(row[0])\n",
        "    codelists2.at[idx, 'theme'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ld1KSIUwtF1"
      },
      "outputs": [],
      "source": [
        "codelists['theme']=    codelists2['theme']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xa1MGsqfwuQ1"
      },
      "outputs": [],
      "source": [
        "codelists.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmqjsM7wB3Jq"
      },
      "outputs": [],
      "source": [
        "codelists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcSLzX-HCcoP"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPcMlHu2Fbng"
      },
      "outputs": [],
      "source": [
        "52377 r 52526\tMany are aged 13 to 17, but some are as young .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RidHVESqzIbt"
      },
      "outputs": [],
      "source": [
        "codelists.rename(columns = {'index':'new column name'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgG3loSJ7ada"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElyaxVM9yWoN"
      },
      "outputs": [],
      "source": [
        "master_df = pd.merge(master_df, codelists[['cluster', 'theme']], on='cluster', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDxQnc-_Wr6Z"
      },
      "outputs": [],
      "source": [
        "master_df.to_csv('/content/drive/MyDrive/master_embed.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRbkd_-HR0W9"
      },
      "outputs": [],
      "source": [
        "master_df = pd.merge(master_df, codelists[['cluster', 'theme']], on='cluster', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW845z0wDC4G"
      },
      "outputs": [],
      "source": [
        "master_df = pd.merge(master_df, codelists[['cluster', 'theme']], on='cluster', how='left')\n",
        "master_df.drop('theme_x', axis=1, inplace=True)\n",
        "master_df.rename(columns={'theme_y': 'theme'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykIx0yEDy6Nh"
      },
      "outputs": [],
      "source": [
        "master_df = pd.merge(master_df, codelists[['cluster', 'theme']], on='cluster', how='left')\n",
        "master_df.drop('theme_x', axis=1, inplace=True)\n",
        "master_df.rename(columns={'theme_y': 'theme'}, inplace=True)\n",
        "master_df.to_csv('/content/drive/MyDrive/master_embed.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arQFL9Td5hLz"
      },
      "outputs": [],
      "source": [
        "master_df=pd.read_csv('/content/drive/MyDrive/master_embed.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnzgt3DS62dq"
      },
      "outputs": [],
      "source": [
        "master_df.drop('theme_x', axis=1, inplace=True)\n",
        "master_df.rename(columns={'theme_y': 'theme'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnYxONha1yr3"
      },
      "outputs": [],
      "source": [
        "array(['malaymail', 'The Star', 'The Sun', 'Malaysiakini',\n",
        "       'The Malaysian Insight'], dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWHBj17b5Te5"
      },
      "outputs": [],
      "source": [
        "master_df['dateold']=master_df['date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5TQ8IRT5YdZ"
      },
      "outputs": [],
      "source": [
        "master_df['date']=master_df['dateold'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPU0AFC-1NMB"
      },
      "outputs": [],
      "source": [
        "master_df.loc[master_df.source=='Malaysiakini','date'].str.split('+').str[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqo_-Hfc_Ssd"
      },
      "outputs": [],
      "source": [
        "master_df[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd_9yoPkBRbb"
      },
      "outputs": [],
      "source": [
        "master_df['date']=master_df['dateold'].astype(str).str.split('+').str[0]\n",
        "master_df['date']=pd.to_datetime(master_df['date'], errors='coerce')\n",
        "#master_df['date'].apply(lambda t: t.replace(tzinfo=None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UukjfK-49XJj"
      },
      "outputs": [],
      "source": [
        "master_df[pd.isnull(master_df['date'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB7rYVW-8ha0"
      },
      "outputs": [],
      "source": [
        " master_df.groupby(['date', 'theme']).size().unstack(fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAvhsrQZK_VT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "master_df.loc[master_df.theme.isna(),'theme']='None'\n",
        "# First, we need to ensure 'date' is of datetime type and extract only the date part\n",
        "master_df['date'] = pd.to_datetime(master_df['date']).dt.date\n",
        "\n",
        "# Then, we create the new dataframe with counts for each theme and day.\n",
        "# We achieve this by using the groupby function and then unstacking the result.\n",
        "themecounts = master_df.groupby(['date', 'theme']).size().unstack(fill_value=0)\n",
        "themecounts = themecounts[5:]\n",
        "\n",
        "# Add a total column to sum across all theme counts for each day\n",
        "themecounts['total'] = themecounts.sum(axis=1)\n",
        "\n",
        "# Calculate percentage share for each theme\n",
        "themecounts_percentage = themecounts.copy()\n",
        "themes = themecounts_percentage.columns[:-1]  # Exclude the 'total' column\n",
        "for theme in themes:\n",
        "    themecounts_percentage[theme] = themecounts_percentage[theme] / themecounts_percentage['total']\n",
        "\n",
        "# Transpose the DataFrame\n",
        "transposed_themecounts=themecounts_percentage.transpose()\n",
        "\n",
        "# Add a 'total' row with sum across all dates\n",
        "transposed_themecounts.loc[:, 'total'] = transposed_themecounts.sum(axis=1)\n",
        "\n",
        "# Sort the DataFrame by total row\n",
        "sorted_themecounts = transposed_themecounts.sort_values(by='total', axis=0, ascending=False)\n",
        "\n",
        "# Print the sorted, transposed DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUgCRvcRvGxk"
      },
      "outputs": [],
      "source": [
        "sorted_themecounts.index[5:-1][0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCvirNvJRLMt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Select the top 10 themes (excluding the 'total' row)\n",
        "top_themes = sorted_themecounts.index[5:-1][0:10]\n",
        "\n",
        "# Calculate moving averages for each theme (excluding the 'total' row and column)\n",
        "moving_averages = sorted_themecounts.loc[top_themes, sorted_themecounts.columns != 'total'].rolling(window=100, min_periods=10, axis=1, win_type='gaussian').mean(std=5)\n",
        "\n",
        "# Create an empty figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add traces for each theme\n",
        "for theme in moving_averages.index:\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=moving_averages.columns,\n",
        "        y=moving_averages.loc[theme],\n",
        "        mode='lines',\n",
        "        name=theme\n",
        "    ))\n",
        "\n",
        "# Set layout properties\n",
        "fig.update_layout(\n",
        "    title=\"Moving Averages for Top 10 Themes\",\n",
        "    xaxis_title=\"Dates\",\n",
        "    yaxis_title=\"Moving Average\",\n",
        "    legend_title=\"Themes\"\n",
        ")\n",
        "\n",
        "# Show the figure\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM-n-YrTNX_k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Select the top 10 themes (excluding the 'total' row)\n",
        "top_themes = sorted_themecounts.index[5:-1][0:20]\n",
        "\n",
        "# Calculate moving averages for each theme (excluding the 'total' row and column)\n",
        "moving_averages = sorted_themecounts.loc[top_themes, sorted_themecounts.columns != 'total'].rolling(window=100, min_periods=10, axis=1, win_type='gaussian').mean(std=5)\n",
        "\n",
        "# Create an empty figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add traces for each theme\n",
        "for theme in moving_averages.index:\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=moving_averages.columns,\n",
        "        y=moving_averages.loc[theme],\n",
        "        mode='lines',\n",
        "        name=theme\n",
        "    ))\n",
        "\n",
        "# Set layout properties\n",
        "fig.update_layout(\n",
        "    title=\"Moving Averages for Top 10 Themes\",\n",
        "    xaxis_title=\"Dates\",\n",
        "    yaxis_title=\"Moving Average\",\n",
        "    legend_title=\"Themes\"\n",
        ")\n",
        "\n",
        "# Show the figure\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh3ycBc_P0rH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGWSjMYojl7T"
      },
      "outputs": [],
      "source": [
        "top_themes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRp-fnBdKgew"
      },
      "outputs": [],
      "source": [
        "sampled_df[(sampled_df.source=='The Sun')&(sampled_df.cluster==-1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3gmf4MhJtK5"
      },
      "outputs": [],
      "source": [
        "codelists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tavyAHz9Jqu0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGEx7NZv8jAW"
      },
      "source": [
        "# Alternative coding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q45bbU238tK-"
      },
      "outputs": [],
      "source": [
        "master_df=pd.read_csv('/content/drive/MyDrive/master_embedalt.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zVPvl-tnO2a"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLPyjj7pmdNc"
      },
      "outputs": [],
      "source": [
        "master_df.loc[master_df.title=='Rohingyas urged to get tested',['summary','text','codingb']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPkzDxktj5Gj"
      },
      "outputs": [],
      "source": [
        " master_df.to_csv('/content/drive/MyDrive/master_embedalt.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbjcA3dQ8yVt"
      },
      "outputs": [],
      "source": [
        "master_df=pd.read_csv('/content/drive/MyDrive/master_embedalt.csv')\n",
        "# Apply the sampling for each cluster\n",
        "master_df=master_df[master_df.text.str.len()>50]\n",
        "sampled_df = master_df.groupby('cluster').apply(lambda x: x.sample(n=4, random_state=1))\n",
        "\n",
        "# Sample 20 more rows where cluster == -1\n",
        "sampled_minus1 = master_df[master_df['cluster'] == -1].sample(n=16, random_state=1)\n",
        "\n",
        "# Sample 25 rows where cluster == NaN\n",
        "sampled_nan = master_df[master_df['cluster'].isna()].sample(n=20, random_state=1)\n",
        "\n",
        "# Append the sampled dfs\n",
        "sampled_df = sampled_df.append([sampled_minus1, sampled_nan], ignore_index=True).sort_values('cluster')\n",
        "\n",
        "sampled_df\n",
        "#sampled_df.to_csv(\"/content/drive/MyDrive/testsubset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gp0_TLN-jIC"
      },
      "outputs": [],
      "source": [
        "sampled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFsux-uDLIaN"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM0S9rS3-Pcn"
      },
      "outputs": [],
      "source": [
        "\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.0\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are coding excerpts of news articles for a thematic analysis of attitudes towards refugees in news media in Malaysia.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,summary):\n",
        "    #print(excerpt)\n",
        "    try:\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article. Summary of full article:  ### '  + str(summary) + ' ### specific passage to analyze: ### ' + str(excerpt)+ ' ### Give the theme of this SPECIFIC passage (not the summary as a whole) as it embodies, relates to or reflects attitudes towards refugees in Malaysia, or return \"Irrelevant\". Before answering, analyze step by step: 1. Is this passage relevant to attitudes to refugees? True/False. If false, \"N/A\" for all further questions. If true, 2. Whose attitudes are being reflected? Examples: the government, Malaysians, NGOs, the author. 3. Who is the target of the attitudes? Examples: the Rohingya, the government, UNHCR. Finally, 4. in 15 words or less, return the theme (if relevant) as it relates to attitudes towards refugees in Malaysia . Respond ONLY in the following python dictionary format: {\"1. Relevant?\": True/False, \"2. Whose Attitude?\":stringval2,\"3. Target\":stringval3,\"4. Theme\":stringval4}'  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(master_df, 25)]\n",
        "    for segment in segments:\n",
        "      segment['codingb'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['codingb'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','summary']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'codingb'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).drop(['index'], axis=1).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['codingb'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    grouped_df.at[idx, 'codingb'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in grouped_df[grouped_df['codingb'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary'])\n",
        "    grouped_df.at[idx, 'codingb'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kFr9oewJ4oO"
      },
      "outputs": [],
      "source": [
        "grouped_df[['summary','coding','codingb']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lWb-6K7KG_Y"
      },
      "outputs": [],
      "source": [
        "    grouped_df.to_csv('/content/drive/MyDrive/master_embedalt.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbjwDrsro7js"
      },
      "outputs": [],
      "source": [
        "articles3=pd.read_csv('/content/drive/MyDrive/master_embedalt.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-rfcW5UpZ6W"
      },
      "outputs": [],
      "source": [
        "articles3.loc[50000:,['codingb','text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZHCjb94rlo3"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTesi4JOdigM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s3CbQNXpMaL"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from retry import retry\n",
        "\n",
        "model=\"text-embedding-ada-002\"\n",
        "irel=openai.Embedding.create(input=['Irrelevant.'], model=model)['data'][0]['embedding']\n",
        "articles3['ada_embedding'] = articles3.apply(lambda row: irel, axis=1)\n",
        "articles3.loc[~articles3['coding'].str.contains('rrelevant'),'ada_embedding'] = None\n",
        "\n",
        "# define the function to apply to each row\n",
        "subarts=articles3[articles3['ada_embedding'].isnull()]\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(text):\n",
        "    try:\n",
        "      text = text.replace(\"\\n\", \" \")\n",
        "      if random.random() < 0.01:\n",
        "        print(iter)\n",
        "      return openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(subarts, 25)]\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['ada_embedding'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['codingb']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'ada_embedding'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).reset_index()\n",
        "for idx, row in grouped_df[grouped_df['ada_embedding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['codingb'])\n",
        "    grouped_df.at[idx, 'ada_embedding'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "articles3.loc[articles3['ada_embedding'].isnull(), 'ada_embedding'] = grouped_df.ada_embedding.values\n",
        "#articles3.to_csv(\"/content/drive/MyDrive/kiniembed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pugOtX2Kw4T0"
      },
      "outputs": [],
      "source": [
        "for idx, row in grouped_df[grouped_df['ada_embedding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['codingb'])\n",
        "    grouped_df.at[idx, 'ada_embedding'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "articles3.loc[articles3['ada_embedding'].isnull(), 'ada_embedding'] = grouped_df.ada_embedding.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3i-TmIl2Cek"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "# Assuming `articles3.ada_embedding.values` is a numpy array of strings\n",
        "str_array = articles3.ada_embedding.values\n",
        "\n",
        "# Use ast.literal_eval to safely evaluate each string into a list\n",
        "list_array = [ast.literal_eval(s) for s in str_array]\n",
        "\n",
        "# Convert the list of lists to a 2D numpy array\n",
        "data = np.array(list_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVQlK_CKYBgc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "str_array = master_df.ada_embedding2.values\n",
        "list_array = [ast.literal_eval(s) for s in str_array]\n",
        "data = np.array(list_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhUoKduWZlP4"
      },
      "outputs": [],
      "source": [
        "data = np.array(list_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQqPCdi1w7ou"
      },
      "outputs": [],
      "source": [
        "master_df2= articles3.loc[~articles3['coding'].str.contains('rrelevant')].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0288dtQ2fmP"
      },
      "outputs": [],
      "source": [
        "data = [np.array(x) for x in master_df.ada_embedding2.values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nAVCedVhG_Y"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_718_LLhH_9"
      },
      "source": [
        "# Manual Coding Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfxaIje1hM6g"
      },
      "outputs": [],
      "source": [
        "Full Text\tLink\tTitle\tSummary\tText to Code\tCluster\tChatGPT Coding\tAssessment (Strongly Disagree, Disagree, Unsure, Agree, Strongly Agree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeC3fkQ3hgfb"
      },
      "outputs": [],
      "source": [
        "master_df2=pd.read_csv('/content/drive/MyDrive/master_embed.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5FUKuIfhhda"
      },
      "outputs": [],
      "source": [
        "review=pd.read_csv('/content/drive/MyDrive/codingreview.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs-93idniMsi"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnSaZFmPxtQ7"
      },
      "outputs": [],
      "source": [
        "review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E43Rf-9ghzFa"
      },
      "outputs": [],
      "source": [
        "review=pd.read_csv('/content/drive/MyDrive/rawcodes.csv')\n",
        "merged = review.merge(master_df[['fulltext', 'text', 'Cluster']],\n",
        "                      how='left',\n",
        "                      on=['fulltext', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4upXYc5S2wxb"
      },
      "outputs": [],
      "source": [
        "master_df2[master_df2['text'].str.contains('Bernama spoke to several female students at Pelangi Kasih, who admitted to knowing several girls who got married when they were still underage.')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwMi9doA4-l5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcBY8akw5AvR"
      },
      "outputs": [],
      "source": [
        "duplicates_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owYAhJuG43XC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I4HLPqv0Rld"
      },
      "outputs": [],
      "source": [
        "merged[['text','url','title','Cluster']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSkjOwN80HUo"
      },
      "outputs": [],
      "source": [
        "duplicates_df = master_df[master_df.duplicated(subset=['fulltext', 'text'], keep=False)]\n",
        "\n",
        "# Sort the duplicates dataframe by 'text'\n",
        "duplicates_df = duplicates_df.sort_values(by='text')\n",
        "\n",
        "# Print the duplicates dataframe\n",
        "duplicates_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJTUOMsn1Kxn"
      },
      "outputs": [],
      "source": [
        "# Sort the dataframe by 'fulltext', 'text', and 'url', considering NaN values last\n",
        "master_df = master_df.sort_values(by=['fulltext', 'text', 'url'], key=lambda col: col.fillna(''))\n",
        "\n",
        "# Drop the duplicate rows, keeping the first nonempty 'url'\n",
        "master_df = master_df.drop_duplicates(subset=['fulltext', 'text'], keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux5fXJpBzgAd"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKw8DCOji2HF"
      },
      "outputs": [],
      "source": [
        "subset_master_df = master_df[['coding', 'text', 'codingb']]\n",
        "\n",
        "# Merge codingreview with the subset of master_df\n",
        "review = review.merge(subset_master_df, on=['coding', 'text'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jgYgOBdi8Nn"
      },
      "outputs": [],
      "source": [
        "review.loc[:,['summary','text','coding','codingb']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3OjAbhS65lL"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkulbvmc-KbW"
      },
      "outputs": [],
      "source": [
        "reviewfull=review.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsyKjajQ-NTE"
      },
      "outputs": [],
      "source": [
        "review=reviewfull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIJROnB6C93l"
      },
      "outputs": [],
      "source": [
        "review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhj12NffdfS1"
      },
      "outputs": [],
      "source": [
        "review.to_csv('/content/drive/MyDrive/codingreview.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9VLRYydtNy3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6EN8XlNjNg3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "from retry import retry\n",
        "\n",
        "# define the function to process each excerpt\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt):\n",
        "    excerpt = ' '.join(excerpt.split()[:1500])\n",
        "    try:\n",
        "      completion = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\", temperature=0.0,\n",
        "          messages=[\n",
        "              {\"role\": \"system\", \"content\": \"You are summarizing news articles for a thematic analysis of attitudes towards refugees in news media.\"},\n",
        "              {\"role\": \"user\", \"content\": \"Read this article ### \" + excerpt + \"### Now, return a terse summary of the article in no more than 35 words: \" }\n",
        "          ]\n",
        "      )\n",
        "      print('done')\n",
        "      print(excerpt[:15])\n",
        "      print(completion.choices[0].message['content'])\n",
        "      return completion.choices[0].message['content']\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing row {idx}: {e}\")\n",
        "      return None\n",
        "\n",
        "# split the dataframe into segments\n",
        "num_segments = 1\n",
        "if 'segments' not in globals():\n",
        "  print('newsegs')\n",
        "  review['summary2']=None\n",
        "  segment_size = len(review) // num_segments\n",
        "  segments = [review.iloc[i:i+segment_size] for i in range(0, len(review), segment_size)]\n",
        "\n",
        "# define a function to process each segment in parallel\n",
        "def process_segment(segment):\n",
        "    segment.loc[segment['summary2'].isna(), 'summary2'] = segment.loc[segment['summary2'].isna(), 'fulltext'].apply(process_excerpt)\n",
        "    return segment\n",
        "# process each segment in parallel using concurrent.futures\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    # apply the process_segment function to each segment in parallel\n",
        "    results = list(executor.map(process_segment, segments))\n",
        "\n",
        "# merge the results and update the original dataframe\n",
        "review = pd.concat(results).sort_index()\n",
        "for idx, row in review[review['summary2'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['fulltext'])\n",
        "    review.at[idx, 'summary2'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAHFfKcC7aC9"
      },
      "outputs": [],
      "source": [
        "segments[summary2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFc9hqmm6do6"
      },
      "outputs": [],
      "source": [
        "for idx, row in review[review['summary2'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['fulltext'])\n",
        "    reviewf.at[idx, 'summary2'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lOBxuQF6gCM"
      },
      "outputs": [],
      "source": [
        "review[['summary','summary2']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26wN-Gvjv-9v"
      },
      "outputs": [],
      "source": [
        "review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNmkSGv6qEFt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C45Y9fyEzm6M"
      },
      "outputs": [],
      "source": [
        "{\"1. Relevant?\": True}['1. Relevant?']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYBFZ1BOpR4E"
      },
      "outputs": [],
      "source": [
        "del segments\n",
        "review['codingc']=None\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "temperature = 0.0\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are coding passages from news articles for a thematic analysis of attitudes towards refugees in news media in Malaysia.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,summary2):\n",
        "    #print(excerpt)\n",
        "    try:\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Summary of full article to provide context for the passage, but not to be used to answer the question:  ### '  + str(summary2) + ' ### Does the text of the following passage: ### ' + str(excerpt)+ '  specifically reveal something about attitudes towards refugees in Malaysia? Examples: if the summary discusses refugees but the passage only gives ancillary information like “The court date was set for 15 June” or “Prof. Yi is faculty at UM”, return False. Respond ONLY in the following python dictionary format: {\"1. Passage specifically about refugees in Malaysia?\": True/False}'  }]).choices[0].message['content']\n",
        "    except Exception as e:###\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(review,2)]\n",
        "    for segment in segments:\n",
        "      segment['codingc'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['codingc'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','summary2']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'codingc'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "#['level_0','index']\n",
        "grouped_df= pd.concat(segments).sort_index()\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['codingc'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary2'])\n",
        "    grouped_df.at[idx, 'codingc'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in grouped_df[grouped_df['codingc'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary2'])\n",
        "    grouped_df.at[idx, 'codingc'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "def extract_relevant(x):\n",
        "    try:\n",
        "        return eval(x).get(\"1. Passage specifically about refugees in Malaysia?\", np.nan) if isinstance(eval(x), dict) else np.nan\n",
        "    except (SyntaxError, TypeError, NameError):\n",
        "        return np.nan\n",
        "\n",
        "grouped_df['extract'] = grouped_df['codingc'].apply(extract_relevant)\n",
        "grouped_df.loc[grouped_df.extract==False,['summary2','text','codingb']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbsqB7r23okR"
      },
      "outputs": [],
      "source": [
        "grouped_df.loc[grouped_df.extract==False,['summary2','text','codingb']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ornofrq921ST"
      },
      "outputs": [],
      "source": [
        "2 5 5 8 12 22 23 27 29 39 53"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCjg_Vx-slps"
      },
      "outputs": [],
      "source": [
        "grouped_df['extract'] = grouped_df['codingc'].apply(lambda x: eval(x)[\"1. Relevant?\"])\n",
        "grouped_df.loc[grouped_df.extract==False,['summary2','text','codingb']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0u0cIA-2QJs"
      },
      "outputs": [],
      "source": [
        "5 is wrong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XefA6dGSqfxj"
      },
      "outputs": [],
      "source": [
        "grouped_df= pd.concat(segments).sort_index()\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['codingc'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary2'])\n",
        "    grouped_df.at[idx, 'codingc'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in grouped_df[grouped_df['codingc'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'], row['summary2'])\n",
        "    grouped_df.at[idx, 'codingc'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebvJf4Pap916"
      },
      "outputs": [],
      "source": [
        " grouped_df[['summary2','text','codingb','codingc']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1pYwWiG6Mds"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvBbWgsC3-ig"
      },
      "source": [
        "# Iteration 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhD7ac_B5pGi"
      },
      "source": [
        "Clean master_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaAaaasx6KFI"
      },
      "source": [
        "1. Fix malaymail urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb0qohLU5-gw"
      },
      "outputs": [],
      "source": [
        "master_df=pd.read_csv('/content/drive/MyDrive/master_embed.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fsgt5pPHtFl"
      },
      "outputs": [],
      "source": [
        "master_df.to_csv('/content/drive/MyDrive/master_embed2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBX6edrh8ZZs"
      },
      "outputs": [],
      "source": [
        "master_df[(master_df.source=='malaymail')][['url','title']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl7V9oPl7-NT"
      },
      "outputs": [],
      "source": [
        "master_df[['title','url','text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S916-o6__JD"
      },
      "outputs": [],
      "source": [
        "find_duplicates(url_to_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQqeEmNL-jgz"
      },
      "outputs": [],
      "source": [
        "urllist[url_to_title .duplicated(subset=[0], keep=False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0vWm3Pb7KcN"
      },
      "outputs": [],
      "source": [
        "urllist = pd.read_csv('/content/drive/MyDrive/malaymailurls.txt',header=None)\n",
        "!pip install thefuzz\n",
        "from thefuzz import fuzz\n",
        "from urllib.parse import unquote\n",
        "\n",
        "# Function to extract title from url\n",
        "def extract_title(url):\n",
        "    # Extract the part of the url that represents the title\n",
        "    title = url.split('/')[-2]\n",
        "\n",
        "    # Replace the '-' or '_' in the title with space\n",
        "    title = title.replace('-', ' ').replace('_', ' ')\n",
        "\n",
        "    # Decode the url encoded title\n",
        "    title = unquote(title)\n",
        "\n",
        "    return title\n",
        "\n",
        "# Create the mapping dictionary\n",
        "url_to_title = {url : extract_title(url) for url in urllist[0]}\n",
        "\n",
        "MMURLS = master_df[master_df.source=='malaymail'].title.unique()\n",
        "\n",
        "# Initialize dictionary with 'title' as keys and None as values for 'url'\n",
        "data = {'title': MMURLS, 'url': [None] * len(MMURLS)}\n",
        "\n",
        "# DataFrame to hold titles and their best matching urls\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Function to find the best matching url based on title\n",
        "def get_matching_url(title):\n",
        "    max_score = -1\n",
        "    matching_url = ''\n",
        "\n",
        "    for url , url_title in url_to_title.items():\n",
        "        score = fuzz.ratio(title.lower(), url_title.lower())\n",
        "\n",
        "        if score > max_score and score > 70:\n",
        "            max_score = score\n",
        "            matching_url = url\n",
        "\n",
        "    if max_score > 70:\n",
        "        url_to_title.pop(matching_url)\n",
        "        return matching_url\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Apply the function to the 'title' column\n",
        "df['url'] = df['title'].apply(get_matching_url)\n",
        "\n",
        "# Extract rows where source is 'malaymail'\n",
        "temp_df = master_df[master_df['source'] == 'malaymail'].copy()\n",
        "\n",
        "# Merge the URLs from df into temp_df\n",
        "temp_df = temp_df.merge(df, on='title', how='left', suffixes=('', '_new'))\n",
        "\n",
        "# If there is a new URL, use it, otherwise keep the old URL\n",
        "temp_df['url'] = temp_df['url_new'].combine_first(temp_df['url'])\n",
        "\n",
        "# Drop the extra column 'url_new'\n",
        "temp_df = temp_df.drop(columns='url_new')\n",
        "\n",
        "# Get indices where source is not 'malaymail'\n",
        "other_indices = master_df[master_df['source'] != 'malaymail'].index\n",
        "\n",
        "# Concatenate the temp_df and the part of master_df where source is not 'malaymail'\n",
        "master_df = pd.concat([master_df.loc[other_indices], temp_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEb_JZQlDWHo"
      },
      "source": [
        "Remove dupes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De4LoRfPHpg7"
      },
      "outputs": [],
      "source": [
        "duplicate_columns = [\"fulltext\", \"text\"]\n",
        "master_df.drop_duplicates(subset=duplicate_columns, keep='first', inplace=True)\n",
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPFNN5997W_4"
      },
      "outputs": [],
      "source": [
        "master_df[['title','url','text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knjAWmDp9yJa"
      },
      "outputs": [],
      "source": [
        "duplicates_df = master_df[master_df.duplicated(subset=['fulltext', 'text'], keep=False)]\n",
        "\n",
        "# Sort the duplicates dataframe by 'text'\n",
        "duplicates_df = duplicates_df.sort_values(by='text')\n",
        "\n",
        "# Print the duplicates dataframe\n",
        "duplicates_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VjGHQsWD5Lj"
      },
      "source": [
        "Remove short passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTxBXpjsEAer"
      },
      "outputs": [],
      "source": [
        "master_df=master_df[master_df.text.str.len()>100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1PpeZnl5_yS"
      },
      "source": [
        "Remove obviously irrelevant content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqblZtWCSoIe"
      },
      "outputs": [],
      "source": [
        "openai.Embedding.create(input=['blah'], model=model)['data'][0]['embedding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT1Ml9xUE6JT"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OYd7SdDEz4_"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from retry import retry\n",
        "\n",
        "model=\"text-embedding-ada-002\"\n",
        "# define the function to apply to each row\n",
        "master_df['ada_embedding2']=None\n",
        "subarts=master_df\n",
        "\n",
        "@retry(delay=3, tries=2)\n",
        "def process_excerpt(text):\n",
        "    try:\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        api_key = random.choice([samkey, melkey])\n",
        "        openai.api_key = api_key\n",
        "        output = openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing excerpt: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(subarts, 25)]\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['ada_embedding2'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=18) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'ada_embedding2'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments)\n",
        "for idx, row in grouped_df[grouped_df['ada_embedding2'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'])\n",
        "    grouped_df.at[idx, 'ada_embedding2'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "master_df.loc[master_df['ada_embedding2'].isnull(), 'ada_embedding2'] = grouped_df.ada_embedding2.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RB5YxtJtINY"
      },
      "outputs": [],
      "source": [
        "grouped_df['ada_embedding2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is1UY47u08wt"
      },
      "outputs": [],
      "source": [
        "string_elements = [element for element in data if isinstance(element, str)]\n",
        "\n",
        "print(\"String elements in the list:\", string_elements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cZgWNQJ1gX6"
      },
      "outputs": [],
      "source": [
        "def find_string_indices(data):\n",
        "    string_indices = [index for index, element in enumerate(data) if isinstance(element, str) or (isinstance(element, list) and any(isinstance(item, str) for item in element))]\n",
        "    return string_indices\n",
        "\n",
        "\n",
        "# Example usage\n",
        "data = ['hello', [1, 2, 3], 'world', ['embedded', 'in', 'string'], [4, 5]]\n",
        "indices = find_string_indices(data)\n",
        "print(indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibsFEdbCJ6u6"
      },
      "outputs": [],
      "source": [
        "data = master_df.ada_embedding2.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H1Zvv3a58x3"
      },
      "outputs": [],
      "source": [
        "\n",
        "#create fine clusters to check for droppable clusters\n",
        "reducer = umap.UMAP(n_components=2,metric='cosine',n_neighbors=40, min_dist = 0.0001)\n",
        "embedding = reducer.fit_transform(data)\n",
        "embedding.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hSyfAkfKIof"
      },
      "outputs": [],
      "source": [
        "import hdbscan\n",
        "hdbscan = hdbscan.HDBSCAN(min_cluster_size=20, min_samples = 1)\n",
        "labels = hdbscan.fit_predict(embedding)\n",
        "hdbscan.condensed_tree_.plot(select_clusters=True)\n",
        "np.unique(labels)\n",
        "###fine clusters for manual removal of irrelevant content: min_clust 20 min_samp 1, 389 clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN2c-r9u5rmT"
      },
      "outputs": [],
      "source": [
        "master_df['Cluster'] = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXZZ34k67fmI"
      },
      "outputs": [],
      "source": [
        "master_df.Cluster.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No5r4aAlCjYa"
      },
      "outputs": [],
      "source": [
        "master_df.groupby('Cluster').apply(lambda x: x.sample(n=5, random_state=1)).reset_index(drop=True)[['Cluster','text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDOCW8P7NN2T"
      },
      "outputs": [],
      "source": [
        "master_df = master_df[~master_df['Cluster'].isin([0, 1, 2, 3, 4, 5, 6, 7, 9,10])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCQX7umrDAgl"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJcp06gv8H4R"
      },
      "source": [
        "Find partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMKIDTdQ8Kjd"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "matrix = np.vstack(embedding)\n",
        "n_clusters = 400\n",
        "\n",
        "kmeans = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\n",
        "kmeans.fit(matrix)\n",
        "master_df['Cluster'] = kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm46OkBhDv4k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_centroids(labels, embedding):\n",
        "    centroids = {}\n",
        "    for label in np.unique(labels):\n",
        "        centroids[label] = embedding[labels == label].mean(axis=0)\n",
        "    return centroids\n",
        "\n",
        "def create_distance_matrix(centroids):\n",
        "    # Extract the centroids into a list and sort by cluster label to ensure ordering\n",
        "    centroid_list = [centroids[key] for key in sorted(centroids)]\n",
        "    # Convert list of centroids to a NumPy array\n",
        "    centroid_array = np.array(centroid_list)\n",
        "    # Calculate the distance matrix\n",
        "    distance_matrix = np.sqrt(((centroid_array[:, np.newaxis, :] - centroid_array[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
        "    return distance_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN6mZYyeEfMn"
      },
      "outputs": [],
      "source": [
        "# To find the centroids of each cluster:\n",
        "import numpy as np\n",
        "\n",
        "def calculate_centroids(labels, embedding):\n",
        "    centroids = {}\n",
        "    for label in np.unique(labels):\n",
        "        centroids[label] = embedding[labels == label].mean(axis=0)\n",
        "    return centroids\n",
        "def create_distance_matrix(centroids):\n",
        "    # Extract the centroids into a list and sort by cluster label to ensure ordering\n",
        "    centroid_list = [centroids[key] for key in sorted(centroids)]\n",
        "    # Convert list of centroids to a NumPy array\n",
        "    centroid_array = np.array(centroid_list)\n",
        "    # Calculate the distance matrix\n",
        "    distance_matrix = np.sqrt(((centroid_array[:, np.newaxis, :] - centroid_array[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
        "    return distance_matrix\n",
        "def rank_distances(distance_matrix):\n",
        "    # Use argsort which returns the indices that would sort the array\n",
        "    # Since each cluster's self distance is 0, it will naturally come first\n",
        "    ranked_indices = np.argsort(np.argsort(distance_matrix, axis=1), axis=1)\n",
        "    return ranked_indices\n",
        "\n",
        "# Now calculate the centroids\n",
        "centroids = calculate_centroids(kmeans.labels_, embedding)\n",
        "#centroids\n",
        "ord=rank_distances(create_distance_matrix(centroids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3IB9ggiNJH_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize colors array with NaN values\n",
        "colors = np.full(shape=(400,), fill_value=np.nan)  # assuming 400 clusters\n",
        "\n",
        "# Assuming 'ranks' is the rank matrix with shape (400, 400)\n",
        "for i in range(len(colors)):\n",
        "    # List of available colors\n",
        "    curcolors = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
        "    print('cluster ', i )\n",
        "    # Iterate through the ranked indices until you find 9 non-self indices\n",
        "    for j in ord[i]:\n",
        "\n",
        "        if j == i or ord[i,j] >= 8:  # Skip the cluster itself and indices >= 9\n",
        "            continue\n",
        "\n",
        "        # Check if the color at this index is already assigned\n",
        "        if not np.isnan(colors[j]) and colors[j] in curcolors:\n",
        "            # Remove the color from the current available list\n",
        "            print('column ', j)\n",
        "            curcolors.remove(colors[j])\n",
        "\n",
        "    # Randomly choose from the remaining available colors\n",
        "    if curcolors:  # Check if there are any colors left to assign\n",
        "        colors[i] = np.random.choice(curcolors)\n",
        "\n",
        "# Ensure all clusters have been assigned a color\n",
        "assert not np.any(np.isnan(colors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x73FpkHUL3W"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "labels= kmeans.labels_\n",
        "# Assuming 'embedding' is your array of positions with shape (n_samples, n_features)\n",
        "# 'labels' is your array of cluster labels from kmeans.labels_\n",
        "# 'colors' is the array of color indices you have assigned to each cluster\n",
        "\n",
        "# Define a list of distinct colors to use for plotting\n",
        "distinct_colors = [\n",
        "    'red', 'green', 'blue', 'yellow', 'orange', 'purple', 'pink', 'brown', 'grey'\n",
        "]\n",
        "\n",
        "# Assign a color to each cluster label based on the color indices\n",
        "cluster_colors = [distinct_colors[int(color_index)] for color_index in colors]\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(20, 16))\n",
        "for cluster_label in np.unique(labels):\n",
        "    # Find the indices of points in this cluster\n",
        "    indices = np.where(labels == cluster_label)\n",
        "    # Extract the embeddings of these points\n",
        "    cluster_embedding = embedding[indices]\n",
        "    # Plot these points with the assigned color\n",
        "    plt.scatter(cluster_embedding[:, 0], cluster_embedding[:, 1],\n",
        "                label=f'Cluster {cluster_label}',\n",
        "                color=cluster_colors[cluster_label],\n",
        "                alpha=1,s=2)\n",
        "\n",
        "plt.title('Cluster Visualization')\n",
        "plt.xlabel('Embedding Dimension 1')\n",
        "plt.ylabel('Embedding Dimension 2')\n",
        "plt.xlim(9, 11)  # Crop the x-axis to the range [6, 14]\n",
        "plt.ylim(10,12)  # Crop the y-axis to the range [4, 14]\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWXP89p8-tey"
      },
      "outputs": [],
      "source": [
        "import feedparser\n",
        "!pip install chromadb\n",
        "import chromadb\n",
        "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK6JukuIXrCW"
      },
      "outputs": [],
      "source": [
        "datatest=data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aoavVxrRvgl"
      },
      "outputs": [],
      "source": [
        "#create fine clusters to check for droppable clusters\n",
        "reducer = umap.UMAP(n_components=4,metric='cosine',n_neighbors=100, min_dist = 0.0001)\n",
        "embedding = reducer.fit_transform(data)\n",
        "embedding.shape\n",
        "!pip install hdbscan\n",
        "\n",
        "import hdbscan\n",
        "hdbscan = hdbscan.HDBSCAN(min_cluster_size=20, min_samples = 1)\n",
        "labels = hdbscan.fit_predict(embedding)\n",
        "hdbscan.condensed_tree_.plot(select_clusters=True)\n",
        "np.unique(labels)\n",
        "###fine clusters for manual removal of irrelevant content: min_clust 20 min_samp 1, 389 clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_P06WLfA7gQ"
      },
      "outputs": [],
      "source": [
        "import hdbscan\n",
        "hdbscan = hdbscan.HDBSCAN(min_cluster_size=20, min_samples = 1)\n",
        "labels = hdbscan.fit_predict(embedding)\n",
        "hdbscan.condensed_tree_.plot(select_clusters=True)\n",
        "np.unique(labels)\n",
        "###fine clusters for manual removal of irrelevant content: min_clust 20 min_samp 1, 389 clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0B-6kr8R0kY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR0dr_QhHNYW"
      },
      "outputs": [],
      "source": [
        "read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1LDulcxSL5B"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o9RRCeGJGgi"
      },
      "outputs": [],
      "source": [
        "master_df=pd.read_csv('/content/drive/MyDrive/master_embed2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeYcsAysF8y9"
      },
      "outputs": [],
      "source": [
        "master_df.to_csv('/content/drive/MyDrive/master_embed2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MizsmFveHElI"
      },
      "outputs": [],
      "source": [
        "master_df[['url','summary','text','coding']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA4CP53XbYC6"
      },
      "outputs": [],
      "source": [
        "master_df=pd.read_csv('/content/drive/MyDrive/master_embed2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvbXmwx98ttF"
      },
      "outputs": [],
      "source": [
        "master_df.iloc[:,10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkWhLz328eC8"
      },
      "outputs": [],
      "source": [
        "master_df.loc[master_df['date'].astype('datetime64[ns]').groupby(master_df['source']).idxmax()][['source', 'date']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BGcBHgvbp9J"
      },
      "outputs": [],
      "source": [
        "master_df['source'].unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZo2yI49cF-W"
      },
      "outputs": [],
      "source": [
        "embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbLfmv7_d6TJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaVLFov7KjAx"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(grouped_df, missing[['fulltext', 'summary']], on='fulltext', how='left')\n",
        "\n",
        "# if 'summary_x' is null (i.e. was missing in master_df), fill with 'summary_y' (from missing df)\n",
        "merged_df['summary_x'] = merged_df['summary_x'].fillna(merged_df['summary_y'])\n",
        "\n",
        "# now drop the 'summary_y' column as it's not needed\n",
        "merged_df = merged_df.drop(columns=['summary_y'])\n",
        "\n",
        "# rename 'summary_x' back to 'summary'\n",
        "merged_df = merged_df.rename(columns={'summary_x': 'summary'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAsNnCugkgFj"
      },
      "outputs": [],
      "source": [
        "grouped_df=merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYzof-1YF76P"
      },
      "outputs": [],
      "source": [
        "#fix missing summary values\n",
        "missing = pd.DataFrame(master_df[master_df.summary.isna()].fulltext.unique(), columns=['fulltext'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x47K1Pajelg"
      },
      "outputs": [],
      "source": [
        "#fix missing summary values\n",
        "missing = pd.DataFrame(grouped_df[grouped_df.summary.isna()].fulltext.unique(), columns=['fulltext'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiF2_QkrKw0g"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(grouped_df, missing[['fulltext', 'summary']], on='fulltext', how='left')\n",
        "merged_df['summary_x'] = merged_df['summary_x'].fillna(merged_df['summary_y'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1u9hXn_KlIX"
      },
      "outputs": [],
      "source": [
        "grouped_df = merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRNh92ryjlhu"
      },
      "outputs": [],
      "source": [
        "missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bpNt4NlcoQD"
      },
      "outputs": [],
      "source": [
        "def process_excerpt(excerpt):\n",
        "    excerpt = ' '.join(excerpt.split()[:1500])\n",
        "    try:\n",
        "      completion = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo-0613\", temperature=0.0,\n",
        "          messages=[\n",
        "              {\"role\": \"system\", \"content\": \"You are summarizing news articles for a thematic analysis of attitudes towards refugees in news media.\"},\n",
        "              {\"role\": \"user\", \"content\": \"Read this article ### \" + excerpt + \"### Now, return a terse summary of the article in no more than 35 words: \" }\n",
        "          ]\n",
        "      )\n",
        "      print('done')\n",
        "      print(excerpt[:15])\n",
        "      print(completion.choices[0].message['content'])\n",
        "      return completion.choices[0].message['content']\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing row {idx}: {e}\")\n",
        "      return None\n",
        "# Apply the process_excerpt function to each row in 'fulltext' column\n",
        "missing['summary'] = missing['fulltext'].apply(lambda x: process_excerpt(x))\n",
        "\n",
        "# Print the updated 'missing' dataframe\n",
        "print(missing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATGfOjqZj8ba"
      },
      "outputs": [],
      "source": [
        "missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK926krljyYS"
      },
      "outputs": [],
      "source": [
        "missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZudGXklu4B1S"
      },
      "outputs": [],
      "source": [
        "# Apply the sampling for each cluster\n",
        "master_df=master_df[master_df.text.str.len()>100]\n",
        "sampled_df = master_df.groupby('cluster').apply(lambda x: x.sample(n=4, random_state=1))\n",
        "\n",
        "# Sample 20 more rows where cluster == -1\n",
        "sampled_minus1 = master_df[master_df['cluster'] == -1].sample(n=10, random_state=1)\n",
        "\n",
        "# Sample 25 rows where cluster == NaN\n",
        "sampled_nan = master_df[master_df['cluster'].isna()].sample(n=20, random_state=1)\n",
        "\n",
        "# Append the sampled dfs\n",
        "sampled_df = sampled_df.append([sampled_minus1, sampled_nan], ignore_index=True).sort_values('cluster')\n",
        "\n",
        "sampled_df\n",
        "#sampled_df.to_csv(\"/content/drive/MyDrive/testsubset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtNeX_LtRBUl"
      },
      "outputs": [],
      "source": [
        "sampled_df=sampled_df[-30:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey4ey0Nb4KMp"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLufJOm-T6P-"
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJUmwaP3XTmK"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3YAwT_2VE2r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzkgnaBDCEu7"
      },
      "outputs": [],
      "source": [
        "output = openai.Embedding.create(input=['blah'], model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhtgp76X6Ibj"
      },
      "outputs": [],
      "source": [
        "print(output['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHZEPMzreCUF"
      },
      "outputs": [],
      "source": [
        "master_df['ada_embedding2']=master_df.text.apply(lambda x: get_embedding(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnyLHtNHD_bc"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR7P7Et2CxJ8"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from retry import retry\n",
        "\n",
        "model=\"text-embedding-ada-002\"\n",
        "# define the function to apply to each row\n",
        "master_df['ada_embedding2']=None\n",
        "subarts=master_df\n",
        "\n",
        "@retry(delay=3, tries=2)\n",
        "def process_excerpt(text):\n",
        "    try:\n",
        "      text = text.replace(\"\\n\", \" \")\n",
        "      output = openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n",
        "\n",
        "      if random.random() < 0.1:\n",
        "        print(iter)\n",
        "      return output\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(subarts, 25)]\n",
        "    print(segments[1])\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['ada_embedding2'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'ada_embedding2'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments)\n",
        "for idx, row in grouped_df[grouped_df['ada_embedding2'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'])\n",
        "    grouped_df.at[idx, 'ada_embedding2'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "master_df.loc[master_df['ada_embedding2'].isnull(), 'ada_embedding2'] = grouped_df.ada_embedding2.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i81afPcJ3jr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuiX8X5Kqje4"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XatijOLmq69t"
      },
      "outputs": [],
      "source": [
        "for index, row in data.iterrows():\n",
        "    documents.append(row['text'])\n",
        "    metadata = {\n",
        "        \"url\": row['url'],\n",
        "        \"title\": row['title'],\n",
        "        \"theme\": row['labels2'],\n",
        "\"keywords\": ', '.join(row['keywords'])  # Convert list to string\n",
        "    }\n",
        "    metadatas.append(metadata)\n",
        "    ids.append(f\"id{index+1}\")  # Assuming you want to use \"id1\", \"id2\", \"id3\", ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfbPcsCy_Iin"
      },
      "outputs": [],
      "source": [
        "\n",
        "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
        "chroma_client = chromadb.Client()\n",
        "embedding_function = OpenAIEmbeddingFunction(api_key=os.environ.get('OPENAI_API_KEY'), model_name=EMBEDDING_MODEL)\n",
        "articledb = chroma_client.create_collection(name='articles', embedding_function=embedding_function)\n",
        "# Iterate over the rows of the dataframe and create lists for documents, metadatas, and ids\n",
        "documents = []\n",
        "metadatas = []\n",
        "embeddings = []\n",
        "ids = []\n",
        "\n",
        "for index, row in master_df.iterrows():\n",
        "    documents.append(row['text'])\n",
        "    embeddings.append(row['ada_embedding2'])\n",
        "    metadata = {\n",
        "        \"coding\": row['coding']  # Convert list to string\n",
        "    }\n",
        "    metadatas.append(metadata)\n",
        "    ids.append(f\"{index}\")  # Assuming you want to use \"id1\", \"id2\", \"id3\", ...\n",
        "\n",
        "# Add the documents, metadatas, and ids to the vector database\n",
        "articledb.add(documents=documents, metadatas=metadatas,embeddings=embeddings, ids=ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gdu9odkxAdKq"
      },
      "outputs": [],
      "source": [
        "testdata = \"\tArticle type: metered User Type: anonymous web User Status: Campaign ID: 1 Cxense type: free User access status: 0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p39zrk8AAAO-"
      },
      "outputs": [],
      "source": [
        " articledb.query(query_texts=[testdata],n_results=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaZefim7v8-b"
      },
      "outputs": [],
      "source": [
        "# Get all texts from the DataFrame\n",
        "texts = sampled_df['text'].tolist()\n",
        "\n",
        "# Query the database once for all texts\n",
        "query_results = articledb.query(query_texts=texts, n_results=20)\n",
        "\n",
        "# Process the results: calculate the 'similar' value for each row\n",
        "similar_counts = []\n",
        "for result in query_results['metadatas']:\n",
        "    codings = [metadata['coding'] for metadata in result]\n",
        "    count = sum('rrelevant' in coding.lower() for coding in codings)\n",
        "    similar_counts.append(count)\n",
        "\n",
        "# Update the 'similar' column in the DataFrame\n",
        "sampled_df['similar'] = similar_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnTN0zhQNZmS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_cX36-UwkUR"
      },
      "outputs": [],
      "source": [
        "sampled_df.loc[sampled_df.similar==5,['text','summary','coding']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECUAu63rdHtC"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "sampled_df['ada_embedding'] = sampled_df['ada_embedding'].apply(ast.literal_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49aAPhGK5HgR"
      },
      "outputs": [],
      "source": [
        "embedding2 = openai.Embedding.create(input=['malaysia'], model=model)['data'][0]['embedding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH2hlctUdHJd"
      },
      "outputs": [],
      "source": [
        "\n",
        "model=\"text-embedding-ada-002\"\n",
        "embedding = openai.Embedding.create(input=['refugee issues'], model=model)['data'][0]['embedding']\n",
        "embedding2 = openai.Embedding.create(input=['malaysia'], model=model)['data'][0]['embedding']\n",
        "cosine_similarity(embedding2, embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhqywuqMUarY"
      },
      "outputs": [],
      "source": [
        "\n",
        "model=\"text-embedding-ada-002\"\n",
        "embedding = openai.Embedding.create(input=['refugee issues'], model=model)['data'][0]['embedding']\n",
        "embedding2 = openai.Embedding.create(input=['malaysia'], model=model)['data'][0]['embedding']\n",
        "sampled_df['refugee']  = sampled_df.ada_embedding2.apply(lambda x: cosine_similarity(x, embedding))\n",
        "sampled_df['malaysia']  = sampled_df.ada_embedding2.apply(lambda x: cosine_similarity(x, embedding2))\n",
        "sampled_df['score']=(sampled_df['refugee']**2)*(sampled_df['malaysia']**2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX7CYLLX4r_-"
      },
      "outputs": [],
      "source": [
        "sampled_df.text[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5Ere1CtZS0j"
      },
      "outputs": [],
      "source": [
        "sampled_df[['score','refugee','malaysia','text']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cMd2UD9V3wi"
      },
      "outputs": [],
      "source": [
        "sampled_df['ada_embedding'] = sampled_df['ada_embedding'].apply(lambda x: np.fromstring(x, sep=' '))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haiaktWNbgkD"
      },
      "outputs": [],
      "source": [
        "del segments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgjgbTMnTMEz"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSx2H159EcUk"
      },
      "outputs": [],
      "source": [
        "grouped_df[['relevant','coding','text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDYR_mD8FhH5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH5AAuQjLKq0"
      },
      "outputs": [],
      "source": [
        "master_df['Cluster'] = kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGreceGVRjZQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "matrix = np.vstack(embedding)\n",
        "n_clusters = 100\n",
        "\n",
        "kmeans2 = KMeans(n_clusters = n_clusters, init='k-means++', random_state=42)\n",
        "kmeans2.fit(matrix)\n",
        "#master_df['Cluster'] = kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HS_u21WvUCU"
      },
      "outputs": [],
      "source": [
        "unique_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyIOdmpxR_Q1"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpnBJ-7qTUOn"
      },
      "outputs": [],
      "source": [
        "sampled_df.iloc[:,10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw0LzFHZCXrg"
      },
      "outputs": [],
      "source": [
        "embedding[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGmrLAIgDjsh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj0U7KzUSNet"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Get unique labels and assign colors to each label\n",
        "unique_labels = np.unique(master_df.Cluster)\n",
        "num_colors = len(unique_labels)\n",
        "cmap = plt.get_cmap('prism', num_colors)\n",
        "\n",
        "# Adjust label colors to make the first cluster light gray\n",
        "label_colors = [cmap(i) for i in range(num_colors)]\n",
        "label_colors.insert(0, (0.85, 0.85, 0.85))  # Light gray color for the first cluster\n",
        "\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1],  cmap=ListedColormap(label_colors), marker='.', s=0.5)\n",
        "plt.colorbar()  # Add colorbar for label color mapping\n",
        "plt.xlim(3, 7)  # Crop the x-axis to the range [6, 14]\n",
        "plt.ylim(3,7)  # Crop the y-axis to the range [4, 14]\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('Scatterplot of Embedding Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMHJAvj0Z_8J"
      },
      "outputs": [],
      "source": [
        "grouped_df = master_df.groupby('Cluster').apply(lambda x: x.sample(n=4, random_state=1))\n",
        "grouped_df = grouped_df.reset_index(level='Cluster', drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM2nBfjgd9fS"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSbOLu7JRWQ6"
      },
      "outputs": [],
      "source": [
        "master_df['Cluster']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3UWzO63L5Wk"
      },
      "outputs": [],
      "source": [
        "n=333\n",
        "master_df.loc[master_df.cluster==n,'text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZILbFtUVLWM-"
      },
      "outputs": [],
      "source": [
        "master_df2=master_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GB3cLoOalDC"
      },
      "outputs": [],
      "source": [
        "sampled_df[[\"text\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tAdDK6_DaXB"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ7h7-rp-k0v"
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.0\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are a professor of sociology doing a thematic analysis of attitudes towards refugees in Malaysian news media. You will skeptically assess the relevance of a given passage.\"}]\n",
        "\n",
        "\n",
        "# define the function to apply to each row\n",
        "grouped_df['relevant']=None\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt):\n",
        "    #print(excerpt)\n",
        "    try:\n",
        "        api_key = random.choice([samkey, melkey])\n",
        "        openai.api_key = api_key\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article ### ' + str(excerpt)+ ' ### Is this passage a piece of text such as 1. a disclaimer of opinion, 2. a photo caption. Or is it a complete passage from the body of a news article? Respond only in the following python dictionary format: {“1. disclaimer?”: True/False, “2. caption?”: True/False, “Body?”: True/False } '  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(grouped_df, 2)]\n",
        "    for segment in segments:\n",
        "      segment['relevant'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['relevant'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'relevant'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['relevant'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'])\n",
        "    grouped_df.at[idx, 'relevant'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in grouped_df[grouped_df['relevant'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'])\n",
        "    grouped_df.at[idx,'relevant'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QIthIMvNhKk"
      },
      "outputs": [],
      "source": [
        "grouped_df=pd.concat(segments).reset_index().drop('index', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x737nl-chCsV"
      },
      "outputs": [],
      "source": [
        "grouped_df=pd.concat(segments).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLbAbc7thEL-"
      },
      "outputs": [],
      "source": [
        "grouped_df['relevant'] = grouped_df['relevant'].apply(eval)\n",
        "df= pd.json_normalize(grouped_df['relevant'])\n",
        "grouped_df = pd.concat([grouped_df, df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mozmn4HLhYU-"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP9YpgQDD9mk"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlu-DnfN4Ptm"
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.0\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are a professor of sociology doing a thematic analysis of attitudes towards refugees in Malaysian news media. You will skeptically assess the relevance of a given passage.\"}]\n",
        "\n",
        "\n",
        "# define the function to apply to each row\n",
        "grouped_df['relevant2']=None\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,one,two):\n",
        "    flag = 0\n",
        "    note = 'Note: this passage has been flagged as possibly meeting the following criteria for irrelevance: '\n",
        "    if one==True:\n",
        "      flag = 1\n",
        "      note = note + ' Passage is a disclaimer of personal opinion, '\n",
        "    if two==True:\n",
        "      note = note + ' Passage is a photo caption, '\n",
        "      flag = 1\n",
        "    note = note + 'If true, the passage is irrelevant. '\n",
        "    #if flag == 0:\n",
        "    note = ''\n",
        "    print(note)\n",
        "    try:\n",
        "        api_key = random.choice([samkey, melkey])\n",
        "        openai.api_key = api_key\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article ### ' + str(excerpt)+ ' ### Step by step, answer the following questions: 1. Does the passage explicitly, unambiguously discuss refugees? Note: most passages are not about refugees. 2. Does the passage explicitly, unambiguously reference Malaysia? Note: most passages are about other countries.' + note + ' Now respond in the following Python dictionary format: {\"1. Refugees?\": ”Yes.”/”No.”, \"2. Malaysia?\": ”Yes.”/”No.”}'  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(grouped_df, 2)]\n",
        "    for segment in segments:\n",
        "      segment['relevant2'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['relevant2'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','1. disclaimer?','2. caption?']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'relevant2'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).drop(['index'], axis=1).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['relevant2'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['1. disclaimer?'],row['2. caption?'])\n",
        "    grouped_df.at[idx, 'relevant2'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in grouped_df[grouped_df['relevant2'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['1. disclaimer?'],row['2. caption?'])\n",
        "    grouped_df.at[idx,'relevant2'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjf15Vb5WugY"
      },
      "outputs": [],
      "source": [
        "test=grouped_df[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSo6fVu9IjD4"
      },
      "outputs": [],
      "source": [
        "grouped_df[['relevant2','text'] ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAQzge7OHB_e"
      },
      "outputs": [],
      "source": [
        "grouped_df['relevant2'] = grouped_df['relevant2'].apply(eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWthJahSNi2V"
      },
      "outputs": [],
      "source": [
        "grouped_df['relevant2'] = grouped_df['relevant2'].apply(eval)\n",
        "df= pd.json_normalize(grouped_df['relevant2'])\n",
        "grouped_df = pd.concat([grouped_df, df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au6Ml3VFXdZk"
      },
      "outputs": [],
      "source": [
        "grouped_df.loc[grouped_df['Body?']==False,['text','relevant2','Cluster']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh2VFugJsW3V"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTDvGq9jSf1F"
      },
      "outputs": [],
      "source": [
        "grouped_df=grouped_df.iloc[:,4:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5tzYFrBr3I9"
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.0\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are a professor of sociology doing a thematic analysis of attitudes towards refugees in Malaysian news media. You will be very skeptical of the relevance of a given passage and identify any reasons why it might be irrelevant.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "grouped_df['relevant3']=None\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,one,two,three,four):\n",
        "    flag = 0\n",
        "    note = 'Note: this passage has been flagged as possibly meeting the following criteria for irrelevance: '\n",
        "    if one==True:\n",
        "      flag = 1\n",
        "      note = note + ' Passage is a disclaimer of personal opinion, '\n",
        "    if two==True:\n",
        "      note = note + ' Passage is a photo caption, '\n",
        "      flag = 1\n",
        "    if \"No\" in three:\n",
        "      note = note + ' Not about refugees, '\n",
        "      flag = 1\n",
        "    if \"No\" in four:\n",
        "      note = note + ' Not about Malaysia, '\n",
        "      flag = 1\n",
        "    note = note + ' ### If any of these criteria are true, you should answer \"No.\" or \"Maybe.\" '\n",
        "    if flag == 0:\n",
        "      note = ''\n",
        "    print(note)\n",
        "    try:\n",
        "        api_key = random.choice([samkey, melkey])\n",
        "        openai.api_key = api_key\n",
        "        input = messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article ### ' + str(excerpt)+ ' ### Answer step by step: 1. Might this passage be relevant to attitudes towards refugees in Malaysia? If it clearly is, answer \"Yes.\" If it might be, depending on the context of the article the passage is from--eg. the identity of the subject and their location--answer \"Maybe.\" If it is definitely irrelevant regardless of context, answer \"No.\" ' + note + ' 2. If \"No.\" or \"Maybe.\", in 15 words or less give any and all reasons why it might be irrelevant--both those provided earlier and any others you identify, such as irrelevant output from a content management system or editorial annotations to the article. Respond in the following python dictionary format: {\"1. Relevant?\":\"Yes.\"/\"Maybe.\"/\"No.\",\"2. Why Not?\":string or None} '  }]\n",
        "        output = openai.ChatCompletion.create(model=model, temperature=temperature, messages=input ).choices[0].message['content']\n",
        "        print(input)\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(grouped_df, 2)]\n",
        "    for segment in segments:\n",
        "      segment['relevant3'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['relevant3'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','1. disclaimer?','2. caption?','1. Refugees?','2. Malaysia?']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'relevant3'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).drop(['index'], axis=1).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['relevant3'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['1. disclaimer?'],row['2. caption?'],row['1. Refugees?'],row['2. Malaysia?'])\n",
        "    grouped_df.at[idx, 'relevant3'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in grouped_df[grouped_df['relevant3'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['1. disclaimer?'],row['2. caption?'],row['1. Refugees?'],row['2. Malaysia?'])\n",
        "    grouped_df.at[idx,'relevant3'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDFUGGi2czBD"
      },
      "outputs": [],
      "source": [
        "grouped_df.to_csv('/content/drive/MyDrive/newssubsample.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt_Ot7TLV7Qm"
      },
      "outputs": [],
      "source": [
        "grouped_df[grouped_df.relevant3.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9RNb8glpnQt"
      },
      "outputs": [],
      "source": [
        "grouped_df[['codingd','text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRFPBOhHCCvr"
      },
      "outputs": [],
      "source": [
        "grouped_df=pd.read_csv('/content/drive/MyDrive/newssubsample.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g61xIuSx2529"
      },
      "outputs": [],
      "source": [
        "subset_df = grouped_df.drop(grouped_df.columns[[12,16]], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts4sHWycwaUi"
      },
      "outputs": [],
      "source": [
        "subset=master_df.loc[master_df.title=='Rohingya migrants in limbo']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5oT-vrj72An"
      },
      "outputs": [],
      "source": [
        "master_df.loc[master_df.Cluster==4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQdhxeuNY2bo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def parse_json(json_str):\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        return None\n",
        "\n",
        "grouped_df['relevant3'] = grouped_df['relevant3'].apply(parse_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e3KDxVtwEB4"
      },
      "outputs": [],
      "source": [
        "grouped_df[grouped_df.relevant3.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqde1ssoyu9F"
      },
      "outputs": [],
      "source": [
        "grouped_df['relevant3']=grouped_df['relevant3test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7roF7gdfvQ2"
      },
      "outputs": [],
      "source": [
        "grouped_df=grouped_df.iloc[:,:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ar1EgyD00sD"
      },
      "outputs": [],
      "source": [
        "def blah(relevant):\n",
        "  print(relevant)\n",
        "for idx, row in grouped_df.iterrows():\n",
        "  print(idx)\n",
        "  blah(row['relevant3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YLAIGqbrkFl"
      },
      "outputs": [],
      "source": [
        "grouped_df.loc[grouped_df.relevant3.isna(),['relevant3','text','relevant4','codingd']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZhYXwfYutLz"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UuEDglBt7Oj"
      },
      "outputs": [],
      "source": [
        "test=grouped_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML83xvhFlnD2"
      },
      "outputs": [],
      "source": [
        "grouped_df=test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkbLO2FOlqFq"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLJaCyDenCmd"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX0I8bzq3xn1"
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "import ast\n",
        "\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.0\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are a professor of refugee studies doing thematic analysis. Carefully parse the passages and only code themes that are supported by the text. Do not assume refugees or Malaysia are involved. Contextualize attitudes by identifying who holds the attitude, who the attitude targets, and whether that attitude is hostile or sympathetic to the target, if relevant.\"}]\n",
        "\n",
        "# define the function to apply to each row\n",
        "grouped_df['codingc']=None\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,one,two,three,four,relevant):\n",
        "    flag = 0\n",
        "    relevant=ast.literal_eval(relevant)\n",
        "    try:\n",
        "      rel = relevant['1. Relevant?']\n",
        "    except:\n",
        "      rel = ''\n",
        "    try:\n",
        "      reason = relevant['2. Why Not?']\n",
        "    except:\n",
        "      reason = ''\n",
        "    if reason == None:\n",
        "      reason = ''\n",
        "    if \"aybe\" in rel:\n",
        "      note = ' Previous analysis found that this passage might be irrelevant for this reason: ' + reason + '### Take this into account.'\n",
        "      flag = 1\n",
        "    if \"No\" in rel:\n",
        "      note = ' Previous analysis found that this passage is irrelevant for this reason: ' + reason + '### Take this into account.'\n",
        "      flag = 1\n",
        "    if flag == 0:\n",
        "      note = ''\n",
        "    #print(note)\n",
        "    try:\n",
        "            api_key = random.choice([samkey, melkey])\n",
        "            openai.api_key = api_key\n",
        "            input = messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article ### ' + str(excerpt)+ ' ### Give the theme of this passage as it embodies, relates to or reflects attitudes towards refugees in Malaysia if it is relevant to that topic. If it is not relevant, simply summarize the passage in a few words. Note that this passage may simply be text from the web interface and not from an article at all. Before answering, analyze step by step: 1. in 14 words or less, return the theme. Do not offer a generic theme like \"attitudes towards refugees in Malaysia\", but give a specific theme. 2. Whose attitudes are being reflected? Examples: the Malaysian government, The Bangladeshi government, Malaysians, NGOs, the author. 3. Who is the target of the attitudes? Examples: migrant workers, Myanmar, the Rohingya, the government, UNHCR. 4. What is the valence of attitudes towards the target, if any?:  \"Sympathetic.\", \"Hostile.\", or \"N/A\". ' + note + ' Finally, Respond ONLY in the following python dictionary format: {\"1. Theme\": stringval1, \"2. Whose Attitude?\":stringval2,\"3. Target\":stringval3,\"4. Valence\": \"Sympathetic.\"/\"Hostile.\"/\"N/A\"}'  }]\n",
        "            output = openai.ChatCompletion.create(model=model, temperature=temperature, messages=input ).choices[0].message['content']\n",
        "            print(input)\n",
        "            return output\n",
        "    except Exception as e:\n",
        "            print(f\"Error processing row {idx}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        return \"None\"\n",
        "\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(grouped_df, 2)]\n",
        "    for segment in segments:\n",
        "      segment['codingc'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['codingc'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','1. disclaimer?','2. caption?','1. Refugees?','2. Malaysia?','relevant3']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'codingc'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).drop(['index'], axis=1).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['codingc'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['1. disclaimer?'],row['2. caption?'],row['1. Refugees?'],row['2. Malaysia?'],row['relevant3'])\n",
        "    grouped_df.at[idx, 'codingc'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in grouped_df[grouped_df['codingc'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['1. disclaimer?'],row['2. caption?'],row['1. Refugees?'],row['2. Malaysia?'],row['relevant3'])\n",
        "    grouped_df.at[idx,'codingc'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ4vTdcUkiAB"
      },
      "outputs": [],
      "source": [
        "grouped_df[['text','relevant','relevant2']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arIZ3GSnfYUJ"
      },
      "outputs": [],
      "source": [
        "input:\n",
        "print(relevant)\n",
        "output:\n",
        "{'1. Relevant?': 'No.', '2. Why Not?': 'Not about refugees or Malaysia.'}\n",
        "\n",
        "input: print(relevant['1. Relevant?'])\n",
        "output: TypeError: string indices must be integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi6URgkZfBy9"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O3hbdIDxbpL"
      },
      "outputs": [],
      "source": [
        "grouped_df[['text','relevant3','codingc']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My61_VRnCTZM"
      },
      "outputs": [],
      "source": [
        "grouped_df[['text','relevant3','codingc','codingd']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkBTmpVcpeCL"
      },
      "outputs": [],
      "source": [
        "grouped_df=grouped_df.loc[[2,3,8,10,12,16,17,20,22,1491,226,238,247,1492,1499],:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2bgnIci2aAr"
      },
      "outputs": [],
      "source": [
        "grouped_df['codingc']=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XygBx-mcTK2D"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbEomZ9ik4Cz"
      },
      "outputs": [],
      "source": [
        "grouped_df[['text','codingd','codingcheck']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lh0uiK9roQP"
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.0\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are a professor of refugee studies doing thematic analysis of attitudes towards refugees in Malaysian news media. When assessing a passage, you must be certain it is relevant to attitudes towards refugees in Malaysia. If not, you must reject it as irrelevant.  Contextualize attitudes by identifying who holds the attitude, who the attitude targets, and whether that attitude is hostile or sympathetic to the target, if relevant. Consult the summary of the excerpted article for context, but only code the excerpt itself.\"}]\n",
        "\n",
        "\n",
        "# define the function to apply to each row\n",
        "grouped_df['codingd']=None\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,one,two,three,four,relevant,precode,summary):\n",
        "    flag = 0\n",
        "    relevant=ast.literal_eval(relevant)\n",
        "    try:\n",
        "      rel = relevant['1. Relevant?']\n",
        "    except:\n",
        "      rel = ''\n",
        "    try:\n",
        "      reason = relevant['2. Why Not?']\n",
        "    except:\n",
        "      reason = ''\n",
        "    if reason == None:\n",
        "      reason = ''\n",
        "    if \"aybe\" in rel:\n",
        "      note = ' Previous analysis found that this SPECIFIC passage might be irrelevant for this reason: ' + reason + '### Does the summary clarify this?.'\n",
        "      flag = 1\n",
        "    if \"No\" in rel:\n",
        "      note = ' Previous analysis found that this SPECIFIC passage might be irrelevant for this reason: ' + reason + '### Does the summary clarify this?.'\n",
        "      flag = 1\n",
        "    if flag == 0:\n",
        "      note = ''\n",
        "    try:\n",
        "            api_key = random.choice([samkey, melkey])\n",
        "            openai.api_key = api_key\n",
        "            input = messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article ### ' + str(excerpt)+ ' ### The theme of this passage was coded as ### ' + str(precode) + ' ### but this analysis ignores the article summary and is therefore unreliable. Reassess the theme of this passage as it relates to attitudes towards refugees in Malaysia, given the context of this summary of the article it came from ### ' + str(summary) + ' ### Before answering, analyze step by step: 1. in 14 words or less, return the reassessed theme (if relevant) as it relates to attitudes towards refugees in Malaysia, or return None. Do not give a generic theme like \"attitudes towards refugees in Malaysia\", but provide a specific theme. If irrelevant, return None for all further questions. If relevant, 2. Whose attitudes are being reflected? Examples: the government, Malaysians, NGOs, the author. 3. Who is the target of the attitudes? Examples: the Rohingya, the government, UNHCR.  4. What is the valence of the attitude towards the target, if any?:  \"Sympathetic.\", \"Hostile.\", or \"N/A\". ' + note + ' Once again, the passage to code is ### ' +  str(excerpt)+ ' ### Finally, Respond ONLY in the following python dictionary format: {\"1. Theme\": stringval1/None, \"2. Whose Attitude?\":stringval2,\"3. Target\":stringval3\",4. Valence\": \"Sympathetic.\"/\"Hostile.\"/\"N/A\"}'  }]\n",
        "            output = openai.ChatCompletion.create(model=model, temperature=temperature, messages=input ).choices[0].message['content']\n",
        "            print(input)\n",
        "            return output\n",
        "    except Exception as e:\n",
        "            print(f\"Error processing row {idx}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(grouped_df, 2)]\n",
        "    for segment in segments:\n",
        "      segment['codingd'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['codingd'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','1. disclaimer?','2. caption?','1. Refugees?','2. Malaysia?','relevant3','codingc','summary']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'codingd'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).drop(['index'], axis=1).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['codingd'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['1. disclaimer?'],row['2. caption?'],row['1. Refugees?'],row['2. Malaysia?'],row['relevant3'],row['codingc'],row['summary'])\n",
        "    grouped_df.at[idx, 'codingd'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in grouped_df[grouped_df['codingd'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['1. disclaimer?'],row['2. caption?'],row['1. Refugees?'],row['2. Malaysia?'],row['relevant3'],row['codingc'],row['summary'])\n",
        "    grouped_df.at[idx,'codingd'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npznAvbhwg2w"
      },
      "outputs": [],
      "source": [
        "grouped_df[['text','codingd']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD8y7CEHud9z"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3-bIOK9ubu5"
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.0\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are a professor of refugee studies doing thematic analysis of attitudes towards refugees in Malaysian news media. You are critically reviewing the work of your coders to identify any mistakes. Codes should capture the theme of the passage, using the summary as context. They should describe whose attitude is being coded, the target of that attitude, and the valence, if relevant.\"}]\n",
        "\n",
        "\n",
        "# define the function to apply to each row\n",
        "grouped_df['codingcheck']=None\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,one,two,three,four,relevant,precode,summary):\n",
        "    flag = 0\n",
        "    relevant=ast.literal_eval(relevant)\n",
        "    try:\n",
        "      rel = relevant['1. Relevant?']\n",
        "    except:\n",
        "      rel = ''\n",
        "    try:\n",
        "      reason = relevant['2. Why Not?']\n",
        "    except:\n",
        "      reason = ''\n",
        "    if reason == None:\n",
        "      reason = ''\n",
        "    if \"aybe\" in rel:\n",
        "      note = ' Previous analysis found that this SPECIFIC passage might be irrelevant for this reason: ' + reason + '### Does the summary clarify this?.'\n",
        "      flag = 1\n",
        "    if \"No\" in rel:\n",
        "      note = ' Previous analysis found that this SPECIFIC passage might be irrelevant for this reason: ' + reason + '### Does the summary clarify this?.'\n",
        "      flag = 1\n",
        "    if flag == 0:\n",
        "      note = ''\n",
        "    try:\n",
        "            api_key = random.choice([samkey, melkey])\n",
        "            openai.api_key = api_key\n",
        "            input = messages + [{\"role\": \"user\", \"content\": 'Read a passage from a news article ### ' + str(excerpt)+ ' ### The theme of this passage was coded as ### ' + str(precode) + ' ### but the coder may be unreliable. Does this code capture the theme of this passage as it relates to attitudes towards refugees in Malaysia (or accurately identify the passage as irrelevant), given the context of this summary of the article it came from? ### ' + str(summary) + ' ### Before answering, analyze step by step: 1. How well does this code concisely (in just a few words) capture the theme of the passage? Integers 1-5, where 1 is terrible, 5 is excellent, and 3 is fair. If the passage is irrelevant the theme values should be None. If irrelevant and None, the score should be 5. 2. In 20 words or fewer, justify your assessment. Respond only in the following Python dictionary format: {“1. Quality\":1/2/3/4/5,\"2. Assessment\":stringval}'  }]\n",
        "            output = openai.ChatCompletion.create(model=model, temperature=temperature, messages=input ).choices[0].message['content']\n",
        "            print(input)\n",
        "            return output\n",
        "    except Exception as e:\n",
        "            print(f\"Error processing row {idx}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(grouped_df, 2)]\n",
        "    for segment in segments:\n",
        "      segment['codingcheck'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['codingcheck'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','1. disclaimer?','2. caption?','1. Refugees?','2. Malaysia?','relevant3','codingd','summary']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'codingcheck'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments).drop(['index'], axis=1).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n",
        "grouped_df['index'] = grouped_df.index\n",
        "for idx, row in grouped_df[grouped_df['codingcheck'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['1. disclaimer?'],row['2. caption?'],row['1. Refugees?'],row['2. Malaysia?'],row['relevant3'],row['codingd'],row['summary'])\n",
        "    grouped_df.at[idx, 'codingcheck'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in grouped_df[grouped_df['codingcheck'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['1. disclaimer?'],row['2. caption?'],row['1. Refugees?'],row['2. Malaysia?'],row['relevant3'],row['codingd'],row['summary'])\n",
        "    grouped_df.at[idx,'codingcheck'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3xJLllwE8Hl"
      },
      "outputs": [],
      "source": [
        "# Concatenate the codingd strings for each cluster\n",
        "concat_df = grouped_df.groupby('Cluster')['codingd'].apply(lambda x: ' Next Passage: '.join(x)).reset_index()\n",
        "\n",
        "# Rename the codingd column to codinglist\n",
        "concat_df.rename(columns={'codingd': 'codinglist'}, inplace=True)\n",
        "\n",
        "concat_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Mdjc3e0KRLi"
      },
      "outputs": [],
      "source": [
        "concat_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB3q4IEiL8v7"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFLZ3rlgPyiN"
      },
      "outputs": [],
      "source": [
        "concat_df=pd.concat(segments)\n",
        "for idx, row in concat_df[concat_df['codesum'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['codinglist'])\n",
        "    concat_df.at[idx, 'codesum'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvth1S2iJ0NJ"
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.0\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are a professor of sociology doing a thematic analysis of attitudes towards refugees in Malaysian news media. You are summarizing themes across several passages. If all themes are 'None.', you must code that that 'None are.' relevant\"}]\n",
        "\n",
        "\n",
        "# define the function to apply to each row\n",
        "grouped_df['codesum']=None\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt):\n",
        "    try:\n",
        "            api_key = random.choice([samkey, melkey])\n",
        "            openai.api_key = api_key\n",
        "            input = messages+[{\"role\": \"user\", \"content\": 'Read a list of four themes from a cluster of passages ### ' + str(excerpt)+ ' ### Step by step, answer the following: 1 Are all of these themes both present and relevant to attitudes towards refugees in Malaysia? \"All are.\"/\"None are.\"/\"Some are.\". If irrelevant, return none to all further questions.  2. If relevant, return the overarching theme as it relates to attitudes towards refugees in Malaysia, or return None. Do not give a generic theme like \"attitudes towards refugees in Malaysia\", but provide a specific and detailed theme. If relevant, 3. Whose attitudes are being reflected? Examples: the government, Malaysians, NGOs, the author. 4. Who is the target of the attitudes? Examples: the Rohingya, the government, UNHCR. 5. What is the overall valence, if any? Finally, Respond ONLY in the following python dictionary format: {\"1. Are Passages Relevant?\":\"All are.\"/\"None are.\"/\"Some are.\",\"2. Theme\": stringval1/None, \"3. Whose Attitude?\":stringval2,\"4. Target\":stringval3,\"5. Valence\": \"Sympathetic.\"/\"Hostile.\"/\"N/A\"}'  }]\n",
        "            output = openai.ChatCompletion.create(model=model, temperature=temperature, messages=input ).choices[0].message['content']\n",
        "            print(input)\n",
        "            return output\n",
        "    except Exception as e:\n",
        "            print(f\"Error processing row {idx}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(concat_df, 2)]\n",
        "    for segment in segments:\n",
        "      segment['codesum'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['codesum'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['codinglist']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'codesum'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "# Merge the segments\n",
        "concat_df=pd.concat(segments)\n",
        "#['level_0','index']\n",
        "concat_df['index'] = concat_df.index\n",
        "for idx, row in concat_df[concat_df['codesum'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['codinglist'])\n",
        "    concat_df.at[idx, 'codesum'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in concat_df[concat_df['codesum'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['codinglist'])\n",
        "    concat_df.at[idx,'codesum'] = result\n",
        "    print(f\"Row {idx} processed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-bL_s1FNAnv"
      },
      "outputs": [],
      "source": [
        "concat_df.to_csv('/content/drive/MyDrive/clusterthemes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qDa_2Fmk1Xb"
      },
      "outputs": [],
      "source": [
        "concat=pd.read_csv('/content/drive/MyDrive/clusterthemes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSLpV6jT1-PV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBmfEC8rz7HF"
      },
      "outputs": [],
      "source": [
        "concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSmd9-SWJnjd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Function to change dictionary\n",
        "def change_dict(d):\n",
        "    target_dict = {\"1. Are Passages Relevant?\": \"Some are.\", \"2. Theme\": None,\n",
        "                   \"3. Whose Attitude?\": None, \"4. Target\": None, \"5. Valence\": \"N/A\"}\n",
        "\n",
        "    new_dict = {\"1. Are Passages Relevant?\": \"None are.\", \"2. Theme\": None,\n",
        "                \"3. Whose Attitude?\": None, \"4. Target\": None, \"5. Valence\": \"N/A\"}\n",
        "\n",
        "    return new_dict if d == target_dict else d\n",
        "\n",
        "# Assuming codesum contains string representations of dictionaries,\n",
        "# first, we convert them to actual dictionaries\n",
        "concat_df['codesum'] = concat_df['codesum'].apply(ast.literal_eval)\n",
        "\n",
        "# Now apply the function to the 'codesum' column\n",
        "concat_df['codesum'] = concat_df['codesum'].apply(change_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fssTCXCiI1__"
      },
      "outputs": [],
      "source": [
        "{\"1. Are Passages Relevant?\": \"Some are.\", \"2. Theme\": None, \"3. Whose Attitude?\": None, \"4. Target\": None, \"5. Valence\": \"N/A\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihgLy-FsMyGe"
      },
      "outputs": [],
      "source": [
        "master_df=master_df.merge(concat_df, on='Cluster', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuvfqMhyJ17p"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y7ulB00QKAv"
      },
      "outputs": [],
      "source": [
        "sampled_df = master_df.sample(frac=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCStJhWCTxU_"
      },
      "outputs": [],
      "source": [
        "'Read a passage from a news article ### ' + str(excerpt)+ ' ### The theme of this passage was coded as ### ' + str(precode) ' ### Reassess the theme of this SPECIFIC passage as it relates to attitudes towards refugees in Malaysia, given the context of this summary of the article it came from ### ' + str(summary) + ' ### Before answering, analyze step by step: 1. in 15 words or less, return the theme (if relevant) as it relates to attitudes towards refugees in Malaysia, or return None. If irrelevant, return None for all further questions. If relevant, 2. Whose attitudes are being reflected? Examples: the government, Malaysians, NGOs, the author. 3. Who is the target of the attitudes? Examples: the Rohingya, the government, UNHCR. ' + note + ' Finally, Respond ONLY in the following python dictionary format: {\"1. Theme\": stringval1/None, \"2. Whose Attitude?\":stringval2,\"3. Target\":stringval3}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBmi6818Tftx"
      },
      "outputs": [],
      "source": [
        "[{\"role\": \"user\", \"content\": 'Read a passage from a news article ### ' + str(excerpt)+ ' ### The theme of this passage was coded as ### ' + str(precode) + ' ### Reassess the theme of this SPECIFIC passage as it relates to attitudes towards refugees in Malaysia, given the context of this summary of the article it came from ### ' + str(summary) + ' ### Before answering, analyze step by step: 1. in 15 words or less, return the theme (if relevant) as it relates to attitudes towards refugees in Malaysia, or return None. If irrelevant, return None for all further questions. If relevant, 2. Whose attitudes are being reflected? Examples: the government, Malaysians, NGOs, the author. 3. Who is the target of the attitudes? Examples: the Rohingya, the government, UNHCR. ' + note + ' Finally, Respond ONLY in the following python dictionary format: {\"1. Theme\": stringval1/None, \"2. Whose Attitude?\":stringval2,\"3. Target\":stringval3}'  }]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgRt-OrZwE6B"
      },
      "outputs": [],
      "source": [
        "row['text'],row['relevant']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiWw66ZzvUpB"
      },
      "outputs": [],
      "source": [
        "for idx, row in grouped_df[grouped_df['codingc'].isnull()].iterrows():\n",
        "  print(row[['text','relevant']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCNvKJBNvlrb"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LHU6dNJRDH1"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJh0FQFfQOU0"
      },
      "outputs": [],
      "source": [
        "sampled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2cmk2he0Qu3"
      },
      "outputs": [],
      "source": [
        "review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_q-7apBwRhb"
      },
      "outputs": [],
      "source": [
        "review=pd.read_csv(\"/content/drive/MyDrive/rawcodes.csv\")\n",
        "merged = review.merge(master_df[['fulltext', 'text', 'Cluster','codesum']],\n",
        "                      how='left',\n",
        "                      on=['fulltext', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EBSg61FKvBb"
      },
      "outputs": [],
      "source": [
        "merged[['text','cluster','Cluster']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi4e7JcoMu5T"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNCB1v1x7czP"
      },
      "outputs": [],
      "source": [
        "###add 2nd round codes to master_df where already complete:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Add 'codingd' to master_df with default NaN values\n",
        "master_df['codingd'] = np.nan\n",
        "\n",
        "# Select only the relevant columns from sampled_df for the merge operation\n",
        "temp_df = sampled_df[['text', 'title', 'codingd']]\n",
        "\n",
        "# Perform a left merge on 'text' and 'title' to create a new column for 'codingd' from sampled_df\n",
        "# Note: We use suffixes to differentiate between the original 'codingd' in master_df and the one from sampled_df\n",
        "merged_df = pd.merge(master_df, temp_df, on=['text', 'title'], how='left', suffixes=('', '_from_sampled'))\n",
        "\n",
        "# Fill NaN values in the original 'codingd' column with the values from 'codingd_from_sampled'\n",
        "merged_df['codingd'] = merged_df['codingd'].fillna(merged_df['codingd_from_sampled'])\n",
        "\n",
        "# Drop the temporary 'codingd_from_sampled' column as it's no longer needed\n",
        "merged_df.drop(columns=['codingd_from_sampled'], inplace=True)\n",
        "\n",
        "# Now, merged_df contains your master_df with 'codingd' updated from sampled_df, without altering other columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T4DnfnSAyJF"
      },
      "outputs": [],
      "source": [
        "!pip install aiohttp tiktoken\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = samkey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd8ju-6yBi9D"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "test_requests = [\n",
        "    {\"model\": \"text-embedding-3-small\", \"input\": \"embed me\", \"metadata\": {\"row_id\": 1}},\n",
        "    {\"model\": \"text-embedding-3-small\", \"input\": \"another test\", \"metadata\": {\"row_id\": 2}}\n",
        "]\n",
        "\n",
        "with open('example_requests_to_parallel_process.jsonl', 'w') as f:\n",
        "    for request in test_requests:\n",
        "        print(json.dumps(request), file=f)\n",
        "\n",
        "!python api.py \\\n",
        "  --requests_filepath example_requests_to_parallel_process.jsonl \\\n",
        "  --save_filepath example_results.jsonl \\\n",
        "  --request_url https://api.openai.com/v1/embeddings \\\n",
        "  --max_requests_per_minute 200 \\\n",
        "  --max_tokens_per_minute 1000000 \\\n",
        "  --token_encoding_name cl100k_base \\\n",
        "  --max_attempts 2 \\\n",
        "  --logging_level 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbK0w5Vs_lzV"
      },
      "outputs": [],
      "source": [
        "testdf=merged_df[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENgjZuDyTcfA"
      },
      "outputs": [],
      "source": [
        "!pip install nest_asyncio\n",
        "!pip install httpx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8Bouv6bXpbq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Example DataFrame setup\n",
        "# testdf = pd.DataFrame({'text': ['Example question 1', 'Example question 2'], 'codingd': [np.nan, np.nan]})\n",
        "\n",
        "# Filter DataFrame to get rows where 'codingd' is NaN\n",
        "df_to_process = merged_df[merged_df['codingd'].isna()]\n",
        "system = [{\"role\": \"system\", \"content\": \"You are a professor of sociology doing a thematic analysis of attitudes towards refugees in Malaysian news media. When assessing a passage, you must be certain it is relevant to attitudes towards refugees in Malaysia. If not, you must reject it as irrelevant. Contextualize attitudes by identifying who holds the attitude, who the attitude targets, and whether that attitude is hostile or sympathetic to the target, if relevant. Consult the summary of the excerpted article for context, only code the excerpt itself.\"}]\n",
        "# Step 2: Create a JSONL file with text from DataFrame\n",
        "def create_jsonl_file(df, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for index, row in df.iterrows():\n",
        "            # Adapt this dict structure as needed for your specific API request format\n",
        "            data = {\n",
        "                \"model\": \"gpt-3.5-turbo-0613\",\n",
        "                \"messages\": system +[{\"role\": \"user\", \"content\": 'Read this passage from a news article ### ' + str(row['text']) + ' ###  If relevant, give the theme of this SPECIFIC passage as it embodies, relates to, or reflects attitudes towards refugees in Malaysia. The following summary of the excerpted article may provide context for the passage (e.g. who is being discussed and where events are occurring): ### ' + str(row['summary']) + ' ### Here is an overview of how several passages similar to this one have been coded: ### ' + str(row['codesum']) + ' ### DO NOT copy this coding verbatim, but use it as reference and be careful if only some or none of the similar passages were deemed relevant. Before answering, analyze step by step: 1. in 12 words or less, return the theme (if relevant) as it relates to attitudes towards refugees in Malaysia, or return None. Do not give a generic theme like \"attitudes towards refugees in Malaysia\", but provide a specific single theme. If irrelevant, return None for all further questions. If relevant, 2. Whose attitudes are being reflected? Examples: the government, Malaysians, NGOs, the author. 3. Who is the target of the attitudes? Examples: the Rohingya, the government, UNHCR. 4. What is the valence of attitudes towards the target, if any?:  \"Sympathetic.\", \"Hostile.\", or \"N/A\". Once again, the passage to code is ### ' + str(row['text']) + ' ###  Finally, Respond ONLY in the following python dictionary format: {\"1. Theme\": None/stringval1, \"2. Whose Attitude?\":None/stringval2,\"3. Target\":None/stringval3, 4. Valence\": \"Sympathetic.\"/\"Hostile.\"/\"N/A\"}'  }],\n",
        "                \"temperature\": 0.0,\n",
        "                \"metadata\": {\"df_index\": index}  # Store DataFrame index in metadata\n",
        "            }\n",
        "            file.write(json.dumps(data) + '\\n')\n",
        "\n",
        "requests_filepath = '/content/sample_questions.jsonl'\n",
        "create_jsonl_file(df_to_process, requests_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ycxTMY3TSlRo"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "import os\n",
        "\n",
        "# Apply nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Assuming the content of api.py is available and imported or integrated here directly\n",
        "from api import process_api_requests_from_file\n",
        "\n",
        "# Configuration\n",
        "open('/content/sample_questions_results.jsonl', 'w').close()\n",
        "requests_filepath = '/content/sample_questions.jsonl'  # Adjust path as needed\n",
        "save_filepath = '/content/sample_questions_results.jsonl'  # Adjust path as needed\n",
        "request_url = \"https://api.openai.com/v1/chat/completions\"  # API endpoint\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")  # Ensure this environment variable is set\n",
        "max_requests_per_minute = 600  # Adjust based on your rate limit\n",
        "max_tokens_per_minute = 1000000  # Adjust based on your rate limit\n",
        "token_encoding_name = \"cl100k_base\"  # Default encoding for token estimation\n",
        "max_attempts = 3  # Number of retries for failed requests\n",
        "logging_level = 20  # INFO logging level\n",
        "\n",
        "# Directly run the coroutine without needing to manually get or set the event loop\n",
        "asyncio.run(\n",
        "    process_api_requests_from_file(\n",
        "        requests_filepath=requests_filepath,\n",
        "        save_filepath=save_filepath,\n",
        "        request_url=request_url,\n",
        "        api_key=api_key,\n",
        "        max_requests_per_minute=max_requests_per_minute,\n",
        "        max_tokens_per_minute=max_tokens_per_minute,\n",
        "        token_encoding_name=token_encoding_name,\n",
        "        max_attempts=max_attempts,\n",
        "        logging_level=logging_level,\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-HYI9c4aWl4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def update_dataframe_from_results(df, results_filepath):\n",
        "    with open(results_filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            result = json.loads(line)\n",
        "            df_index = result[2]['df_index']  # Accessing the DataFrame index correctly\n",
        "            response_content = result[1]['choices'][0]['message']['content']\n",
        "            df.at[df_index, 'codingd'] = response_content  #\n",
        "\n",
        "# Assuming 'results_filepath' points to the file where your API results are saved\n",
        "update_dataframe_from_results(merged_df, results_filepath)\n",
        "\n",
        "# Now, 'testdf' will have the 'codingd' column updated with responses from the API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNMIUhkUcEnV"
      },
      "outputs": [],
      "source": [
        "merged_df.to_csv('/content/drive/MyDrive/fullcodes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOWWb4doRACc"
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.0\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are a professor of sociology doing a thematic analysis of attitudes towards refugees in Malaysian news media. When assessing a passage, you must be certain it is relevant to attitudes towards refugees in Malaysia. If not, you must reject it as irrelevant. Contextualize attitudes by identifying who holds the attitude, who the attitude targets, and whether that attitude is hostile or sympathetic to the target, if relevant. Consult the summary of the excerpted article for context, only code the excerpt itself.\"}]\n",
        "\n",
        "\n",
        "# define the function to apply to each row\n",
        "merged['codingd']=None\n",
        "\n",
        "@retry(delay=2, tries=2)\n",
        "def process_excerpt(excerpt,relevant,summary):\n",
        "    try:\n",
        "            input = messages+[{\"role\": \"user\", \"content\": 'Read this passage from a news article ### ' + str(excerpt) + ' ###  If relevant, give the theme of this SPECIFIC passage as it embodies, relates to, or reflects attitudes towards refugees in Malaysia. The following summary of the excerpted article may provide context for the passage (e.g. who is being discussed and where events are occurring): ### ' + str(summary) + ' ### Here is an overview of how several passages similar to this one have been coded: ### ' + str(relevant) + ' ### DO NOT copy this coding verbatim, but use it as reference and be careful if only some or none of the similar passages were deemed relevant. Before answering, analyze step by step: 1. in 12 words or less, return the theme (if relevant) as it relates to attitudes towards refugees in Malaysia, or return None. Do not give a generic theme like \"attitudes towards refugees in Malaysia\", but provide a specific single theme. If irrelevant, return None for all further questions. If relevant, 2. Whose attitudes are being reflected? Examples: the government, Malaysians, NGOs, the author. 3. Who is the target of the attitudes? Examples: the Rohingya, the government, UNHCR. 4. What is the valence of attitudes towards the target, if any?:  \"Sympathetic.\", \"Hostile.\", or \"N/A\". Once again, the passage to code is ### ' + str(excerpt)+ ' ###  Finally, Respond ONLY in the following python dictionary format: {\"1. Theme\": None/stringval1, \"2. Whose Attitude?\":None/stringval2,\"3. Target\":None/stringval3, 4. Valence\": \"Sympathetic.\"/\"Hostile.\"/\"N/A\"}'  }]\n",
        "            output = openai.ChatCompletion.create(model=model, temperature=temperature, messages=input ).choices[0].message['content']\n",
        "            print(input)\n",
        "            return output\n",
        "    except Exception as e:\n",
        "            print(f\"Error processing row {idx}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(merged, 2)]\n",
        "    for segment in segments:\n",
        "      segment['codingd'] = None\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['codingd'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['text','codesum','summary']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'codingd'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "merged=pd.concat(segments).reset_index()\n",
        "#['level_0','index']\n",
        "merged['index'] = merged.index\n",
        "for idx, row in merged[merged['codingd'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['codesum'],row['summary'])\n",
        "    merged.at[idx, 'codingd'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in merged[merged['codingd'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['codesum'],row['summary'])\n",
        "    merged.at[idx,'codingd'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJyudT7HzIP_"
      },
      "outputs": [],
      "source": [
        "merged.loc[merged['Cluster'].isna(), 'codingd'] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA6euV6xNLkb"
      },
      "outputs": [],
      "source": [
        "merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJmONQifNWHr"
      },
      "outputs": [],
      "source": [
        "merged.to_csv('/content/drive/MyDrive/rawcodes2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnFImdmEXWxm"
      },
      "outputs": [],
      "source": [
        "sampled_df=pd.concat(segments).drop(['index'], axis=1).reset_index().drop('index', axis=1)\n",
        "#['level_0','index']\n",
        "sampled_df['index'] = sampled_df.index\n",
        "for idx, row in sampled_df[sampled_df['codingd'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['codesum'],row['summary'])\n",
        "    sampled_df.at[idx, 'codingd'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "for idx, row in sampled_df[sampled_df['codingd'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['text'],row['codesum'],row['summary'])\n",
        "    sampled_df.at[idx,'codingd'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJEF6bk6SOEk"
      },
      "outputs": [],
      "source": [
        "sampled_df[['codingd','codesum','text','summary']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Vkyjd7XSTPi"
      },
      "outputs": [],
      "source": [
        "sampled_df.to_csv('/content/drive/MyDrive/halfcoded.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiOkqswOuToT"
      },
      "outputs": [],
      "source": [
        "sampled_df.to_csv('/content/drive/MyDrive/halfcoded.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTUTLVqy7kmL"
      },
      "outputs": [],
      "source": [
        "master_df.iloc[:,10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Dp1FqzH73z-"
      },
      "outputs": [],
      "source": [
        "text\ttitle    codinglist\tcodesum codingd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yMhTz9q83uO"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoKvfso7SPlW"
      },
      "outputs": [],
      "source": [
        "test=sampled_df.iloc[150:255,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEEBziIszrUi"
      },
      "outputs": [],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz9zYVvCzeXn"
      },
      "outputs": [],
      "source": [
        "test=test.drop(columns=['ada_embedding2', 'ada_embedding'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFjgHND4zGG1"
      },
      "outputs": [],
      "source": [
        "sampled_df=pd.read_csv('/content/drive/MyDrive/halfcoded.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_d0WGaPj0Wb"
      },
      "outputs": [],
      "source": [
        "test.to_csv('/content/drive/MyDrive/reviewcontent.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjiXx0G8NYBi"
      },
      "outputs": [],
      "source": [
        "sampled_df=sampled_df[sampled_df.codingd!='{\"1. Theme\": None, \"2. Whose Attitude?\": None, \"3. Target\": None}']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xkSLD0yMk1w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.spatial.distance import jaccard\n",
        "\n",
        "# Pivot the data: themes as rows, articles as columns\n",
        "pivot_df = pd.pivot_table(sampled_df, index='cluster', columns='fulltext', aggfunc='size', fill_value=0)\n",
        "\n",
        "# Convert to binary: 1 if theme is present in article, 0 otherwise\n",
        "binary_df = pivot_df.applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Calculate Jaccard distances\n",
        "jaccard_distances = pairwise_distances(binary_df.values, metric = \"jaccard\")\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "dist_matrix = pd.DataFrame(jaccard_distances, index=binary_df.index, columns=binary_df.index)\n",
        "\n",
        "print(dist_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvI6wbuSQxPt"
      },
      "outputs": [],
      "source": [
        "jaccard_distances = pairwise_distances(binary_df.values, metric = \"jaccard\")**15\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "dist_matrix = pd.DataFrame(jaccard_distances, index=binary_df.index, columns=binary_df.index)\n",
        "\n",
        "print(dist_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3w4EnsXM1TD"
      },
      "outputs": [],
      "source": [
        "jaccard_distances.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wji0QdlNTWZC"
      },
      "outputs": [],
      "source": [
        "subsamp = sampled_df.groupby('cluster').apply(lambda x: x.sample(4)).reset_index(drop=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB1C-ZUfOCvK"
      },
      "outputs": [],
      "source": [
        "reducer = umap.UMAP(n_components=2,metric='cosine',n_neighbors=6, min_dist = 0.0001)\n",
        "embedding = reducer.fit_transform(jaccard_distances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGP-IEizSTNS"
      },
      "outputs": [],
      "source": [
        "sampled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrBWTWnZTeG1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming 'subsamp' is your DataFrame\n",
        "\n",
        "# Function to add missing quotations to values\n",
        "def add_missing_quotations(value):\n",
        "    # Check if value starts with { and ends with }\n",
        "    if value.startswith(':') and value.endswith('}'):\n",
        "        # Find all key-value pairs in the string\n",
        "        pattern = r'\"(\\d+\\.\\s*[\\w\\s]+)\":\\s*(.+?)(?=,|$)'\n",
        "        matches = re.findall(pattern, value)\n",
        "\n",
        "        # Replace values without quotations\n",
        "        for match in matches:\n",
        "            key, val = match\n",
        "            quoted_val = f'\"{val}\"'\n",
        "            value = value.replace(f'{key}: {val}', f'{key}: {quoted_val}')\n",
        "\n",
        "    return value\n",
        "\n",
        "# Apply the function to the 'codingd' column\n",
        "subsamp['codingd'] = subsamp['codingd'].apply(add_missing_quotations)\n",
        "\n",
        "# Print the modified DataFrame\n",
        "subsamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvNGeSGyZF-n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "def try_literal_eval(s):\n",
        "    try:\n",
        "        return ast.literal_eval(s)\n",
        "    except ValueError:\n",
        "        return {}  # or some other value that makes sense\n",
        "\n",
        "# Assuming your dataframe is called subsamp and the column with string dictionaries is called 'codingd'\n",
        "subsamp['codingd'] = subsamp['codingd'].apply(try_literal_eval)\n",
        "\n",
        "# This will convert the dictionaries into a DataFrame and concatenate it with the original DataFrame\n",
        "subsamp = pd.concat([subsamp.drop(['codingd'], axis=1), subsamp['codingd'].apply(pd.Series)], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xynd8nNZoY-"
      },
      "outputs": [],
      "source": [
        "subsamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmul6Mg7YcQ9"
      },
      "outputs": [],
      "source": [
        "subsamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGszP9mqV2X0"
      },
      "outputs": [],
      "source": [
        "# Create labels dataframe with 'cluster', 'codingd', and 'cluster2' columns from subsamp\n",
        "labels = subsamp[['cluster', '1. Theme','2. Whose Attitude?', '3. Target', 'cluster2']].copy()\n",
        "\n",
        "labels = labels.groupby('cluster').agg({\n",
        "    '1. Theme': lambda x: ', '.join(x.dropna()) if x.dtype != 'float64' else \"\",\n",
        "    '2. Whose Attitude?': lambda x: ', '.join(x.dropna()) if x.dtype != 'float64' else \"\",\n",
        "    '3. Target': lambda x: ', '.join(x.dropna()) if x.dtype != 'float64' else \"\",\n",
        "    'cluster2': 'min'\n",
        "}).reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw7IYp41gxgY"
      },
      "outputs": [],
      "source": [
        "subsamp[['cluster', '1. Theme','2. Whose Attitude?', '3. Target', 'codingd','cluster2']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r6hIEWRJwji"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNRRd1JjToAi"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4arv-UfMTYaq"
      },
      "outputs": [],
      "source": [
        "processed_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CMNN33Fif8xY"
      },
      "outputs": [],
      "source": [
        "#@title Find Overarching Themes\n",
        "from retry import retry\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.1\n",
        "messages = []\n",
        "@retry(delay=2, tries=3)\n",
        "def process_excerpt(text):\n",
        "    try:\n",
        "        output = openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\":  ' List of themes: ' + str(text)+ ' ### In 16 words or less, summarize the overarching theme of this cluster of subthemes\"'  }]).choices[0].message['content']\n",
        "        print(text,output)\n",
        "        print(text,output)\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "processed_strings = [process_excerpt(label) for label in labels['1. Theme']]\n",
        "print(processed_strings)\n",
        "themelist=', '.join(processed_strings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwpvV5bNRSSg"
      },
      "outputs": [],
      "source": [
        " for label in labels['2. Whose Attitude?']:\n",
        "  print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uckU6c_CRUv2"
      },
      "outputs": [],
      "source": [
        "#@title Find Overarching Themes\n",
        "from retry import retry\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.1\n",
        "messages = [{\"role\": \"system\", \"content\": \"In 8 words or less, summarize who is being described or write None.\"}]\n",
        "@retry(delay=2, tries=3)\n",
        "def process_excerpt(text):\n",
        "    if len(text) <2:\n",
        "      text = \"None\"\n",
        "    try:\n",
        "        output = openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\":  ' List of people or groups: ' + str(text) + 'In 8 words or less, summarize who is being described or write \"None\" if no one is.' }]).choices[0].message['content']\n",
        "        print(text)\n",
        "        print(output)\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "processed_strings2 = [process_excerpt(label) for label in labels['2. Whose Attitude?']]\n",
        "print(processed_strings2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IEm7F6TyV2Q6"
      },
      "outputs": [],
      "source": [
        "#@title Find Overarching Themes\n",
        "from retry import retry\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.1\n",
        "messages = [{\"role\": \"system\", \"content\": \"In 8 words or less, summarize who is being described or write None.\"}]\n",
        "@retry(delay=2, tries=3)\n",
        "def process_excerpt(text):\n",
        "    if len(text) <2:\n",
        "      text = \"None\"\n",
        "    try:\n",
        "        output = openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\":  ' List of people or groups: ' + str(text) + 'In 8 words or less, summarize who is being described or write \"None\" if no one is.' }]).choices[0].message['content']\n",
        "        print(text)\n",
        "        print(output)\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "processed_strings3 = [process_excerpt(label) for label in labels['3. Target']]\n",
        "print(processed_strings3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqXeCu42Z4WX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming processed_strings, processed_strings2, and processed_strings3 are already defined\n",
        "\n",
        "# Verify that the lengths of the lists match the number of rows in the DataFrame\n",
        "if len(processed_strings) == len(processed_strings2) == len(processed_strings3) == len(labels):\n",
        "    print(\"Lists and DataFrame have matching lengths. Populating DataFrame columns.\")\n",
        "    # Assign values from the lists to the respective columns\n",
        "    labels['1. Theme'] = processed_strings\n",
        "    labels['2. Whose Attitude?'] = processed_strings2\n",
        "    labels['3. Target'] = processed_strings3\n",
        "else:\n",
        "    print(\"Error: Lengths of lists and DataFrame do not match.\")\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRz2ZgVNazsw"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG6MDg5wOG7w"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Convert the dictionaries in 'codingd' to strings if they're not already\n",
        "\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add scatter trace with medium sized markers\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        mode='markers',\n",
        "        x=embedding[:, 0],\n",
        "        y=embedding[:, 1],\n",
        "        marker=dict(\n",
        "            size=10,\n",
        "            color=labels['cluster2'],  # set color to 'cluster2' labels\n",
        "            colorscale='Turbo',  # choose a colorscale\n",
        "            colorbar=dict(title=\"Coarse Clusters\")  # add a colorbar with a title\n",
        "        ),\n",
        "        text=['1. Theme: ' + str(theme) + '<br>2. Whose Attitude?: ' + str(attitude) + '<br>3. Target: ' + str(target) for theme, attitude, target in zip(labels['1. Theme'], labels['2. Whose Attitude?'], labels['3. Target'])], # set the hover text\n",
        "        hoverinfo='text'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Set title and axis labels\n",
        "fig.update_layout(\n",
        "    title=\"Co-occuring themes\",\n",
        "    hovermode=\"closest\",\n",
        "    autosize=False,\n",
        "    width=1300,  # set figure width\n",
        "    height=800,  # set figure height\n",
        "    title_font=dict(size=44)\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC-4keEd5LFv"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKvBlxqUz8zF"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from retry import retry\n",
        "\n",
        "model=\"text-embedding-ada-002\"\n",
        "# define the function to apply to each row\n",
        "sampled_df['ada_embedding']=None\n",
        "\n",
        "@retry(delay=3, tries=2)\n",
        "def process_excerpt(text):\n",
        "    try:\n",
        "      text = text.replace(\"\\n\", \" \")\n",
        "      text = text.strip(\"{}\")\n",
        "\n",
        "      output = openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n",
        "      if random.random() < 0.01:\n",
        "        print(text)\n",
        "        print(iter)\n",
        "      return output\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(sampled_df, 10)]\n",
        "    print(segments[1])\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['ada_embedding'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['codingd']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'ada_embedding'] = result\n",
        "            #print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "sampled_df=pd.concat(segments).reset_index()\n",
        "for idx, row in sampled_df[sampled_df['ada_embedding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['codingd'])\n",
        "    sampled_df.at[idx, 'ada_embedding'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K3Qd8l10mtM"
      },
      "outputs": [],
      "source": [
        "for idx, row in sampled_df[sampled_df['ada_embedding'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['codingd'])\n",
        "    sampled_df.at[idx, 'ada_embedding'] = result\n",
        "    print(f\"Row {idx} processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXJeaR85HF10"
      },
      "outputs": [],
      "source": [
        "sampled_df=pd.concat(segments).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9yV80jtG_4g"
      },
      "outputs": [],
      "source": [
        "  sampled_df.loc[idx, 'ada_embedding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKeWqBq6Fwlc"
      },
      "outputs": [],
      "source": [
        "  sampled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOG5y-fipbde"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn\n",
        "import umap\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdDe9QSaVE5g"
      },
      "outputs": [],
      "source": [
        "sampled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE2ObWWYUxIk"
      },
      "outputs": [],
      "source": [
        "data = sampled_df.ada_embedding.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11vJZNiLjLZ9"
      },
      "outputs": [],
      "source": [
        "sampled_df['cluster'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo_0b7F6brU1"
      },
      "outputs": [],
      "source": [
        "most_common_value = sampled_df['codingd'].value_counts().idxmax()\n",
        "\n",
        "# Subsetting the dataframe with the most common 'codingd' value\n",
        "subset_df = sampled_df[sampled_df['codingd'] == most_common_value]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7qvwPHnRNZh"
      },
      "outputs": [],
      "source": [
        "model=\"text-embedding-ada-002\"\n",
        "# define the function to apply to each row\n",
        "text = '{\"1. Theme\": None, \"2. Whose Attitude?\": None, \"3. Target\": None}'\n",
        "text = text.replace(\"\\n\", \" \")\n",
        "openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24zxRw9mLVcU"
      },
      "outputs": [],
      "source": [
        "model=\"text-embedding-ada-002\"\n",
        "# define the function to apply to each row\n",
        "text = '{\"1. Theme\": None, \"2. Whose Attitude?\": None, \"3. Target\": None}'\n",
        "text = text.replace(\"\\n\", \" \")\n",
        "openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp9uXGuwbs6s"
      },
      "outputs": [],
      "source": [
        "sampled_df[['summary','codingd']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICNHPzrSwv-q"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "sampled_df['ada_embedding'] = sampled_df['ada_embedding'].apply(ast.literal_eval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eruiTi54zb9I"
      },
      "outputs": [],
      "source": [
        "sampled_df=sampled_df[ ~sampled_df.codingd.str.contains('\"Attitudes towards refugees in Malaysia\"') ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6Q8AMFupsy5"
      },
      "outputs": [],
      "source": [
        "sampled_df.loc[ sampled_df.codingd.str.contains('\"Attitudes towards refugees in Malaysia\"'),['codingd','text'] ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdyDCzpAUwjK"
      },
      "outputs": [],
      "source": [
        "#create fine clusters to check for droppable clusters\n",
        "reducer = umap.UMAP(n_components=6,metric='cosine',n_neighbors=20, min_dist = 0.0001)\n",
        "embedding = reducer.fit_transform(data)\n",
        "embedding.shape\n",
        "!pip install hdbscan\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_eaSaYX0coR"
      },
      "outputs": [],
      "source": [
        "import hdbscan\n",
        "hdbscan = hdbscan.HDBSCAN(min_cluster_size=100, min_samples = 100)\n",
        "labels = hdbscan.fit_predict(embedding)\n",
        "hdbscan.condensed_tree_.plot(select_clusters=True)\n",
        "np.unique(labels)\n",
        "###fine clusters for manual removal of irrelevant content: min_clust 20 min_samp 1, 389 clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDfA1nn3dGJe"
      },
      "outputs": [],
      "source": [
        "sampled_df['cluster2']= labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuoOBVGIcPdk"
      },
      "outputs": [],
      "source": [
        "original_df = sampled_df.__self__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teSgVKTHcSmT"
      },
      "outputs": [],
      "source": [
        "sampled_df=sampled_df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQVZ20mZavTN"
      },
      "outputs": [],
      "source": [
        "sampled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhMQy6u2dIh7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
        "chroma_client = chromadb.Client()\n",
        "embedding_function = OpenAIEmbeddingFunction(api_key=os.environ.get('OPENAI_API_KEY'), model_name=EMBEDDING_MODEL)\n",
        "articledb = chroma_client.create_collection(name='articles', embedding_function=embedding_function)\n",
        "# Iterate over the rows of the dataframe and create lists for documents, metadatas, and ids\n",
        "documents = []\n",
        "metadatas = []\n",
        "embeddings = []\n",
        "ids = []\n",
        "\n",
        "for index, row in sampled_df.iterrows():\n",
        "    documents.append(row['codingd'])\n",
        "    embeddings.append(row['ada_embedding'])\n",
        "    metadata = {\n",
        "        \"cluster\": row['cluster']  # Convert list to string\n",
        "    }\n",
        "    metadatas.append(metadata)\n",
        "    ids.append(f\"{index}\")  # Assuming you want to use \"id1\", \"id2\", \"id3\", ...\n",
        "\n",
        "# Add the documents, metadatas, and ids to the vector database\n",
        "articledb.add(documents=documents, metadatas=metadatas,embeddings=embeddings, ids=ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghNCdKSKd655"
      },
      "outputs": [],
      "source": [
        " articledb.query(query_texts=['hostility to refugees'],n_results=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOhcfY0F6TKs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4XEArj_ezth"
      },
      "outputs": [],
      "source": [
        "202},\n",
        "   {'cluster': 204},\n",
        "   {'cluster': 206},\n",
        "   {'cluster': -1},\n",
        "   {'cluster': -1},\n",
        "   {'cluster': 204},\n",
        "   {'cluster': -1},\n",
        "   {'cluster': -1},\n",
        "   {'cluster': 140},\n",
        "   {'cluster': 183},\n",
        "   {'cluster': 140},"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlrRXKPxe9NC"
      },
      "outputs": [],
      "source": [
        " sampled_df.loc[sampled_df['cluster']==204,['codingd','text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDve8YsrfHZ1"
      },
      "outputs": [],
      "source": [
        "sampled_df[sampled_df['cluster'] != -1]['cluster'].value_counts().nlargest(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E2pyvxHe2vv"
      },
      "outputs": [],
      "source": [
        "top_clusters = sampled_df[sampled_df['cluster2'] != -1]['cluster2'].value_counts().nlargest(8).index.tolist()\n",
        "\n",
        "\n",
        "# Subset the dataframe to include rows with the top 10 clusters excluding the most common cluster\n",
        "focus_df = sampled_df[(sampled_df['cluster2'].isin(top_clusters)) & (sampled_df['cluster2'] != -1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmbyP6rVe8R5"
      },
      "outputs": [],
      "source": [
        "focus_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA5qv4_p6pZO"
      },
      "outputs": [],
      "source": [
        "clusters = [108,106,29,28]\n",
        "\n",
        "focus_df = sampled_df[sampled_df['cluster'].isin(clusters)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xhMRgskjS2-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssLxdWDMguZk"
      },
      "outputs": [],
      "source": [
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.5\n",
        "messages = [{\"role\": \"system\", \"content\": \"You are a professor of sociology doing a thematic analysis of attitudes towards refugees in Malaysian news media. You will ensure that every overarching theme is distinct from all others, emphasizing what makes this cluster distinct. Some questions to consider: Who is the target of the attitudes? Are the attitudes sympathetic or hostile?  Whose attitudes? Are the attitudes being criticized by a third party (like the author) or voiced directly? Avoid repetitive language like 'attitudes towards'\"}]\n",
        "\n",
        "@retry(delay=1, tries=3)\n",
        "def process_excerpt(text):\n",
        "    try:\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\":  ' List of thematic analysis codes: ' + str(text)+ ' ### In 24 words or less, summarize the overarching theme of this cluster of codes. In other words, what facet of attitudes towards refugees in Malaysia is most prominently explored in these themes? '  }]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "processed_strings = [process_excerpt(', '.join(focus_df[focus_df['cluster2'] == label][:10]['codingd'])) for label in focus_df['cluster2'].unique()]\n",
        "themelist=', '.join(processed_strings)\n",
        "print(themelist)\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "temperature = 0.9\n",
        "@retry(delay=1, tries=3)\n",
        "def process_excerpt(text, firstpass):\n",
        "    try:\n",
        "        user_message = ' List of thematic analysis codes: ' + str(text) + ' ### First pass at list of overarching themes for other clusters: ' + str(firstpass) + ' ### In 14 words or less, summarize the overarching theme of this cluster of subthemes. What facet of attitudes towards refugees in Malaysia is most prominent and distinctive? Avoid overlap, redundancy, and repetition with the themes from other clusters in first pass list. Instead, focus on what makes this overarching theme different from all the others. Do not start your response with \"The overarching theme is\", but simply state the theme.'\n",
        "        return openai.ChatCompletion.create(model=model, temperature=temperature, messages=messages + [{\"role\": \"user\", \"content\": user_message}]).choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        return None\n",
        "\n",
        "processed_strings2 = []\n",
        "labels_unique = focus_df['cluster2'].unique()\n",
        "for i, label in enumerate(labels_unique):\n",
        "    firstpass = ', '.join([processed_strings[j] for j in range(len(processed_strings)) if j != i])\n",
        "    processed_string = process_excerpt(', '.join(focus_df[focus_df['cluster2'] == label][:10]['codingd']), firstpass)\n",
        "    print(processed_string)\n",
        "    processed_strings2.append(processed_string)\n",
        "\n",
        "\n",
        "processed_strings3 = []\n",
        "labels_unique = focus_df['cluster2'].unique()\n",
        "for i, label in enumerate(labels_unique):\n",
        "    firstpass = ', '.join([processed_strings2[j] for j in range(len(processed_strings2)) if j != i])\n",
        "    processed_string = process_excerpt(', '.join(focus_df[focus_df['cluster2'] == label][:10]['codingd']), firstpass)\n",
        "    print(processed_string)\n",
        "    processed_strings3.append(processed_string)\n",
        "\n",
        "focus_df['labels2'] = focus_df['cluster2'].map({label: processed_strings3[i] for i, label in enumerate(focus_df['cluster2'].unique())})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8lVXMyIfMJ0"
      },
      "outputs": [],
      "source": [
        "len(processed_strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzvrCv36grA-"
      },
      "outputs": [],
      "source": [
        "focus_df['date'] = pd.to_datetime(sampled_df['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLFYTI9K0Msb"
      },
      "outputs": [],
      "source": [
        "labels = focus_df['labels2'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MXbGIN9FVbB"
      },
      "outputs": [],
      "source": [
        "focus_df.loc[(focus_df.date< '2022-06-18')&(focus_df.cluster==40),['codingd','text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU-qpqXdplUK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "window_size = 100\n",
        "labels = focus_df['labels2'].unique()\n",
        "labels[1]='Negative attitudes towards Rohingya refugees\\' presence in Malaysia from the Malaysian government'\n",
        "# Calculate moving averages with Gaussian weighting for each label and store in DataFrame\n",
        "moving_avgs_df = pd.concat(\n",
        "    [focus_df[focus_df['labels2'] == label].set_index('date').resample('D').size().rolling(window=window_size, win_type='gaussian').mean(std=window_size/4) for label in labels],\n",
        "    axis=1,\n",
        "    keys=labels\n",
        ")\n",
        "\n",
        "# Normalize each value to get relative frequencies\n",
        "moving_avgs_df = moving_avgs_df.div(moving_avgs_df.sum(axis=1), axis=0)\n",
        "\n",
        "# Multiply by 100 to get percentages\n",
        "moving_avgs_df = moving_avgs_df.multiply(100)\n",
        "\n",
        "# Define a contrasty colormap\n",
        "colors = ['rgb(0, 0, 255)', 'rgb(255, 0, 0)', 'rgb(0, 200, 0)', 'rgb(255, 180, 0)', 'rgb(255, 0, 255)', 'rgb(0, 255, 255)','rgb(127, 127, 127)','rgb(127, 0, 255)']\n",
        "\n",
        "# Plot\n",
        "fig = go.Figure()\n",
        "for i, col in enumerate(moving_avgs_df.columns):\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=moving_avgs_df.index,\n",
        "        y=moving_avgs_df[col],\n",
        "        mode='lines',\n",
        "        name=col,\n",
        "        stackgroup='one',  # this line will make the plot stacked area chart\n",
        "        line_color=colors[i % len(colors)]  # set line color from the colormap\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Moving average of the number of rows for each cluster over time',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Moving Average Count',\n",
        "    xaxis_range=['2017-01-01', '2023-06-18'],\n",
        "    width=2800,  # Set the width of the figure in pixels\n",
        "    height=1000  # Set the height of the figure in pixels\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlGUpFSY8D90"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "window_size = 100\n",
        "labels = focus_df['labels2'].unique()\n",
        "\n",
        "# Calculate moving averages with Gaussian weighting for each label and store in DataFrame\n",
        "moving_avgs_df = pd.concat(\n",
        "    [focus_df[focus_df['labels2'] == label].set_index('date').resample('D').size().rolling(window=window_size, win_type='gaussian').mean(std=window_size/4) for label in labels],\n",
        "    axis=1,\n",
        "    keys=labels\n",
        ")\n",
        "\n",
        "# Normalize each value to get relative frequencies\n",
        "moving_avgs_df = moving_avgs_df.div(moving_avgs_df.sum(axis=1), axis=0)\n",
        "\n",
        "# Multiply by 100 to get percentages\n",
        "moving_avgs_df = moving_avgs_df.multiply(100)\n",
        "# Multiply by 100 to get percentages\n",
        "\n",
        "# Define a contrasty colormap\n",
        "colors = ['rgb(0, 0, 255)', 'rgb(255, 0, 0)', 'rgb(0, 200, 0)', 'rgb(255, 180, 0)', 'rgb(255, 0, 255)', 'rgb(0, 255, 255)','rgb(127, 127, 127)','rgb(127, 0, 255)']\n",
        "\n",
        "# Plot\n",
        "fig = go.Figure()\n",
        "for i, col in enumerate(moving_avgs_df.columns):\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=moving_avgs_df.index,\n",
        "        y=moving_avgs_df[col],\n",
        "        mode='lines',\n",
        "        name=col,\n",
        "        stackgroup='one',  # this line will make the plot stacked area chart\n",
        "        line_color=colors[i % len(colors)]  # set line color from the colormap\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Moving average of the relative frequency of a given theme',\n",
        "    title_font=dict(size=36),  # Adjust title font size\n",
        "    xaxis_title='Date',\n",
        "    xaxis_title_font=dict(size=30),  # Adjust x-axis label font size\n",
        "    yaxis_title='Percent of Passages',\n",
        "    yaxis_title_font=dict(size=30),  # Adjust y-axis label font size\n",
        "    xaxis_range=['2017-01-01', '2023-06-18'],\n",
        "    width=3000,  # Set the width of the figure in pixels\n",
        "    height=1000  # Set the height of the figure in pixels\n",
        ")\n",
        "\n",
        "fig.update_xaxes(tickfont=dict(size=25))  # Adjust x-axis tick label font size\n",
        "fig.update_yaxes(tickfont=dict(size=25))  # Adjust y-axis tick label font size\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-F5KJyoreZq"
      },
      "outputs": [],
      "source": [
        "focus_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Lg0IJbtnKLt"
      },
      "outputs": [],
      "source": [
        "focus_df.loc[focus_df['cluster'] ==85,['codingd','text']][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qGNWLFxnXK8"
      },
      "outputs": [],
      "source": [
        "132    274\n",
        "87     258\n",
        "138    236\n",
        "70     190\n",
        "33     125\n",
        "0      121\n",
        "35     113\n",
        "76     109\n",
        "80     103\n",
        "126    102"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAxncDThqETs"
      },
      "outputs": [],
      "source": [
        "focus_df.ada_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKJ4Ojr7oIMP"
      },
      "outputs": [],
      "source": [
        "{\"1. Theme\": None, \"2. Whose Attitude?\": None, \"3. Target\": None}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4MGmPxgobj1"
      },
      "outputs": [],
      "source": [
        "{\"1. Theme\": None, \"2. Whose Attitude?\": None, \"3. Target\": None}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYbW8ejt_HXw"
      },
      "source": [
        "# Document Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZutsE0V1_Gf7"
      },
      "outputs": [],
      "source": [
        "\n",
        "grouped_df = test.groupby('fulltext')['cluster'].unique().reset_index()\n",
        "# Convert the aggregated sets to lists\n",
        "grouped_df['clusters'] = grouped_df['cluster'].apply(list)\n",
        "# Drop the now unnecessary 'cluster' column\n",
        "grouped_df.drop(columns=['cluster'], inplace=True)\n",
        "\n",
        "grouped_df  # Display the first few rows of the resulting dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltpvPg1GG5Gb"
      },
      "outputs": [],
      "source": [
        "grouped_df.cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-7zym-BHFb0"
      },
      "outputs": [],
      "source": [
        "isinstance(grouped_df['cluster'].iloc[0], str) and grouped_df['cluster'].iloc[0].strip()[0] == '[' and grouped_df['cluster'].iloc[0].strip()[-1] == ']'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGmIm5-aIVcy"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values in the 'cluster' column\n",
        "nan_check = grouped_df['cluster'].isnull().any()\n",
        "\n",
        "# Identify the type of each element in the 'cluster' column\n",
        "types_in_cluster_column = grouped_df['cluster'].apply(type).unique()\n",
        "\n",
        "print(f\"NaN values present: {nan_check}\")\n",
        "print(f\"Types in the 'cluster' column: {types_in_cluster_column}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9y65J47_cqz"
      },
      "outputs": [],
      "source": [
        "grouped_df = test.groupby('fulltext').agg({\n",
        "    'cluster': lambda x: list(x.unique()),\n",
        "    'title': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "# Add a new column 'textlen' to store the character length of the 'fulltext' strings\n",
        "grouped_df['textlen'] = grouped_df['fulltext'].apply(len)\n",
        "\n",
        "grouped_df  # Display the resulting dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsAtlGUWHP_k"
      },
      "outputs": [],
      "source": [
        "grouped_df=grouped_df.loc[grouped_df.fulltext.str.len()>1200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoypKkfeV_SN"
      },
      "outputs": [],
      "source": [
        "test[test.cluster==10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMIlEgzwRYZV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHAG7_Z4FZJ_"
      },
      "outputs": [],
      "source": [
        "!pip install apricot-select"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B5vN_grFg6F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def convert_to_incidence_matrix(grouped_df, cluster_column):\n",
        "    # Identify all unique clusters across all articles\n",
        "    unique_clusters = set(cluster for clusters in grouped_df[cluster_column] for cluster in clusters)\n",
        "\n",
        "    # Create a binary incidence matrix\n",
        "    incidence_matrix = pd.DataFrame(0, index=grouped_df.index, columns=sorted(unique_clusters))\n",
        "\n",
        "    # Fill the incidence matrix with 1's where the article has the cluster\n",
        "    for index, row in grouped_df.iterrows():\n",
        "        incidence_matrix.loc[index, row[cluster_column]] = 1\n",
        "\n",
        "    return incidence_matrix\n",
        "\n",
        "# Example usage:\n",
        "# Suppose 'grouped_df' is your dataframe and 'cluster' is the column name containing clusters as lists\n",
        "# grouped_df = pd.DataFrame({\n",
        "#     'cluster': [[1, 2], [1], [2, 3]]\n",
        "# })\n",
        "incidence_matrix = convert_to_incidence_matrix(grouped_df, 'cluster')\n",
        "print(incidence_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVEeJNXURgK4"
      },
      "outputs": [],
      "source": [
        "from apricot import FeatureBasedSelection\n",
        "\n",
        "# Assuming incidence_matrix and grouped_df are defined as before\n",
        "# Calculate the length of each article\n",
        "article_lengths = grouped_df['fulltext'].apply(len).values\n",
        "\n",
        "# Normalize the sample costs so that the mean is approximately 1\n",
        "normalized_costs = article_lengths / article_lengths.mean()\n",
        "\n",
        "# Initialize the FeatureBasedSelection object with the 'sqrt' concave function\n",
        "selector = FeatureBasedSelection(n_samples=30, concave_func='sqrt', verbose=True)\n",
        "\n",
        "# Fit the model on the binary incidence matrix to find the subset that maximizes the submodular function\n",
        "# using the normalized costs as a knapsack constraint\n",
        "selector.fit(incidence_matrix.values, sample_cost=normalized_costs)\n",
        "\n",
        "# Extract the subset using the transform method\n",
        "selected_features = selector.transform(incidence_matrix.values)\n",
        "\n",
        "# Get the indices of the selected articles\n",
        "selected_indices = selector.ranking\n",
        "\n",
        "# Use the indices to extract the corresponding rows from the original DataFrame\n",
        "selected_articles = grouped_df.iloc[selected_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn-V7sBrQYW7"
      },
      "outputs": [],
      "source": [
        "# Assuming that 'selected_indices' is the array of indices for the selected articles\n",
        "# and 'grouped_df' is the original dataframe.\n",
        "\n",
        "# 1) Subset the grouped_df dataframe with the 20 selected articles\n",
        "selected_articles_df = grouped_df.iloc[selected_indices]\n",
        "\n",
        "# For comparison, select a random subset of 20 articles from the original dataframe\n",
        "random_articles_df = grouped_df.sample(n=30, random_state=1)\n",
        "\n",
        "# Function to count the occurrences of each cluster\n",
        "def count_clusters(df, cluster_column):\n",
        "    cluster_counts = df[cluster_column].explode().value_counts().sort_index()\n",
        "    return cluster_counts\n",
        "\n",
        "# Count the occurrences of each cluster in the selected articles\n",
        "selected_cluster_counts = count_clusters(selected_articles_df, 'cluster')\n",
        "\n",
        "# Count the occurrences of each cluster in the random set of articles\n",
        "random_cluster_counts = count_clusters(random_articles_df, 'cluster')\n",
        "\n",
        "# Now we'll plot the histograms using matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up the figure and axis\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 7), sharey=True)\n",
        "\n",
        "# Plot histogram for selected articles\n",
        "ax[0].bar(selected_cluster_counts.index, selected_cluster_counts.values)\n",
        "ax[0].set_title('Histogram of Clusters for Selected Articles')\n",
        "ax[0].set_xlabel('Cluster')\n",
        "ax[0].set_ylabel('Count')\n",
        "\n",
        "# Plot histogram for random articles\n",
        "ax[1].bar(random_cluster_counts.index, random_cluster_counts.values)\n",
        "ax[1].set_title('Histogram of Clusters for Random Articles')\n",
        "ax[1].set_xlabel('Cluster')\n",
        "# ax[1].set_ylabel('Count') # No need for ylabel as it shares with the first histogram\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sZK9r-dUeqn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NqFlpd0TUAV"
      },
      "outputs": [],
      "source": [
        "random_articles_df[:8].to_csv('/content/drive/MyDrive/warmuparticles.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbICq-dZQjdk"
      },
      "outputs": [],
      "source": [
        "selected_articles_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58Hq7UqU1u2Q"
      },
      "source": [
        "# Reembedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Lg1vVhCbfTi"
      },
      "outputs": [],
      "source": [
        "master_df.iloc[:15,15:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQYWrTpX1tiP"
      },
      "outputs": [],
      "source": [
        "master_df=pd.read_csv('/content/drive/MyDrive/master_embed2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JGn_VCE141h"
      },
      "outputs": [],
      "source": [
        "unique_coding_values = master_df['coding'].unique()\n",
        "\n",
        "# Creating a DataFrame from the unique values\n",
        "unique_coding_df = pd.DataFrame(unique_coding_values, columns=['coding'])\n",
        "\n",
        "unique_coding_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGXuM5DS9lSh"
      },
      "outputs": [],
      "source": [
        "client.embeddings.create(input=[text], model=model).data[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X-BjHqx-l8v"
      },
      "outputs": [],
      "source": [
        "del segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3s5mYN185Tu"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from retry import retry\n",
        "\n",
        "model=\"text-embedding-ada-002\"\n",
        "# define the function to apply to each row\n",
        "unique_coding_df['ada_embedding3']=None\n",
        "subarts=unique_coding_df\n",
        "\n",
        "@retry(delay=3, tries=2)\n",
        "def process_excerpt(text):\n",
        "    try:\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        output = client.embeddings.create(input=[text], model=model).data[0].embedding\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing excerpt: {e}\")\n",
        "        return None\n",
        "\n",
        "if 'segments' not in globals():\n",
        "    print('tic')\n",
        "    segments = [df.reset_index(drop=True) for df in np.array_split(subarts, 25)]\n",
        "\n",
        "for segment_idx, segment in enumerate(segments):\n",
        "    print('newseg')\n",
        "    if not segment['ada_embedding3'].isna().any():\n",
        "        print('toc')\n",
        "        continue\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=18) as executor:\n",
        "        # create a list of futures by submitting tasks to the executor only for rows with 'coding' value None\n",
        "        futures = [executor.submit(process_excerpt, *row[['coding']]) for _, row in segment.iterrows()]\n",
        "        # iterate through the futures and assign the results to the correct row in the dataframe\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            idx = futures.index(future)\n",
        "            result = future.result()\n",
        "            segment.at[idx, 'ada_embedding3'] = result\n",
        "            print(f\"Segment {segment_idx} - Row {idx} processed\")\n",
        "\n",
        "# Merge the segments\n",
        "grouped_df=pd.concat(segments)\n",
        "for idx, row in grouped_df[grouped_df['ada_embedding3'].isnull()].iterrows():\n",
        "    result = process_excerpt(row['coding'])\n",
        "    grouped_df.at[idx, 'ada_embedding3'] = result\n",
        "    print(f\"Row {idx} processed\")\n",
        "unique_coding_df.loc[unique_coding_df['ada_embedding3'].isnull(), 'ada_embedding3'] = grouped_df.ada_embedding3.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbry8E2y_Z-f"
      },
      "outputs": [],
      "source": [
        "unique_coding_df.loc[unique_coding_df['ada_embedding3'].isnull(), 'ada_embedding3'] = grouped_df.ada_embedding3.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-w8Cdqd_bpN"
      },
      "outputs": [],
      "source": [
        "master_df = master_df.merge(unique_coding_df[['coding', 'cluster']], on='coding', how='left')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxNcOS6OJNvK"
      },
      "outputs": [],
      "source": [
        "master_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuGTrL-t_y53"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = unique_coding_df.ada_embedding3.to_list()\n",
        "reducer = umap.UMAP(n_components=2,metric='cosine',n_neighbors=10, min_dist = 0.00001)\n",
        "embedding = reducer.fit_transform(data)\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0zHi_J8NCUu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CspGw3CgIXfQ"
      },
      "outputs": [],
      "source": [
        "unique_coding_df.to_csv('/content/drive/MyDrive/embedding3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qveXyLO1WTAl"
      },
      "outputs": [],
      "source": [
        "unique_coding_df=pd.read_csv('/content/drive/MyDrive/embedding3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Huqbl7SUEsWq"
      },
      "outputs": [],
      "source": [
        "!pip install hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xjwc0WVJxxh"
      },
      "outputs": [],
      "source": [
        "embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aet5NAJYL3G8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Plotting the scatter plot\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1])\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.title('UMAP Scatter Plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJfKqaQXAgv6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import hdbscan\n",
        "hdbscan = hdbscan.HDBSCAN(min_cluster_size=10, min_samples = 1)\n",
        "labels = hdbscan.fit_predict(embedding)\n",
        "hdbscan.condensed_tree_.plot(select_clusters=True)\n",
        "np.unique(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUGWG5SrCvMN"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmCa98pdJejh"
      },
      "outputs": [],
      "source": [
        "test=master_df.loc[:,['text','fulltext','title','url','coding','cluster_y']]\n",
        "test = test.rename(columns={'cluster_y': 'cluster'})\n",
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPSHMcSUPok7"
      },
      "outputs": [],
      "source": [
        "test.to_csv('/content/drive/MyDrive/codingclusters.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yycPHyVRgwQ"
      },
      "outputs": [],
      "source": [
        "test=pd.read_csv('/content/drive/MyDrive/codingclusters.csv')\n",
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R31slsbSlBX9"
      },
      "outputs": [],
      "source": [
        "\n",
        "sampled_df[sampled_df['title'].str.contains(\"was looking to flee to neighbouring country, say police\", case=False, na=False)]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvxaWnWs2lUD"
      },
      "outputs": [],
      "source": [
        "aggregated_data = test.groupby('fulltext').agg(paragraphs=('fulltext', 'size'), title=('title', 'first')).reset_index()\n",
        "aggregated_data[aggregated_data.paragraphs<10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLlUWC8ECvwS"
      },
      "outputs": [],
      "source": [
        "unique_coding_df['cluster']=labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPwhQzndLyZD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "929RlXHFLzAc"
      },
      "source": [
        "# Paper 2 Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS-gut0UL6EH"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install python-docx pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b9ZImj5SaSq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def parse_docx_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "\n",
        "    articles = []\n",
        "\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        with docx.open('word/document.xml') as document_xml:\n",
        "            tree = ET.parse(document_xml)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            current_article = {'title': None, 'comment_count': 0}\n",
        "            for child in root.iter():\n",
        "                if child.tag == '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p':\n",
        "                    text = ''.join(node.text for node in child.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    if text.startswith(\"TITLE:\"):\n",
        "                        if current_article['title'] is not None:  # Not the first article\n",
        "                            articles.append(current_article)\n",
        "                        current_article = {'title': text.replace(\"TITLE:\", \"\").strip(), 'comment_count': 0}\n",
        "                    current_article['comment_count'] += len([c for c in child.iter() if c.tag.endswith('commentRangeStart')])\n",
        "\n",
        "            # Add the last article if it exists\n",
        "            if current_article['title'] is not None:\n",
        "                articles.append(current_article)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Example usage\n",
        "\n",
        "file_path = '/content/drive/MyDrive/ROUND 2 - SHARON.docx'\n",
        "comments_count_per_article = parse_docx_comments(file_path)\n",
        "articles_data = parse_docx_comments(file_path)\n",
        "\n",
        "# Convert the list of dictionaries to a pandas DataFrame\n",
        "df_articles = pd.DataFrame(articles_data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_articles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kJBwWnhbB_M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx8FIyOvbCvG"
      },
      "source": [
        "# add thematic counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd5QLh3ubbop"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = {}\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        try:\n",
        "            with docx.open('word/comments.xml') as comments_xml:\n",
        "                tree = ET.parse(comments_xml)\n",
        "                root = tree.getroot()\n",
        "                for comment in root.findall('.//w:comment', ns):\n",
        "                    comment_id = comment.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                    comment_text = ''.join(node.text for node in comment.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    comments[comment_id] = comment_text\n",
        "        except KeyError:  # Handle case where comments.xml does not exist (no comments)\n",
        "            pass\n",
        "    return comments\n",
        "\n",
        "def parse_docx_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = parse_comments(file_path)\n",
        "    articles = []\n",
        "\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        with docx.open('word/document.xml') as document_xml:\n",
        "            tree = ET.parse(document_xml)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            current_article = {'title': None, 'comment_count': 0, 'comment_texts': []}\n",
        "            for child in root.iter():\n",
        "                if child.tag == '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p':\n",
        "                    text = ''.join(node.text for node in child.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    if text.startswith(\"TITLE:\"):\n",
        "                        if current_article['title'] is not None:  # Not the first article\n",
        "                            articles.append(current_article)\n",
        "                        current_article = {'title': text.replace(\"TITLE:\", \"\").strip(), 'comment_count': 0, 'comment_texts': []}\n",
        "                    for c in child.iter():\n",
        "                        if c.tag.endswith('commentRangeStart'):\n",
        "                            comment_id = c.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                            if comment_id in comments:\n",
        "                                current_article['comment_texts'].append(comments[comment_id])\n",
        "                                current_article['comment_count'] += 1\n",
        "\n",
        "            # Add the last article if it exists\n",
        "            if current_article['title'] is not None:\n",
        "                articles.append(current_article)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# List of docx files to process\n",
        "docx_files = ['ROUND 2 - ZAHRA.docx', 'ROUND 2 - SHARON.docx', 'ROUND 2 - AIMI.docx']\n",
        "base_path = '/content/drive/MyDrive/'\n",
        "\n",
        "# Dictionary to aggregate articles data\n",
        "articles_agg = defaultdict(lambda: {'comment_count': 0, 'comment_texts': []})\n",
        "\n",
        "for file_name in docx_files:\n",
        "    file_path = base_path + file_name\n",
        "    articles_data = parse_docx_comments(file_path)\n",
        "    for article in articles_data:\n",
        "        articles_agg[article['title']]['comment_count'] += article['comment_count']\n",
        "        articles_agg[article['title']]['comment_texts'] += article['comment_texts']\n",
        "\n",
        "# Convert the aggregated data into a list of dictionaries, then into a DataFrame\n",
        "articles_list = [{'title': title, 'comment_count': data['comment_count'], 'comment_texts': data['comment_texts']} for title, data in articles_agg.items()]\n",
        "df_articles = pd.DataFrame(articles_list)\n",
        "\n",
        "# Display the DataFrame\n",
        "#df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9XVlXqq8mYk"
      },
      "outputs": [],
      "source": [
        "df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzufatHZbhgH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# Step 1: Create an index mapping for comments\n",
        "comment_index_mapping = {}\n",
        "index_counter = 0\n",
        "for comment_texts in df_articles['comment_texts']:\n",
        "    for text in comment_texts:\n",
        "        if text not in comment_index_mapping:\n",
        "            comment_index_mapping[text] = index_counter\n",
        "            index_counter += 1\n",
        "themes=pd.read_csv('/content/drive/MyDrive/finalthemes.csv')\n",
        "themes['THEMES'] = themes['THEMES'].fillna(method='ffill')\n",
        "theme_changes = themes['THEMES'] != themes['THEMES'].shift(1)\n",
        "def clean_string(s):\n",
        "    # Remove all non-alphanumeric characters (except spaces)\n",
        "    cleaned = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    # Convert to lowercase\n",
        "    cleaned = cleaned.lower()\n",
        "    # Normalize spaces by removing extra spaces\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "    return cleaned\n",
        "# Convert the boolean series to integers (0 or 1) and compute the cumulative sum to create a theme index\n",
        "themes['themeindex'] = theme_changes.cumsum()\n",
        "cleaned_comment_index_mapping = {clean_string(key): value for key, value in comment_index_mapping.items()}\n",
        "cleaned_comment_index_mapping ['former prime minister thinks it is unfair for malaysia to be on the receiving end in the conflict between refugees and their home country'] = cleaned_comment_index_mapping .pop('prime minister thinks it is unfair for malaysia to be on the receiving end in the conflict between refugees and their home country')\n",
        "\n",
        "# Preprocess the 'CODES' column in the themes DataFrame (assuming 'CODES' contains the thematic descriptions to match)\n",
        "# If 'CODES' is actually meant to be matched with another column that contains descriptions, replace 'CODES' with the correct column name\n",
        "themes['processed_CODES'] = themes['CODES'].apply(clean_string)\n",
        "\n",
        "# Map the cleaned 'CODES' column to the indices using the cleaned dictionary\n",
        "themes['code_index'] = themes['processed_CODES'].map(cleaned_comment_index_mapping)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKGm14sv6QvH"
      },
      "outputs": [],
      "source": [
        "cleaned_comment_index_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9uef0mye1AK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles is your DataFrame from the previous steps\n",
        "# and it includes a 'comment_texts' column with lists of comment texts for each article\n",
        "\n",
        "\n",
        "df_articles['comment_indices'] = df_articles['comment_texts'].apply(lambda texts: [comment_index_mapping[text] for text in texts])\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "all_indices = [index for sublist in df_articles['comment_indices'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in df_articles['comment_indices']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "df_articles['weighted_comment_sum'] = weighted_comment_sums\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtTwO3qre2fb"
      },
      "outputs": [],
      "source": [
        "code_to_theme_mapping = themes.assign(code_index=themes['code_index'].astype(float).astype(int)).set_index('code_index')['themeindex'].to_dict()\n",
        "\n",
        "\n",
        "# Step 2: Map comment_indices to theme_indices using the created mapping\n",
        "df_articles['theme_indices'] = df_articles['comment_indices'].apply(lambda indices: [code_to_theme_mapping.get(index) for index in indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S06osIT7fCCq"
      },
      "outputs": [],
      "source": [
        "all_theme_indices = [index for sublist in df_articles['theme_indices'] for index in sublist if index is not None]  # Ensure None values are excluded\n",
        "theme_counts = pd.Series(all_theme_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3 for themes: Sum the number of comments for each article, weighted by 1/n, using theme indices\n",
        "weighted_theme_sums = []\n",
        "for theme_indices in df_articles['theme_indices']:\n",
        "    # Exclude None values to ensure only valid mappings are considered for the weighted sum\n",
        "    valid_theme_indices = [index for index in theme_indices if index is not None]\n",
        "    weighted_sum = sum(1 / theme_counts[index] for index in valid_theme_indices)\n",
        "    weighted_theme_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted theme sums to the DataFrame\n",
        "df_articles['weighted_theme_sum'] = weighted_theme_sums\n",
        "df_articles['theme_indices'] = df_articles['theme_indices'].apply(lambda indices: [0 if index is None else index for index in indices])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6FU_oHWu2vn"
      },
      "outputs": [],
      "source": [
        "merged_df=pd.read_csv('/content/drive/MyDrive/fullcodes.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYWXgFRHnGhS"
      },
      "outputs": [],
      "source": [
        "!pip install pacmap\n",
        "!pip install hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0impGAemW_K"
      },
      "outputs": [],
      "source": [
        "merged_df['embeddings3'] = merged_df['embeddings3'].apply(eval)\n",
        "\n",
        "# Convert these lists to a numpy array for processing with pacmap\n",
        "embeddings_array = np.array(merged_df['embeddings3'].tolist())`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOcQ1yEInbiR"
      },
      "outputs": [],
      "source": [
        "# Step 3: Apply pacmap to obtain a 2D embedding\n",
        "# Initialize PaCMAP instance and fit-transform the data to get 2D embeddings\n",
        "import pacmap\n",
        "import hdbscan\n",
        "\n",
        "pacmap_instance = pacmap.PaCMAP(n_components=6)\n",
        "embeddings_2d = pacmap_instance.fit_transform(embeddings_array)\n",
        "\n",
        "# Step 4: Apply hdbscan on the pacmap embeddings to perform clustering\n",
        "# Initialize HDBSCAN, fit it on the 2D embeddings, and obtain cluster labels\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=5,min_samples=1, gen_min_span_tree=True)\n",
        "cluster_labels = clusterer.fit_predict(embeddings_2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f-P9R4n5QlY"
      },
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmjeKoF98nmM"
      },
      "outputs": [],
      "source": [
        "merged_df['cluster3'] = cluster_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CewmiRouuu-4"
      },
      "outputs": [],
      "source": [
        "grouped_df = merged_df.groupby('fulltext').agg({\n",
        "   # 'cluster4': lambda x: list(x.unique()),  # Get unique values for 'cluster3'\n",
        "    'cluster3': lambda x: list(x.unique()),  # Get unique values for 'cluster3'\n",
        "    'codingd': lambda x: list(x.unique()),   # Get unique values for 'codingd'\n",
        "    'title': 'first'                         # Keep the first title for each group\n",
        "}).reset_index()\n",
        "\n",
        "# Add a new column 'textlen' to store the character length of the 'fulltext' strings\n",
        "grouped_df['textlen'] = grouped_df['fulltext'].apply(len)\n",
        "\n",
        "# Filter rows where the length of 'fulltext' is greater than 1200 characters\n",
        "grouped_df = grouped_df.loc[grouped_df['textlen'] > 1200]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXnC85f8rw66"
      },
      "outputs": [],
      "source": [
        "all_theme_indices = [index for sublist in df_articles['theme_indices'] for index in sublist if index is not None]  # Ensure None values are excluded\n",
        "theme_counts = pd.Series(all_theme_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3 for themes: Sum the number of comments for each article, weighted by 1/n, using theme indices\n",
        "weighted_theme_sums = []\n",
        "for theme_indices in df_articles['theme_indices']:\n",
        "    # Exclude None values to ensure only valid mappings are considered for the weighted sum\n",
        "    valid_theme_indices = [index for index in theme_indices if index is not None]\n",
        "    weighted_sum = sum(1 / theme_counts[index] for index in valid_theme_indices)\n",
        "    weighted_theme_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted theme sums to the DataFrame\n",
        "df_articles['weighted_theme_sum'] = weighted_theme_sums\n",
        "df_articles['theme_indices'] = df_articles['theme_indices'].apply(lambda indices: [0 if index is None else index for index in indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2z_kdpgqEvF"
      },
      "outputs": [],
      "source": [
        "result_df = pd.merge(df_articles, grouped_df[['title', 'fulltext', 'cluster3', 'codingd','textlen']], on='title', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-OoGlJ1wERk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles is your DataFrame from the previous steps\n",
        "# and it includes a 'comment_texts' column with lists of comment texts for each article\n",
        "\n",
        "# Step 1: Create an index mapping for comments\n",
        "comment_index_mapping = {}\n",
        "index_counter = 0\n",
        "for comment_texts in result_df['codingd']:\n",
        "    for text in comment_texts:\n",
        "        if text not in comment_index_mapping:\n",
        "            comment_index_mapping[text] = index_counter\n",
        "            index_counter += 1\n",
        "\n",
        "result_df['AIcode_indices'] = result_df['codingd'].apply(lambda texts: [comment_index_mapping[text] for text in texts])\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "all_indices = [index for sublist in result_df['AIcode_indices'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in result_df['AIcode_indices']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "result_df['AIcodesum'] = weighted_comment_sums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVe5pW8t-WcT"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqEhu7r7p2jl"
      },
      "outputs": [],
      "source": [
        "\n",
        "all_indices = [index for sublist in result_df['cluster3'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in result_df['cluster3']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "result_df['ThemesumAI'] = weighted_comment_sums\n",
        "result_df['AIcodedensity']=result_df['AIcodesum']/result_df['textlen']*1000\n",
        "result_df['AIthemedensity']=result_df['ThemesumAI']/result_df['textlen']*1000\n",
        "result_df['codedensity']=result_df['weighted_comment_sum']/result_df['textlen']*1000\n",
        "result_df['themedensity']=result_df['weighted_theme_sum']/result_df['textlen']*10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8t2_jpftv4a"
      },
      "outputs": [],
      "source": [
        "result_df['themecodes']=(result_df['weighted_comment_sum']*result_df['weighted_theme_sum'])**0.5\n",
        "result_df['AIthemecodes']=(result_df['AIcodesum']*result_df['weighted_comment_sum']*result_df['ThemesumAI'])**0.33"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwSzl2vqxxPe"
      },
      "outputs": [],
      "source": [
        "result_df[['AIcodesum', 'ThemesumAI', 'weighted_theme_sum', 'weighted_comment_sum','AIthemecodes','themecodes']].corr()**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tejg2FVx-FMY"
      },
      "outputs": [],
      "source": [
        "\tAIcodesum\tThemesumAI\tweighted_theme_sum\tweighted_comment_sum\tAIthemecodes\tthemecodes\n",
        "AIcodesum\t1.000000\t0.713444\t0.282207\t0.705462\t0.916821\t0.465143\n",
        "ThemesumAI\t0.713444\t1.000000\t0.122995\t0.409267\t0.768128\t0.222021\n",
        "weighted_theme_sum\t0.282207\t0.122995\t1.000000\t0.622006\t0.390969\t0.932037\n",
        "weighted_comment_sum\t0.705462\t0.409267\t0.622006\t1.000000\t0.835850\t0.838923\n",
        "AIthemecodes\t0.916821\t0.768128\t0.390969\t0.835850\t1.000000\t0.589395\n",
        "themecodes\t0.465143\t0.222021\t0.932037\t0.838923\t0.589395\t1.000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lleHuP3tDC7"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'result_df' has 'textlen' and 'random' columns\n",
        "try:\n",
        "  result_df.reset_index(inplace=True)  # Resets the index and makes it a regular column\n",
        "except:\n",
        "  pass\n",
        "# Including the index and its quadratic term, as well as 'weighted_sum', as independent variables\n",
        "\n",
        "X = result_df['AIthemecodes']\n",
        "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
        "\n",
        "y = result_df['weighted_comment_sum']  # Dependent variable\n",
        "\n",
        "model = sm.OLS(y, X).fit()  # OLS regression\n",
        "print(model.summary())  # Prints a summary of the regression results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF9lzh7isHl9"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Pm-XOrpkMz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles and df_comment_indices are already defined\n",
        "\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "df_articles['cluster'] = df_articles['cluster'].apply(lambda x: eval(x))\n",
        "\n",
        "all_indices = [index for sublist in df_articles['cluster'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in df_articles['cluster']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "df_articles['weighted_comment_sumAI'] = weighted_comment_sums\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLIk7Pg3bDlG"
      },
      "source": [
        "# Raw derivations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeD0iSqNZI7j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = {}\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        try:\n",
        "            with docx.open('word/comments.xml') as comments_xml:\n",
        "                tree = ET.parse(comments_xml)\n",
        "                root = tree.getroot()\n",
        "                for comment in root.findall('.//w:comment', ns):\n",
        "                    comment_id = comment.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                    comment_text = ''.join(node.text for node in comment.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    comments[comment_id] = comment_text\n",
        "        except KeyError:  # Handle case where comments.xml does not exist (no comments)\n",
        "            pass\n",
        "    return comments\n",
        "\n",
        "def parse_docx_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = parse_comments(file_path)\n",
        "    articles = []\n",
        "\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        with docx.open('word/document.xml') as document_xml:\n",
        "            tree = ET.parse(document_xml)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            current_article = {'title': None, 'comment_count': 0, 'comment_texts': []}\n",
        "            for child in root.iter():\n",
        "                if child.tag == '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p':\n",
        "                    text = ''.join(node.text for node in child.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    if text.startswith(\"TITLE:\"):\n",
        "                        if current_article['title'] is not None:  # Not the first article\n",
        "                            articles.append(current_article)\n",
        "                        current_article = {'title': text.replace(\"TITLE:\", \"\").strip(), 'comment_count': 0, 'comment_texts': []}\n",
        "                    for c in child.iter():\n",
        "                        if c.tag.endswith('commentRangeStart'):\n",
        "                            comment_id = c.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                            if comment_id in comments:\n",
        "                                current_article['comment_texts'].append(comments[comment_id])\n",
        "                                current_article['comment_count'] += 1\n",
        "\n",
        "            # Add the last article if it exists\n",
        "            if current_article['title'] is not None:\n",
        "                articles.append(current_article)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# List of docx files to process\n",
        "docx_files = ['ROUND 2 - ZAHRA.docx', 'ROUND 2 - SHARON.docx', 'ROUND 2 - AIMI.docx']\n",
        "base_path = '/content/drive/MyDrive/'\n",
        "\n",
        "# Dictionary to aggregate articles data\n",
        "articles_agg = defaultdict(lambda: {'comment_count': 0, 'comment_texts': []})\n",
        "\n",
        "for file_name in docx_files:\n",
        "    file_path = base_path + file_name\n",
        "    articles_data = parse_docx_comments(file_path)\n",
        "    for article in articles_data:\n",
        "        articles_agg[article['title']]['comment_count'] += article['comment_count']\n",
        "        articles_agg[article['title']]['comment_texts'] += article['comment_texts']\n",
        "\n",
        "# Convert the aggregated data into a list of dictionaries, then into a DataFrame\n",
        "articles_list = [{'title': title, 'comment_count': data['comment_count'], 'comment_texts': data['comment_texts']} for title, data in articles_agg.items()]\n",
        "df_articles = pd.DataFrame(articles_list)\n",
        "\n",
        "# Display the DataFrame\n",
        "df_articles\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-saKdHvo_5Cx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "themes=pd.read_csv('/content/drive/MyDrive/finalthemes.csv')\n",
        "themes['THEMES'] = themes['THEMES'].fillna(method='ffill')\n",
        "theme_changes = themes['THEMES'] != themes['THEMES'].shift(1)\n",
        "def clean_string(s):\n",
        "    # Remove all non-alphanumeric characters (except spaces)\n",
        "    cleaned = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    # Convert to lowercase\n",
        "    cleaned = cleaned.lower()\n",
        "    # Normalize spaces by removing extra spaces\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "    return cleaned\n",
        "# Convert the boolean series to integers (0 or 1) and compute the cumulative sum to create a theme index\n",
        "themes['themeindex'] = theme_changes.cumsum()\n",
        "cleaned_comment_index_mapping = {clean_string(key): value for key, value in comment_index_mapping.items()}\n",
        "cleaned_comment_index_mapping ['former prime minister thinks it is unfair for malaysia to be on the receiving end in the conflict between refugees and their home country'] = cleaned_comment_index_mapping .pop('prime minister thinks it is unfair for malaysia to be on the receiving end in the conflict between refugees and their home country')\n",
        "\n",
        "\n",
        "# Preprocess the 'CODES' column in the themes DataFrame (assuming 'CODES' contains the thematic descriptions to match)\n",
        "# If 'CODES' is actually meant to be matched with another column that contains descriptions, replace 'CODES' with the correct column name\n",
        "themes['processed_CODES'] = themes['CODES'].apply(clean_string)\n",
        "\n",
        "# Map the cleaned 'CODES' column to the indices using the cleaned dictionary\n",
        "themes['code_index'] = themes['processed_CODES'].map(cleaned_comment_index_mapping)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VReV4JTDD0LH"
      },
      "outputs": [],
      "source": [
        "themes.code_index.unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDYg0q69_8f6"
      },
      "outputs": [],
      "source": [
        "cleaned_comment_index_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6lOj53uJXbo"
      },
      "outputs": [],
      "source": [
        "code_to_theme_mapping = themes.assign(code_index=themes['code_index'].astype(float).astype(int)).set_index('code_index')['themeindex'].to_dict()\n",
        "\n",
        "\n",
        "# Step 2: Map comment_indices to theme_indices using the created mapping\n",
        "df_articles['theme_indices'] = df_articles['comment_indices'].apply(lambda indices: [code_to_theme_mapping.get(index) for index in indices])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ3bec5uaWux"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles is your DataFrame from the previous steps\n",
        "# and it includes a 'comment_texts' column with lists of comment texts for each article\n",
        "\n",
        "# Step 1: Create an index mapping for comments\n",
        "comment_index_mapping = {}\n",
        "index_counter = 0\n",
        "for comment_texts in df_articles['comment_texts']:\n",
        "    for text in comment_texts:\n",
        "        if text not in comment_index_mapping:\n",
        "            comment_index_mapping[text] = index_counter\n",
        "            index_counter += 1\n",
        "\n",
        "df_articles['comment_indices'] = df_articles['comment_texts'].apply(lambda texts: [comment_index_mapping[text] for text in texts])\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "all_indices = [index for sublist in df_articles['comment_indices'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in df_articles['comment_indices']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "df_articles['weighted_comment_sum'] = weighted_comment_sums\n",
        "\n",
        "all_theme_indices = [index for sublist in df_articles['theme_indices'] for index in sublist if index is not None]  # Ensure None values are excluded\n",
        "theme_counts = pd.Series(all_theme_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3 for themes: Sum the number of comments for each article, weighted by 1/n, using theme indices\n",
        "weighted_theme_sums = []\n",
        "for theme_indices in df_articles['theme_indices']:\n",
        "    # Exclude None values to ensure only valid mappings are considered for the weighted sum\n",
        "    valid_theme_indices = [index for index in theme_indices if index is not None]\n",
        "    weighted_sum = sum(1 / theme_counts[index] for index in valid_theme_indices)\n",
        "    weighted_theme_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted theme sums to the DataFrame\n",
        "df_articles['weighted_theme_sum'] = weighted_theme_sums\n",
        "df_articles['theme_indices'] = df_articles['theme_indices'].apply(lambda indices: [0 if index is None else index for index in indices])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZcRaD4Fofh7"
      },
      "outputs": [],
      "source": [
        "df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTfVZMKagbTN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import openai\n",
        "\n",
        "# Assuming client is already defined as per:\n",
        "# client = openai.OpenAI(api_key=\"your_api_key_here\")\n",
        "\n",
        "# Updated function to get embeddings for a given text using the new API\n",
        "def get_embedding(text):\n",
        "    model = \"text-embedding-3-large\"  # Change model name if needed\n",
        "    response = client.embeddings.create(\n",
        "        input=[text],  # The API now takes a list of inputs\n",
        "        model=model\n",
        "    )\n",
        "    # The API response structure might vary, ensure to access the embedding correctly\n",
        "    embedding = response.data[0].embedding\n",
        "    return np.array(embedding)\n",
        "\n",
        "# Apply the function to each comment_text and store the result in a new 'embedding' column\n",
        "df_comment_indices['embedding'] = df_comment_indices['comment_text'].apply(get_embedding)\n",
        "\n",
        "# Display the DataFrame to verify the new 'embedding' column\n",
        "df_comment_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmCZ2N78i1mI"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3nPNtM3i42k"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df_comment_indices['embedding'] contains the embeddings\n",
        "\n",
        "# Normalize the embeddings to make Euclidean distance act like cosine similarity\n",
        "normalized_embeddings = normalize(np.stack(df_comment_indices['embedding'].values))\n",
        "\n",
        "# Perform k-means clustering\n",
        "# Choose the number of clusters (k) based on your data and requirements\n",
        "k = 1  # Example number of clusters\n",
        "kmeans = KMeans(n_clusters=k, random_state=42).fit(normalized_embeddings)\n",
        "\n",
        "# Assign the cluster labels to your DataFrame\n",
        "df_comment_indices['cluster'] = kmeans.labels_\n",
        "\n",
        "# Display the DataFrame with cluster assignments\n",
        "print(df_comment_indices.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOe6AWKRiOAZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Initialize index2 column\n",
        "df_comment_indices['index2'] = df_comment_indices['comment_index']\n",
        "\n",
        "# A reasonable similarity threshold for starting could be 0.8 or 0.9, indicating high similarity\n",
        "similarity_threshold = 0.75\n",
        "\n",
        "# Normalize the embeddings if not already normalized\n",
        "normalized_embeddings = normalize(np.stack(df_comment_indices['embedding'].values))\n",
        "\n",
        "# Iterate through each cluster\n",
        "for cluster in df_comment_indices['cluster'].unique():\n",
        "    cluster_indices = df_comment_indices[df_comment_indices['cluster'] == cluster].index.tolist()\n",
        "\n",
        "    # Iterate through each unique pair in the cluster (triangular matrix iteration)\n",
        "    for i in range(len(cluster_indices)):\n",
        "        for j in range(i + 1, len(cluster_indices)):\n",
        "            idx_i, idx_j = cluster_indices[i], cluster_indices[j]\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = cosine_similarity([normalized_embeddings[idx_i]], [normalized_embeddings[idx_j]])[0][0]\n",
        "\n",
        "            # If similarity exceeds threshold, set index2 of the higher index to the lower index2 value\n",
        "            if similarity > similarity_threshold:\n",
        "                higher_idx, lower_idx = (idx_j, idx_i) if idx_j > idx_i else (idx_i, idx_j)\n",
        "                df_comment_indices.at[higher_idx, 'index2'] = df_comment_indices.at[lower_idx, 'index2']\n",
        "\n",
        "# Display the DataFrame to verify the new 'index2' adjustments\n",
        "df_comment_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu2CmGiwpdsW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles and df_comment_indices are already defined\n",
        "\n",
        "# Step 0: Create a mapping from comment text to index2\n",
        "text_to_index2 = pd.Series(df_comment_indices['index2'].values, index=df_comment_indices['comment_text']).to_dict()\n",
        "\n",
        "# Step 1: Translate comment_texts to comment_indices using index2\n",
        "df_articles['comment_indices'] = df_articles['comment_texts'].apply(lambda texts: [text_to_index2[text] for text in texts])\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "all_indices = [index for sublist in df_articles['comment_indices'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in df_articles['comment_indices']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "df_articles['weighted_comment_sum'] = weighted_comment_sums\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df_articles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLUtF-2iVA_q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to the CSV files\n",
        "path_random = '/content/drive/MyDrive/randomarticles.csv'\n",
        "path_selected = '/content/drive/MyDrive/Selectedarticles.csv'\n",
        "\n",
        "# Reading the CSV files into DataFrames\n",
        "df_random = pd.read_csv(path_random)\n",
        "df_selected = pd.read_csv(path_selected)\n",
        "\n",
        "# Assign the 'random' value for each dataframe\n",
        "df_random['random'] = 1\n",
        "df_selected['random'] = 0\n",
        "\n",
        "# Combine the two dataframes\n",
        "df_combined = pd.concat([df_random, df_selected], ignore_index=True)\n",
        "\n",
        "# Assuming df_articles is your main DataFrame and it's already loaded\n",
        "# df_articles = pd.DataFrame(...) # Your main DataFrame loading logic here\n",
        "\n",
        "# Merge the combined dataframe with df_articles on the title column\n",
        "# This will add 'random', 'textlen', and 'cluster' columns to df_articles for matching titles\n",
        "df_articles = pd.merge(df_articles, df_combined[['title', 'textlen', 'cluster', 'random','fulltext']], on='title', how='left')\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df_articles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9KDV-CvfZ5M"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or_qnO1L0g-T"
      },
      "outputs": [],
      "source": [
        "def count_long_paragraphs(text):\n",
        "    paragraphs = text.split('\\n')\n",
        "    long_paragraphs = [p for p in paragraphs if len(p) > 100]\n",
        "    return len(long_paragraphs)\n",
        "\n",
        "# Apply the function to each row in the 'fulltext' column and record the result in a new 'paragraphs' column\n",
        "df_articles['paragraphs'] = df_articles['fulltext'].apply(count_long_paragraphs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFRvGgZoWVk6"
      },
      "outputs": [],
      "source": [
        "df_articles['commentrate']=df_articles.weighted_comment_sum/df_articles.textlen*1000\n",
        "df_articles['themerate']=df_articles.weighted_theme_sum/df_articles.textlen*10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JocbNe45ETd"
      },
      "outputs": [],
      "source": [
        "title_paragraph_map = pd.Series(aggregated_data.paragraphs.values, index=aggregated_data.title).to_dict()\n",
        "df_articles['paragraphs'] = df_articles['title'].map(title_paragraph_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WPqpcao9hwO"
      },
      "outputs": [],
      "source": [
        "df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn4xqUWHAHzq"
      },
      "outputs": [],
      "source": [
        "unique_titles = df_articles['title'].unique()\n",
        "\n",
        "# Now, filter sampled_df to retain only rows where the 'title' exists in df_articles\n",
        "test2=test[test['title'].isin(unique_titles)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDfEhhRyBEoq"
      },
      "outputs": [],
      "source": [
        "aggregated_df = subset_df.groupby('title').agg(\n",
        "    title_count=('title', 'count'),  # Count the number of occurrences of each title\n",
        ").reset_index()\n",
        "\n",
        "# Rename the index column back to 'title' if desired\n",
        "aggregated_df.rename(columns={'index': 'title'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlBxjhPlBMfZ"
      },
      "outputs": [],
      "source": [
        "aggregated_df2 = test2.groupby('title').agg(\n",
        "    title_count=('title', 'count'),  # Count the number of occurrences of each title\n",
        ").reset_index()\n",
        "\n",
        "# Rename the index column back to 'title' if desired\n",
        "aggregated_df2.rename(columns={'index': 'title'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oQe37LNBeAT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assume aggregated_df and aggregated_df2 are predefined Pandas DataFrames with the same structure, including 'title' and 'title_count' columns.\n",
        "\n",
        "# Merge the two DataFrames on the 'title' column to align them by title.\n",
        "# This merge uses an inner join by default, which means only titles existing in both DataFrames will be included in the result.\n",
        "merged_df = pd.merge(aggregated_df, aggregated_df2, on='title', suffixes=('_ag1', '_ag2'))\n",
        "\n",
        "# Calculate the fraction of title_count between the two DataFrames.\n",
        "# The result is a new column in the merged DataFrame.\n",
        "merged_df['fraction'] = merged_df['title_count_ag1'] / merged_df['title_count_ag2']\n",
        "\n",
        "# Now, merged_df contains the titles, the title counts from both original DataFrames, and the calculated fraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlW8-7k1CENj"
      },
      "outputs": [],
      "source": [
        "merged_df.to_csv('/content/drive/MyDrive/halfvsfull.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTQSWfOwABxd"
      },
      "outputs": [],
      "source": [
        "unique_titles = df_articles['title'].unique()\n",
        "\n",
        "# Now, filter sampled_df to retain only rows where the 'title' exists in df_articles\n",
        "subset_df = sampled_df[sampled_df['title'].isin(unique_titles)]\n",
        "subset_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDezXeMVFCBE"
      },
      "outputs": [],
      "source": [
        "rows_to_sample = min(1000, len(sampled_df))\n",
        "random_sampled_rows = sampled_df.sample(n=rows_to_sample, replace=False, random_state=42)  # replace=False ensures no duplication, random_state for reproducibility\n",
        "\n",
        "# Append these sampled rows to subset_df.\n",
        "# We use ignore_index=True to reindex the new dataframe, avoiding duplicate indices.\n",
        "subset_df = pd.concat([subset_df, random_sampled_rows], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnIDDIonFnM8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import openai\n",
        "\n",
        "# Assuming client is already defined as per:\n",
        "# client = openai.OpenAI(api_key=\"your_api_key_here\")\n",
        "\n",
        "# Updated function to get embeddings for a given text using the new API\n",
        "def get_embedding(text):\n",
        "    model = \"text-embedding-3-large\"  # Change model name if needed\n",
        "    response = client.embeddings.create(\n",
        "        input=[text],  # The API now takes a list of inputs\n",
        "        model=model\n",
        "    )\n",
        "    # The API response structure might vary, ensure to access the embedding correctly\n",
        "    embedding = response.data[0].embedding\n",
        "    return np.array(embedding)\n",
        "\n",
        "# Apply the function to each comment_text and store the result in a new 'embedding' column\n",
        "subset_df['embedding3'] = subset_df['codingd'].apply(get_embedding)\n",
        "\n",
        "# Display the DataFrame to verify the new 'embedding' column\n",
        "subset_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ym9HVBk66-r"
      },
      "outputs": [],
      "source": [
        "!pip install hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NFBS7PYFUOx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import hdbscan\n",
        "\n",
        "# Assuming subset_df is your DataFrame and it already exists\n",
        "\n",
        "# Define the condition to filter the rows that should not be clustered\n",
        "condition = subset_df['codingd'] != '{\"1. Theme\": None, \"2. Whose Attitude?\": None, \"3. Target\": None}'\n",
        "\n",
        "# Separate the DataFrame into two subsets based on the condition\n",
        "to_cluster_df = subset_df[condition]\n",
        "not_to_cluster_df = subset_df[~condition]\n",
        "\n",
        "# Initialize HDBSCAN with desired parameters\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=2, gen_min_span_tree=True)\n",
        "\n",
        "# Fit the model to the data that needs clustering (assuming 'embedding3' is suitable for clustering)\n",
        "cluster_labels = clusterer.fit_predict(list(to_cluster_df['embedding3']))\n",
        "\n",
        "# Assign the cluster indices to the subset that was clustered\n",
        "to_cluster_df['cluster3'] = cluster_labels\n",
        "\n",
        "# For rows that should not be clustered, assign -2 to the 'cluster3' column\n",
        "not_to_cluster_df['cluster3'] = -2\n",
        "\n",
        "# Concatenate the two subsets back into a single DataFrame\n",
        "subset_df = pd.concat([to_cluster_df, not_to_cluster_df], axis=0)\n",
        "\n",
        "# Optionally, you might want to sort the DataFrame based on the original index if order is important\n",
        "subset_df.sort_index(inplace=True)\n",
        "\n",
        "# Now, subset_df contains a 'cluster3' column with either cluster indices or -2 based on your conditions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk_DN0SQQM6T"
      },
      "outputs": [],
      "source": [
        "subset_df.to_csv('/content/drive/MyDrive/codeclusters.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q80RjIldfAu"
      },
      "outputs": [],
      "source": [
        "merged_df['clusts'] = merged_df['title'].apply(lambda title: subset_df[subset_df['title'] == title]['codingd'].tolist())\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles and df_comment_indices are already defined\n",
        "\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "all_indices = [index for sublist in merged_df['clusts'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in merged_df['clusts']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "merged_df['weighted_sum'] = weighted_comment_sums/merged_df.fraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FadfZpXIQbaK"
      },
      "outputs": [],
      "source": [
        "df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qNLEteWN7FX"
      },
      "outputs": [],
      "source": [
        "columns_to_merge = ['title', 'theme_indices', 'weighted_theme_sum', 'themerate']\n",
        "\n",
        "# Merge the dataframes on 'title', including only the specified columns from df_articles\n",
        "result_df = pd.merge(result_df, df_articles[columns_to_merge], on='title', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M68pvDC0M8Bq"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v96mv5c5VCIG"
      },
      "outputs": [],
      "source": [
        "merged_relevant = merged_df[['title', 'weighted_sum']]\n",
        "\n",
        "# Merge df_articles with the relevant portion of merged_df\n",
        "result_df = pd.merge(df_articles, merged_relevant, on='title', how='left')\n",
        "\n",
        "# Convert NaNs in 'weighted_sum' to None\n",
        "result_df['weighted_sum'] = result_df['weighted_sum']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5nLhvrWcLvE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(result_df['weighted_sum'], result_df['weighted_comment_sum'], alpha=0.6)\n",
        "plt.title('Scatter Plot')\n",
        "plt.xlabel('Round-2 density')\n",
        "plt.ylabel('Human coder density')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yxqBax0NlPQ"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'result_df' has 'textlen' and 'random' columns\n",
        "try:\n",
        "  result_df.reset_index(inplace=True)  # Resets the index and makes it a regular column\n",
        "except:\n",
        "  pass\n",
        "# Including the index and its quadratic term, as well as 'weighted_sum', as independent variables\n",
        "\n",
        "X = result_df['weighted_sum']\n",
        "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
        "\n",
        "y = result_df['weighted_comment_sum']  # Dependent variable\n",
        "\n",
        "model = sm.OLS(y, X).fit()  # OLS regression\n",
        "print(model.summary())  # Prints a summary of the regression results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtxpCCIedKto"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'result_df' has 'textlen' and 'random' columns\n",
        "try:\n",
        "  result_df.reset_index(inplace=True)  # Resets the index and makes it a regular column\n",
        "except:\n",
        "  pass\n",
        "# Including the index and its quadratic term, as well as 'weighted_sum', as independent variables\n",
        "result_df['randominteract']=result_df['random']*result_df['weighted_sum']\n",
        "X = result_df[['weighted_sum','random','randominteract']]\n",
        "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
        "\n",
        "y = result_df['weighted_comment_sum']  # Dependent variable\n",
        "\n",
        "model = sm.OLS(y, X).fit()  # OLS regression\n",
        "print(model.summary())  # Prints a summary of the regression results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__-dwYnEb-VP"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfHPoumHmsEu"
      },
      "outputs": [],
      "source": [
        "result_df['weighted_sum'].corr(result_df['weighted_comment_sum'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6on2Si7nBUe"
      },
      "outputs": [],
      "source": [
        "'weighted_comment_sum'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHMY8Vg5SnkS"
      },
      "outputs": [],
      "source": [
        "result_df['wsr']=result_df['weighted_sum']/result_df['textlen']\n",
        "result_df['wcsr']=result_df['weighted_comment_sum']/result_df['textlen']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXicMYO6ZsmS"
      },
      "outputs": [],
      "source": [
        "average_wcsr = result_df['wcsr'].mean()\n",
        "\n",
        "# Calculate the average of the top 50% of rows based on 'wcsr'\n",
        "top_50_percent_wcsr_avg = result_df.nlargest(int(len(result_df) * 0.5), 'wcsr')['wcsr'].mean()\n",
        "\n",
        "# Calculate the average of the top 20% of rows based on 'wcsr'\n",
        "top_20_percent_wcsr_avg = result_df.nlargest(int(len(result_df) * 0.05), 'wcsr')['wcsr'].mean()\n",
        "\n",
        "# Sorting by 'wsr'\n",
        "result_df_sorted_by_wsr = result_df.sort_values(by='wsr', ascending=False)\n",
        "\n",
        "# Calculate mean 'wcsr' for the top 50% and top 20% by 'wsr'\n",
        "top_50_percent_wsr_wcsr_avg = result_df_sorted_by_wsr.head(int(len(result_df_sorted_by_wsr) * 0.5))['wcsr'].mean()\n",
        "top_20_percent_wsr_wcsr_avg = result_df_sorted_by_wsr.head(int(len(result_df_sorted_by_wsr) * 0.05))['wcsr'].mean()\n",
        "\n",
        "average_wcsr, top_50_percent_wcsr_avg, top_20_percent_wcsr_avg, top_50_percent_wsr_wcsr_avg, top_20_percent_wsr_wcsr_avg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Hy-o1iVYi__"
      },
      "outputs": [],
      "source": [
        "result_df.to_csv('/content/drive/MyDrive/codes2vscoders.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01wIKEcm8LlV"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgqV15SqmPjj"
      },
      "outputs": [],
      "source": [
        "result_df=pd.read_csv('/content/drive/MyDrive/codes2vscoders.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF59EoyJmW8F"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XTpZ8QJumzQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles and df_comment_indices are already defined\n",
        "\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "df_articles['cluster'] = df_articles['cluster'].apply(lambda x: eval(x))\n",
        "\n",
        "all_indices = [index for sublist in df_articles['cluster'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in df_articles['cluster']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "df_articles['weighted_comment_sumAI'] = weighted_comment_sums\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edrU-PuwkF-p"
      },
      "outputs": [],
      "source": [
        "df_articles.to_csv('/content/drive/MyDrive/AIsampled.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8u8bgGp80ud"
      },
      "outputs": [],
      "source": [
        "df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZv23ynWvQrt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_articles is your DataFrame and it has a column named 'commentRate'\n",
        "\n",
        "# Calculate the mean and standard deviation of the commentRate column\n",
        "mean_commentRate = df_articles['commentrate'].mean()\n",
        "std_commentRate = df_articles['commentrate'].std()\n",
        "\n",
        "# Calculate the sample size\n",
        "n = df_articles['commentrate'].count()\n",
        "\n",
        "# Calculate the standard error of the mean\n",
        "sem = std_commentRate / np.sqrt(n)\n",
        "\n",
        "# Use scipy to calculate the margin of error for a 95% confidence interval\n",
        "z_score = stats.norm.ppf(0.975)  # for 95% CI, two-tailed\n",
        "margin_of_error = z_score * sem\n",
        "\n",
        "# Calculate the confidence interval\n",
        "ci_lower = mean_commentRate - margin_of_error\n",
        "ci_upper = mean_commentRate + margin_of_error\n",
        "\n",
        "print(f\"95% Confidence Interval for the comment rate: [{ci_lower}, {ci_upper}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhpZaXuL5w9a"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Ensure your DataFrame (df_articles) includes 'weighted_comment_sumAI', 'paragraphs_y', and 'textlen' columns.\n",
        "X = df_articles['graphlen']=df_articles['textlen']/df_articles['paragraphs_y']\n",
        "# Prepare the features (X) and the target variable (y)\n",
        "X = df_articles[['weighted_comment_sumAI', 'graphlen', 'textlen','random']]\n",
        "y = df_articles['weighted_comment_sum']\n",
        "\n",
        "# Add a constant to the model (intercept)\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Print the summary of the model to get coefficients and t-statistics\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6xhQQN5W7y9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter the DataFrame for random = 1 and random = 0\n",
        "df_random_1 = df_articles[df_articles['random'] == 1]\n",
        "df_random_0 = df_articles[df_articles['random'] == 0]\n",
        "\n",
        "# Create the histograms\n",
        "plt.hist(df_random_1['commentrate'], alpha=0.5, label='Random = 1', bins=20)\n",
        "plt.hist(df_random_0['commentrate'], alpha=0.5, label='Random = 0', bins=20)\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Comment Rate')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Overlaid Histogram of Comment Rate for Random = 1 and Random = 0')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dk2_nrYXTWs"
      },
      "outputs": [],
      "source": [
        "average_commentrate_random_1 = df_articles[df_articles['random'] == 1]['commentrate'].mean()\n",
        "average_commentrate_random_0 = df_articles[df_articles['random'] == 0]['commentrate'].mean()\n",
        "\n",
        "average_commentrate_random_1, average_commentrate_random_0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELybO_KsfSer"
      },
      "outputs": [],
      "source": [
        "merged_df=pd.read_csv('/content/drive/MyDrive/fullcodes.csv')\n",
        "\n",
        "grouped_df = merged_df.groupby('fulltext')['cluster3'].unique().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Rv3MxQEsCmc"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuHPX8MXf1j1"
      },
      "source": [
        "# Document Sampling 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNT7bPs9C_vF"
      },
      "outputs": [],
      "source": [
        "merged_df=pd.read_csv('/content/drive/MyDrive/fullcodes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NL8ZiHjebzG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Create a 'length' column based on the string length of the 'text' column\n",
        "merged_df['length'] = merged_df['text'].str.len()\n",
        "\n",
        "# Step 2: Drop rows where 'length' is less than or equal to 100\n",
        "filtered_df = merged_df[merged_df['length'] > 100]\n",
        "\n",
        "# Step 3: Group by 'fulltext' and count the number of paragraphs in each article\n",
        "paragraph_counts = filtered_df.groupby('fulltext').size()\n",
        "\n",
        "# Step 4: Plot a histogram of the paragraph counts\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.hist(paragraph_counts, bins=20, edgecolor='black')\n",
        "plt.title('Histogram of Paragraph Counts by Article')\n",
        "plt.xlabel('Number of Paragraphs')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrrJ8kQRgGGN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "grouped_df = merged_df.groupby('fulltext')['cluster3'].unique().reset_index()\n",
        "# Convert the aggregated sets to lists\n",
        "grouped_df['clusters'] = grouped_df['cluster3'].apply(list)\n",
        "# Drop the now unnecessary 'cluster' column\n",
        "grouped_df.drop(columns=['cluster3'], inplace=True)\n",
        "\n",
        "grouped_df  # Display the first few rows of the resulting dataframe\n",
        "grouped_df = merged_df.groupby('fulltext').agg({\n",
        "    'cluster3': lambda x: list(x.unique()),\n",
        "    'codingd': lambda x: list(x.unique()),\n",
        "    'title': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "# Add a new column 'textlen' to store the character length of the 'fulltext' strings\n",
        "grouped_df['textlen'] = grouped_df['fulltext'].apply(len)\n",
        "\n",
        "grouped_df  # Display the resulting dataframe\n",
        "grouped_df=grouped_df.loc[grouped_df.fulltext.str.len()>1200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_egPo_HC-6F"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL3a9Nvwf9l5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming grouped_df has a 'textlen' column\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.hist(grouped_df['textlen'], bins=20, edgecolor='black')\n",
        "plt.title('Histogram of Article Lengths (Characters)')\n",
        "plt.xlabel('Text Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwVC4uVWh1aM"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of unique 'codingd' values in grouped_df\n",
        "unique_codingd_count = merged_df['codingd'].nunique()\n",
        "\n",
        "unique_codingd_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQF2zT86gGQD"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JltiNdcXgGQE"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oCBFFKngGQE"
      },
      "outputs": [],
      "source": [
        "isinstance(grouped_df['cluster'].iloc[0], str) and grouped_df['cluster'].iloc[0].strip()[0] == '[' and grouped_df['cluster'].iloc[0].strip()[-1] == ']'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zPjuaxYgGQE"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values in the 'cluster' column\n",
        "nan_check = grouped_df['cluster'].isnull().any()\n",
        "\n",
        "# Identify the type of each element in the 'cluster' column\n",
        "types_in_cluster_column = grouped_df['cluster'].apply(type).unique()\n",
        "\n",
        "print(f\"NaN values present: {nan_check}\")\n",
        "print(f\"Types in the 'cluster' column: {types_in_cluster_column}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "836XikavgGQE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnggqH50gGQF"
      },
      "outputs": [],
      "source": [
        "!pip install apricot-select"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWYv-Dg1CiQe"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-bmWR_vgGQF"
      },
      "outputs": [],
      "source": [
        "from apricot import FeatureBasedSelection\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def convert_to_incidence_matrix(grouped_df, cluster_column):\n",
        "    # Identify all unique clusters across all articles\n",
        "    unique_clusters = set(cluster for clusters in grouped_df[cluster_column] for cluster in clusters)\n",
        "\n",
        "    # Create a binary incidence matrix\n",
        "    incidence_matrix = pd.DataFrame(0, index=grouped_df.index, columns=sorted(unique_clusters))\n",
        "\n",
        "    # Fill the incidence matrix with 1's where the article has the cluster\n",
        "    for index, row in grouped_df.iterrows():\n",
        "        incidence_matrix.loc[index, row[cluster_column]] = 1\n",
        "\n",
        "    return incidence_matrix\n",
        "\n",
        "incidence_matrix = convert_to_incidence_matrix(grouped_df, 'cluster3')\n",
        "\n",
        "# Assuming incidence_matrix and grouped_df are defined as before\n",
        "# Calculate the length of each article\n",
        "article_lengths = grouped_df['fulltext'].apply(len).values\n",
        "\n",
        "# Normalize the sample costs so that the mean is approximately 1\n",
        "normalized_costs = article_lengths / article_lengths.mean()\n",
        "\n",
        "# Initialize the FeatureBasedSelection object with the 'sqrt' concave function\n",
        "selector = FeatureBasedSelection(n_samples=20, concave_func='sqrt', verbose=True)\n",
        "\n",
        "# Fit the model on the binary incidence matrix to find the subset that maximizes the submodular function\n",
        "# using the normalized costs as a knapsack constraint\n",
        "selector.fit(incidence_matrix.values, sample_cost=normalized_costs)\n",
        "\n",
        "# Extract the subset using the transform method\n",
        "selected_features = selector.transform(incidence_matrix.values)\n",
        "\n",
        "# Get the indices of the selected articles\n",
        "selected_indices = selector.ranking\n",
        "\n",
        "# Use the indices to extract the corresponding rows from the original DataFrame\n",
        "selected_articles = grouped_df.iloc[selected_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AETFFRD_GTtx"
      },
      "outputs": [],
      "source": [
        "selected_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPN3QPp7NVaH"
      },
      "outputs": [],
      "source": [
        "final_df[final_df['title'].str.contains('card system', na=False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3p1wRwcLXTV"
      },
      "outputs": [],
      "source": [
        "random_articles_df = grouped_df.sample(n=18, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxkflZz8g_Xb"
      },
      "outputs": [],
      "source": [
        "sum(len(lst) for lst in selected_articles['cluster3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE0qV5YiS7ZP"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woKca1MmIZFo"
      },
      "outputs": [],
      "source": [
        "result_df[result_df['title'].str.contains('58')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9Frr9HaJ0gS"
      },
      "outputs": [],
      "source": [
        "def unique_count(values_list):\n",
        "    return len(set(values_list))\n",
        "\n",
        "# Apply this function to each row for the specified columns\n",
        "grouped_df['unique_cluster3'] = grouped_df['cluster3'].apply(unique_count)\n",
        "grouped_df['unique_codingd'] = grouped_df['codingd'].apply(unique_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-K31gC_JOdk"
      },
      "outputs": [],
      "source": [
        " grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQAZsov_XFeh"
      },
      "outputs": [],
      "source": [
        " grouped_df[grouped_df['title'].str.contains('58')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGPOJEBQMI0n"
      },
      "outputs": [],
      "source": [
        "titles_to_select = selected_articles['title'].unique()\n",
        "\n",
        "# Filter result_df to include only rows with a matching title\n",
        "result_df[result_df['title'].isin(titles_to_select)].codedensity.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk7_bi-ZsWNu"
      },
      "outputs": [],
      "source": [
        "titles_to_select2 = selected_articles['title'].unique()\n",
        "\n",
        "# Filter result_df to include only rows with a matching title\n",
        "result_df[result_df['title'].isin(titles_to_select2)].codedensity.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-2iUoE0sduJ"
      },
      "outputs": [],
      "source": [
        "titles_to_select3 = selected_articles['title'].unique()\n",
        "\n",
        "# Filter result_df to include only rows with a matching title\n",
        "result_df[result_df['title'].isin(titles_to_select3)].codedensity.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7l48F1hse76"
      },
      "outputs": [],
      "source": [
        "titles_to_select4 = selected_articles['title'].unique()\n",
        "\n",
        "# Filter result_df to include only rows with a matching title\n",
        "result_df[result_df['title'].isin(titles_to_select4)].codedensity.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvymafOVtsu2"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIQdKWD3LaHb"
      },
      "outputs": [],
      "source": [
        "check_df = result_df.merge(grouped_df[['fulltext', 'unique_cluster3', 'unique_codingd']], on='fulltext', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HON8EbkMDZOD"
      },
      "outputs": [],
      "source": [
        "check2=result_df[result_df.AIcodedensity>2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcsVT0XBs_68"
      },
      "outputs": [],
      "source": [
        "check2['AIcodedensity'].corr(check2['codedensity'])**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tXzGhcFuVs6"
      },
      "outputs": [],
      "source": [
        "check_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g9ZEO_3Lcmx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "selected_df = check_df[check_df['title'].isin(titles_to_select)]\n",
        "not_selected_df = check_df[~check_df['title'].isin(titles_to_select)]\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting data for titles in titles_to_select\n",
        "plt.scatter(selected_df['codedensity'], selected_df['AIcodedensity'], color='blue', label='Selected Titles')\n",
        "\n",
        "# Plotting data for titles not in titles_to_select\n",
        "plt.scatter(not_selected_df['codedensity'], not_selected_df['AIcodedensity'], color='orange', label='Other Titles')\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Comment Rate vs. Weighted Comment Sum AI')\n",
        "plt.xlabel('Comment Rate')\n",
        "plt.ylabel('Weighted Comment Sum AI')\n",
        "\n",
        "# Adding a legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vskrGZWJWNiD"
      },
      "outputs": [],
      "source": [
        "selected_df[selected_df.wsr>0.005]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMoplXcwQ0PY"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create 'fulllist' from 'random_articles_df' and add columns\n",
        "fulllist = random_articles_df.copy()\n",
        "fulllist['random2'] = 1\n",
        "fulllist['select2'] = 0\n",
        "\n",
        "fulllist['random'] = 0\n",
        "\n",
        "# Step 2: Append 'selected_articles' to 'fulllist' with specified values for new columns\n",
        "selected_articles_temp = selected_articles[['title', 'fulltext']].copy()\n",
        "selected_articles_temp['random2'] = 0\n",
        "selected_articles_temp['select2'] = 1\n",
        "\n",
        "selected_articles_temp['random'] = 0\n",
        "fulllist = pd.concat([fulllist, selected_articles_temp], ignore_index=True)\n",
        "\n",
        "# Step 3: Append 'result_df' to 'fulllist'\n",
        "result_df_temp = result_df[['title', 'fulltext', 'random']].copy()\n",
        "result_df_temp['random2'] = 0\n",
        "result_df_temp['select2'] = 0\n",
        "\n",
        "fulllist = pd.concat([fulllist, result_df_temp], ignore_index=True)\n",
        "\n",
        "# Print or use the 'fulllist' DataFrame\n",
        "fulllist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcWuSvSIU4my"
      },
      "outputs": [],
      "source": [
        "columns_to_drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWXkDcoRgwwS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqkBc2qJS7zX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Clfv_GG9SatY"
      },
      "outputs": [],
      "source": [
        "'Help Rohingya? Give the... in both select1 and select2\n",
        "Inhumane to turn back refugee ship in both select1 and select2\n",
        "Rohingyas urged to get tested\tin random1 and select1\n",
        "Syrian refugee Hassan... in random2 and select1\n",
        "Umno veep: Help refugees obtain papers... in both select1 and select2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUpxzbibP9AR"
      },
      "outputs": [],
      "source": [
        "fulllist.to_csv('/content/drive/MyDrive/AIcorpussets1and2full.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JktGe0gtMsJ2"
      },
      "outputs": [],
      "source": [
        "fulllist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkSQq4K-SGup"
      },
      "outputs": [],
      "source": [
        "unique_df = fulllist.drop_duplicates(subset='title', keep=False)\n",
        "\n",
        "# Step 2: Drop rows where both 'random2' and 'select2' are 0\n",
        "final_df = unique_df[(unique_df['random2'] != 0) | (unique_df['select2'] != 0)]\n",
        "\n",
        "final_df.to_csv('/content/drive/MyDrive/AIcorpus2.csv', index=False)\n",
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_05QUJiL34w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkWxn3u-K51A"
      },
      "outputs": [],
      "source": [
        "final_df.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyTbxsvoiws_"
      },
      "outputs": [],
      "source": [
        "final_df = final_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Open a new text file to write\n",
        "with open('output_document.txt', 'w', encoding='utf-8') as file:\n",
        "    # Iterate through each row of the DataFrame\n",
        "    for index, row in final_df.iterrows():\n",
        "        # Write hashes, title, and hashes to the file\n",
        "        file.write('#' * len(row['title']) + '\\n')  # Line of # characters above the title\n",
        "        file.write(row['title'] + '\\n')             # Write the title\n",
        "        file.write('#' * len(row['title']) + '\\n')  # Line of # characters below the title\n",
        "        file.write(row['fulltext'] + '\\n\\n')        # Write the fulltext with two line breaks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky6MhUJ3LGVR"
      },
      "outputs": [],
      "source": [
        "titles_to_select = result_df['title'].unique()\n",
        "\n",
        "# Filter result_df to include only rows with a matching title\n",
        "selected_articles[~selected_articles['title'].isin(titles_to_select)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iURadxOhlzD7"
      },
      "outputs": [],
      "source": [
        "selected_indices2 = selected_indices.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukFdkfO6lVcJ"
      },
      "outputs": [],
      "source": [
        " np.mean(np.in1d(selected_indices2, selected_indices)) * 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CrX8t7pgGQF"
      },
      "outputs": [],
      "source": [
        "# Assuming that 'selected_indices' is the array of indices for the selected articles\n",
        "# and 'grouped_df' is the original dataframe.\n",
        "\n",
        "# 1) Subset the grouped_df dataframe with the 20 selected articles\n",
        "selected_articles_df = grouped_df.iloc[selected_indices]\n",
        "\n",
        "# For comparison, select a random subset of 20 articles from the original dataframe\n",
        "random_articles_df = grouped_df.sample(n=30, random_state=1)\n",
        "\n",
        "# Function to count the occurrences of each cluster\n",
        "def count_clusters(df, cluster_column):\n",
        "    cluster_counts = df[cluster_column].explode().value_counts().sort_index()\n",
        "    return cluster_counts\n",
        "\n",
        "# Count the occurrences of each cluster in the selected articles\n",
        "selected_cluster_counts = count_clusters(selected_articles_df, 'cluster3')\n",
        "\n",
        "# Count the occurrences of each cluster in the random set of articles\n",
        "random_cluster_counts = count_clusters(random_articles_df, 'cluster3')\n",
        "\n",
        "# Now we'll plot the histograms using matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up the figure and axis\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 7), sharey=True)\n",
        "\n",
        "# Plot histogram for selected articles\n",
        "ax[0].bar(selected_cluster_counts.index, selected_cluster_counts.values)\n",
        "ax[0].set_title('Histogram of Clusters for Selected Articles')\n",
        "ax[0].set_xlabel('Cluster')\n",
        "ax[0].set_ylabel('Count')\n",
        "\n",
        "# Plot histogram for random articles\n",
        "ax[1].bar(random_cluster_counts.index, random_cluster_counts.values)\n",
        "ax[1].set_title('Histogram of Clusters for Random Articles')\n",
        "ax[1].set_xlabel('Cluster')\n",
        "# ax[1].set_ylabel('Count') # No need for ylabel as it shares with the first histogram\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzzrBdOTnzXl"
      },
      "outputs": [],
      "source": [
        "title_to_fulltext = merged_df.drop_duplicates(subset='title').set_index('title')['fulltext'].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uuNO9uln_OB"
      },
      "outputs": [],
      "source": [
        "selected_articles_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAi-o-Tho07R"
      },
      "outputs": [],
      "source": [
        "result_df.sort_values(by='commentrate')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgPSzs9BoOMO"
      },
      "outputs": [],
      "source": [
        "\n",
        "selected_articles.merge(result_df, on='title', how='inner')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnCQzHMUoS-f"
      },
      "outputs": [],
      "source": [
        "overlap_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiqhMH_5f1j2"
      },
      "outputs": [],
      "source": [
        "grouped_df.cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyvGFdnVf1j2"
      },
      "outputs": [],
      "source": [
        "isinstance(grouped_df['cluster'].iloc[0], str) and grouped_df['cluster'].iloc[0].strip()[0] == '[' and grouped_df['cluster'].iloc[0].strip()[-1] == ']'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyo5D0Nmf1j2"
      },
      "outputs": [],
      "source": [
        "\n",
        "grouped_df = test.groupby('fulltext')['cluster'].unique().reset_index()\n",
        "# Convert the aggregated sets to lists\n",
        "grouped_df['clusters'] = grouped_df['cluster'].apply(list)\n",
        "# Drop the now unnecessary 'cluster' column\n",
        "grouped_df.drop(columns=['cluster'], inplace=True)\n",
        "\n",
        "grouped_df  # Display the first few rows of the resulting dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp0s_5zyLw6D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76fKqf9sKOWc"
      },
      "source": [
        "# Empirical subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6KUnKAYKqBf"
      },
      "outputs": [],
      "source": [
        "!pip install apricot-select"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnmbUxlfPKzZ"
      },
      "outputs": [],
      "source": [
        " result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aDXjUx5GDAT"
      },
      "outputs": [],
      "source": [
        "grouped_dffull=grouped_df.copy()\n",
        "grouped_dfA = pd.merge(grouped_dffull, result_df[['fulltext', 'comment_indices']], on='fulltext', how='inner')\n",
        "grouped_dfS=grouped_dfA.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soWhmL4UGj6l"
      },
      "outputs": [],
      "source": [
        "grouped_dfS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws8z0a2x31MD"
      },
      "outputs": [],
      "source": [
        "grouped_dfS['merged']=grouped_dfS['codingd']+grouped_dfS['cluster3']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wupOIChyUSHI"
      },
      "outputs": [],
      "source": [
        "selected_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooC5lLz1LovM"
      },
      "outputs": [],
      "source": [
        "from apricot import FeatureBasedSelection\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming convert_to_incidence_matrix function is defined as before\n",
        "\n",
        "# Assuming incidence_matrix and grouped_dfS are defined\n",
        "# Convert to incidence matrix\n",
        "incidence_matrix = convert_to_incidence_matrix(grouped_dfS, 'merged')\n",
        "\n",
        "# Calculate article lengths\n",
        "article_lengths = grouped_dfS['fulltext'].apply(len).values\n",
        "\n",
        "# Normalize costs\n",
        "normalized_costs = article_lengths / article_lengths.mean()\n",
        "\n",
        "# Prepare to collect results\n",
        "results = []\n",
        "\n",
        "# Iterate over each value of n_samples from 1 to 29\n",
        "for n_samples in range(1, 30):\n",
        "    selector = FeatureBasedSelection(n_samples=n_samples, concave_func='sqrt', verbose=False)\n",
        "    selector.fit(incidence_matrix.values, sample_cost=normalized_costs)\n",
        "    selected_features = selector.transform(incidence_matrix.values)\n",
        "    selected_indices = selector.ranking\n",
        "    selected_articles = grouped_dfS.iloc[selected_indices].copy()\n",
        "\n",
        "    # Assuming the rest of processing to calculate weighted_comment_sums is correct\n",
        "    import ast\n",
        "    try:\n",
        "      selected_articles['comment_indices'] = selected_articles['comment_indices'].apply(ast.literal_eval)\n",
        "    except:\n",
        "      pass\n",
        "    all_indices = [index for sublist in selected_articles['comment_indices'] for index in sublist]\n",
        "    comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "    weighted_comment_sums = []\n",
        "    for indices in selected_articles['comment_indices']:\n",
        "        weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "        weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "    selected_articles['weighted_comment_sum'] = weighted_comment_sums\n",
        "    total_weighted_sum = selected_articles['weighted_comment_sum'].sum()\n",
        "    total_len = selected_articles['textlen'].sum()\n",
        "\n",
        "    # Collect results\n",
        "    results.append({'n_samples': n_samples, 'sum_weighted_comments': total_weighted_sum, 'textlen': total_len})\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df2 = pd.DataFrame(results)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results_df2['n_samples'], results_df2['sum_weighted_comments'], marker='o')\n",
        "plt.title('Sum of Weighted Comments vs. N Samples')\n",
        "plt.xlabel('N Samples')\n",
        "plt.ylabel('Sum of Weighted Comments')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "results_df2['density']=results_df2['sum_weighted_comments']/results_df2['textlen']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c_Hbj_u0_fK"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32rNSEXu0-qG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E-iJKJYYgN3"
      },
      "outputs": [],
      "source": [
        "plt.plot(results_df2['n_samples'], results_df2['sum_weighted_comments'], marker='o')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw5dlpL9VEFd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results_df2['n_samples'], results_df2['density'], marker='o')\n",
        "plt.title('Sum of Weighted Comments vs. N Samples')\n",
        "plt.xlabel('N Samples')\n",
        "plt.ylabel('Sum of Weighted Comments')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0XkvoIASveE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming grouped_dfS is your DataFrame containing articles with a 'textlen' column\n",
        "\n",
        "# Prepare to collect results\n",
        "results = []\n",
        "\n",
        "# Number of Monte Carlo simulations for each n_samples value\n",
        "n_simulations = 200\n",
        "\n",
        "# Iterate over each value of n_samples from 1 to 29\n",
        "for n_samples in range(1, 30):\n",
        "    density_results = []  # List to store density results for each simulation\n",
        "\n",
        "    # Perform n_simulations for each n_samples\n",
        "    for _ in range(n_simulations):\n",
        "        # Randomly select n_samples articles\n",
        "        selected_articles = grouped_dfS.sample(n=n_samples)\n",
        "\n",
        "        import ast\n",
        "        try:\n",
        "            selected_articles['comment_indices'] = selected_articles['comment_indices'].apply(ast.literal_eval)\n",
        "        except:\n",
        "            pass\n",
        "        all_indices = [index for sublist in selected_articles['comment_indices'] for index in sublist]\n",
        "        comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "        weighted_comment_sums = []\n",
        "        for indices in selected_articles['comment_indices']:\n",
        "            weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "            weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "        total_weighted_sum = sum(weighted_comment_sums)\n",
        "\n",
        "        # Calculate total text length for the selected articles\n",
        "        total_text_length = selected_articles['textlen'].sum()\n",
        "\n",
        "        # Calculate density for this simulation\n",
        "        if total_text_length > 0:  # Avoid division by zero\n",
        "            density = total_weighted_sum / total_text_length\n",
        "        else:\n",
        "            density = 0\n",
        "        density_results.append(density)\n",
        "\n",
        "    # Calculate 95% and 99% CI for the density results\n",
        "    ci_95_lower = np.percentile(density_results, 2.5)\n",
        "    ci_95_upper = np.percentile(density_results, 97.5)\n",
        "    ci_99_lower = np.percentile(density_results, 0.5)\n",
        "    ci_99_upper = np.percentile(density_results, 99.5)\n",
        "\n",
        "    # Collect results, including CI bounds\n",
        "    results.append({\n",
        "        'n_samples': n_samples,\n",
        "        'avg_density': np.mean(density_results),\n",
        "        '95_ci_lower': ci_95_lower,\n",
        "        '95_ci_upper': ci_95_upper,\n",
        "        '99_ci_lower': ci_99_lower,\n",
        "        '99_ci_upper': ci_99_upper\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Plotting both curves with 95% and 99% CI for Monte Carlo results\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Monte Carlo random selection curve\n",
        "plt.plot(results_df['n_samples'], results_df['avg_density'], marker='o', label='Random Selection (Monte Carlo)')\n",
        "# Filling between the 95% CI lower and upper bounds for Monte Carlo results\n",
        "plt.fill_between(results_df['n_samples'], results_df['95_ci_lower'], results_df['95_ci_upper'], alpha=0.2, label='95% CI (Monte Carlo)')\n",
        "# Filling between the 99% CI lower and upper bounds for Monte Carlo results, with a different color or lower alpha\n",
        "plt.fill_between(results_df['n_samples'], results_df['99_ci_lower'], results_df['99_ci_upper'], alpha=0.1, color='red', label='99% CI (Monte Carlo)')\n",
        "\n",
        "plt.title('Comparison of Density of Weighted Comments with 95% and 99% Confidence Intervals')\n",
        "plt.xlabel('N Samples')\n",
        "plt.ylabel('Density of Weighted Comments')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChCExCpQVxD2"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkZ7AAIETSuv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming results_df is the DataFrame from the Monte Carlo approach with 95% CI columns\n",
        "# and results_df2 is the DataFrame with optimized results.\n",
        "\n",
        "# Plotting both curves with 95% CI for Monte Carlo results\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Monte Carlo random selection curve with 95% CI\n",
        "plt.plot(results_df['n_samples'], results_df['avg_density'], marker='o', label='Random Selection')\n",
        "# Filling between the CI lower and upper bounds for Monte Carlo results\n",
        "plt.fill_between(results_df['n_samples'], results_df['95_ci_lower'], results_df['95_ci_upper'], alpha=0.2, label='95% CI')\n",
        "# Filling between the 99% CI lower and upper bounds for Monte Carlo results, with a different color or lower alpha\n",
        "plt.fill_between(results_df['n_samples'], results_df['99_ci_lower'], results_df['99_ci_upper'], alpha=0.1, color='red', label='99% CI')\n",
        "# Optimized selection curve\n",
        "plt.plot(results_df2['n_samples'], results_df2['density'], marker='x', linestyle='--', label='Optimized Selection')\n",
        "\n",
        "plt.title('Code density for subsets of 49 coded articles')\n",
        "plt.xlabel('Subset size (articles)')\n",
        "plt.ylabel('Density of Codes')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyowdpDVBH7x"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "furLi1JrBIgf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5h411_kqtmf"
      },
      "source": [
        "# Paper 2 Analysis part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0U-aT7nqtmg"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install python-docx pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqzyJd5mqtmg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def parse_docx_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "\n",
        "    articles = []\n",
        "\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        with docx.open('word/document.xml') as document_xml:\n",
        "            tree = ET.parse(document_xml)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            current_article = {'title': None, 'comment_count': 0}\n",
        "            for child in root.iter():\n",
        "                if child.tag == '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p':\n",
        "                    text = ''.join(node.text for node in child.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    if text.startswith(\"TITLE:\"):\n",
        "                        if current_article['title'] is not None:  # Not the first article\n",
        "                            articles.append(current_article)\n",
        "                        current_article = {'title': text.replace(\"TITLE:\", \"\").strip(), 'comment_count': 0}\n",
        "                    current_article['comment_count'] += len([c for c in child.iter() if c.tag.endswith('commentRangeStart')])\n",
        "\n",
        "            # Add the last article if it exists\n",
        "            if current_article['title'] is not None:\n",
        "                articles.append(current_article)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Example usage\n",
        "\n",
        "file_path = '/content/drive/MyDrive/ROUND 2 - SHARON.docx'\n",
        "comments_count_per_article = parse_docx_comments(file_path)\n",
        "articles_data = parse_docx_comments(file_path)\n",
        "\n",
        "# Convert the list of dictionaries to a pandas DataFrame\n",
        "df_articles = pd.DataFrame(articles_data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_articles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kC0RlW9qtmg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnIUHdCNq-xF"
      },
      "source": [
        "# add thematic counts 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgiWHclG58Rb"
      },
      "outputs": [],
      "source": [
        "###round 1\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = {}\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        try:\n",
        "            with docx.open('word/comments.xml') as comments_xml:\n",
        "                tree = ET.parse(comments_xml)\n",
        "                root = tree.getroot()\n",
        "                for comment in root.findall('.//w:comment', ns):\n",
        "                    comment_id = comment.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                    comment_text = ''.join(node.text for node in comment.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    comments[comment_id] = comment_text\n",
        "        except KeyError:  # Handle case where comments.xml does not exist (no comments)\n",
        "            pass\n",
        "    return comments\n",
        "\n",
        "def parse_docx_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = parse_comments(file_path)\n",
        "    articles = []\n",
        "\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        with docx.open('word/document.xml') as document_xml:\n",
        "            tree = ET.parse(document_xml)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            current_article = {'title': None, 'comment_count': 0, 'comment_texts': []}\n",
        "            for child in root.iter():\n",
        "                if child.tag == '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p':\n",
        "                    text = ''.join(node.text for node in child.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    if text.startswith(\"TITLE:\"):\n",
        "                        if current_article['title'] is not None:  # Not the first article\n",
        "                            articles.append(current_article)\n",
        "                        current_article = {'title': text.replace(\"TITLE:\", \"\").strip(), 'comment_count': 0, 'comment_texts': []}\n",
        "                    for c in child.iter():\n",
        "                        if c.tag.endswith('commentRangeStart'):\n",
        "                            comment_id = c.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                            if comment_id in comments:\n",
        "                                current_article['comment_texts'].append(comments[comment_id])\n",
        "                                current_article['comment_count'] += 1\n",
        "\n",
        "            # Add the last article if it exists\n",
        "            if current_article['title'] is not None:\n",
        "                articles.append(current_article)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# List of docx files to process\n",
        "docx_files = ['ROUND 2 - ZAHRA.docx', 'ROUND 2 - SHARON.docx', 'ROUND 2 - AIMI.docx']\n",
        "base_path = '/content/drive/MyDrive/'\n",
        "\n",
        "# Dictionary to aggregate articles data\n",
        "articles_agg = defaultdict(lambda: {'comment_count': 0, 'comment_texts': []})\n",
        "\n",
        "for file_name in docx_files:\n",
        "    file_path = base_path + file_name\n",
        "    articles_data = parse_docx_comments(file_path)\n",
        "    for article in articles_data:\n",
        "        articles_agg[article['title']]['comment_count'] += article['comment_count']\n",
        "        articles_agg[article['title']]['comment_texts'] += article['comment_texts']\n",
        "\n",
        "# Convert the aggregated data into a list of dictionaries, then into a DataFrame\n",
        "articles_list = [{'title': title, 'comment_count': data['comment_count'], 'comment_texts': data['comment_texts'], 'Round': 1} for title, data in articles_agg.items()]\n",
        "df_articles = pd.DataFrame(articles_list)\n",
        "\n",
        "# Display the DataFrame\n",
        "#df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7spXSioJ6aof"
      },
      "outputs": [],
      "source": [
        "df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHIAGONaq-xF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = {}\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        try:\n",
        "            with docx.open('word/comments.xml') as comments_xml:\n",
        "                tree = ET.parse(comments_xml)\n",
        "                root = tree.getroot()\n",
        "                for comment in root.findall('.//w:comment', ns):\n",
        "                    comment_id = comment.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                    comment_text = ''.join(node.text for node in comment.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    comments[comment_id] = comment_text\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return comments\n",
        "\n",
        "def parse_docx_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = parse_comments(file_path)\n",
        "    articles = []\n",
        "    current_article = None\n",
        "    current_text = []\n",
        "    blank_line_found = False\n",
        "    title_next = False\n",
        "\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        with docx.open('word/document.xml') as document_xml:\n",
        "            tree = ET.parse(document_xml)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            for child in root.iter():\n",
        "                if child.tag == '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p':\n",
        "                    text = ''.join(node.text for node in child.iter() if node.tag.endswith('}t') and node.text)\n",
        "\n",
        "                    if text.strip() == \"\":\n",
        "                        blank_line_found = True\n",
        "                    elif text.startswith(\"###\") and text.endswith(\"###\") and blank_line_found:\n",
        "                        if current_article is not None:\n",
        "                            current_article['content'] = ' '.join(current_text).strip()\n",
        "                            articles.append(current_article)\n",
        "                        current_article = {'title': None, 'content': '', 'comments': []}\n",
        "                        current_text = []\n",
        "                        title_next = True\n",
        "                        blank_line_found = False\n",
        "                    elif title_next:\n",
        "                        current_article['title'] = text.strip()\n",
        "                        title_next = False\n",
        "                    else:\n",
        "                        if text.startswith(\"###\") and text.endswith(\"###\"):\n",
        "                            text = text.strip(\"#\").strip()\n",
        "                        current_text.append(text)\n",
        "                        for c in child.iter():\n",
        "                            if c.tag.endswith('commentRangeStart'):\n",
        "                                comment_id = c.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                                if comment_id in comments:\n",
        "                                    current_article['comments'].append(comments[comment_id])\n",
        "\n",
        "            if current_article is not None:\n",
        "                current_article['content'] = ' '.join(current_text).strip()\n",
        "                articles.append(current_article)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Assuming df_articles is already an existing DataFrame\n",
        "# df_articles = pd.DataFrame(...)  # Your existing DataFrame initialization\n",
        "\n",
        "# List of docx files to process\n",
        "docx_files = ['Second Round Articles - Zahra.docx', 'Second Round Articles - Sharon.docx', 'Second Round Articles - Aimi.docx']\n",
        "base_path = '/content/drive/MyDrive/'\n",
        "\n",
        "# Dictionary to aggregate articles data\n",
        "articles_agg = defaultdict(lambda: {'comment_count': 0, 'comment_texts': []})\n",
        "\n",
        "for file_name in docx_files:\n",
        "    file_path = base_path + file_name\n",
        "    articles_data = parse_docx_comments(file_path)\n",
        "    for article in articles_data:\n",
        "        articles_agg[article['title']]['comment_count'] += len(article['comments'])\n",
        "        articles_agg[article['title']]['comment_texts'] += article['comments']\n",
        "\n",
        "# Convert the aggregated data into a list of dictionaries, then append to existing DataFrame\n",
        "articles_list = [{'title': title, 'comment_count': data['comment_count'], 'comment_texts': data['comment_texts'], 'Round': 2} for title, data in articles_agg.items()]\n",
        "df_new_articles = pd.DataFrame(articles_list)\n",
        "df_articles = pd.concat([df_articles, df_new_articles], ignore_index=True)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df_articles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vf5yrb69JwA"
      },
      "outputs": [],
      "source": [
        "df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmMh_M_w6Y4_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# Step 1: Create an index mapping for comments\n",
        "comment_index_mapping = {}\n",
        "index_counter = 0\n",
        "for comment_texts in df_articles['comment_texts']:\n",
        "    for text in comment_texts:\n",
        "        if text not in comment_index_mapping:\n",
        "            comment_index_mapping[text] = index_counter\n",
        "            index_counter += 1\n",
        "themes1=pd.read_csv('/content/drive/MyDrive/finalthemes.csv')\n",
        "themes1['THEMES'] = themes1['THEMES'].fillna(method='ffill')\n",
        "theme_changes = themes1['THEMES'] != themes1['THEMES'].shift(1)\n",
        "def clean_string(s):\n",
        "    # Remove all non-alphanumeric characters (except spaces)\n",
        "    cleaned = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    # Convert to lowercase\n",
        "    cleaned = cleaned.lower()\n",
        "    # Normalize spaces by removing extra spaces\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "    return cleaned\n",
        "# Convert the boolean series to integers (0 or 1) and compute the cumulative sum to create a theme index\n",
        "themes1['themeindex'] = theme_changes.cumsum()\n",
        "cleaned_comment_index_mapping = {clean_string(key): value for key, value in comment_index_mapping.items()}\n",
        "cleaned_comment_index_mapping ['former prime minister thinks it is unfair for malaysia to be on the receiving end in the conflict between refugees and their home country'] = cleaned_comment_index_mapping .pop('prime minister thinks it is unfair for malaysia to be on the receiving end in the conflict between refugees and their home country')\n",
        "\n",
        "# Preprocess the 'CODES' column in the themes DataFrame (assuming 'CODES' contains the thematic descriptions to match)\n",
        "# If 'CODES' is actually meant to be matched with another column that contains descriptions, replace 'CODES' with the correct column name\n",
        "themes1['processed_CODES'] = themes1['CODES'].apply(clean_string)\n",
        "\n",
        "# Map the cleaned 'CODES' column to the indices using the cleaned dictionary\n",
        "themes1['code_index'] = themes1['processed_CODES'].map(cleaned_comment_index_mapping)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZjIWDy06zxy"
      },
      "outputs": [],
      "source": [
        "themes1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQhqeG73K8BG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file with the correct encoding\n",
        "try:\n",
        "    themes = pd.read_csv('/content/drive/MyDrive/codelist.csv', encoding='ISO-8859-1')  # Specify the correct encoding\n",
        "except UnicodeDecodeError as e:\n",
        "    print(f\"Error reading the CSV file: {e}\")\n",
        "    # Handle the error or re-raise it\n",
        "    raise\n",
        "\n",
        "# Drop rows with all blank values\n",
        "themes.dropna(how='all', inplace=True)\n",
        "\n",
        "# Fill missing 'Themes' values with the previous non-missing value\n",
        "themes['THEMES'] = themes['Themes'].fillna(method='ffill')\n",
        "\n",
        "# Identify theme changes\n",
        "theme_changes = themes['THEMES'] != themes['THEMES'].shift(1)\n",
        "\n",
        "# Clean a string by removing non-alphanumeric characters, converting to lowercase, and normalizing spaces\n",
        "def clean_string(s):\n",
        "    cleaned = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    cleaned = cleaned.lower()\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "    return cleaned\n",
        "\n",
        "# Compute the cumulative sum of theme changes to create a theme index\n",
        "themes['themeindex'] = theme_changes.cumsum()\n",
        "\n",
        "# Report types of values in the 'Codes' column to diagnose issues\n",
        "print(\"Diagnosing 'Codes' column values:\")\n",
        "for index, value in themes['Codes'].items():\n",
        "    if not isinstance(value, str):\n",
        "        print(f\"Non-string value found at index {index}: {value} (type: {type(value)})\")\n",
        "\n",
        "# Convert non-string values in 'Codes' to strings\n",
        "themes['Codes'] = themes['Codes'].astype(str)\n",
        "\n",
        "# Preprocess the 'Codes' column in the themes DataFrame\n",
        "themes['processed_CODES'] = themes['Codes'].apply(clean_string)\n",
        "\n",
        "# Create a simple index for each unique processed_CODE\n",
        "processed_code_index_mapping = {code: idx for idx, code in enumerate(themes['processed_CODES'].unique())}\n",
        "\n",
        "# Map the processed_CODES column to the indices using the new dictionary\n",
        "themes['code_index'] = themes['processed_CODES'].map(processed_code_index_mapping)\n",
        "\n",
        "# Additional reporting to verify the results\n",
        "print(\"Processed Codes and Indices:\")\n",
        "print(themes[['Codes', 'processed_CODES', 'code_index']].head())\n",
        "\n",
        "# Generate the crosswalk comment_index_mapping\n",
        "comment_index_mapping = {}\n",
        "for idx, row in themes.iterrows():\n",
        "    processed_code = row['processed_CODES']\n",
        "    code_index = row['code_index']\n",
        "    comment_index_mapping[processed_code] = code_index\n",
        "\n",
        "# Output the final comment_index_mapping\n",
        "print(\"Final comment_index_mapping:\")\n",
        "print(comment_index_mapping)\n",
        "# Preprocess the 'Codes' column in the themes DataFrame\n",
        "themes['processed_CODES'] = themes['Codes'].apply(clean_string)\n",
        "\n",
        "# Create a simple index for each unique processed_CODE\n",
        "processed_code_index_mapping = {code: idx for idx, code in enumerate(themes['processed_CODES'].unique())}\n",
        "\n",
        "# Map the processed_CODES column to the indices using the new dictionary\n",
        "themes['code_index'] = themes['processed_CODES'].map(processed_code_index_mapping)\n",
        "\n",
        "# Additional reporting to verify the results\n",
        "print(\"Processed Codes and Indices:\")\n",
        "print(themes[['Codes', 'processed_CODES', 'code_index']].head())\n",
        "\n",
        "# Generate the crosswalk comment_index_mapping\n",
        "comment_index_mapping = {}\n",
        "for idx, row in themes.iterrows():\n",
        "    processed_code = row['processed_CODES']\n",
        "    code_index = row['code_index']\n",
        "    comment_index_mapping[processed_code] = code_index\n",
        "\n",
        "# Output the final comment_index_mapping\n",
        "print(\"Final comment_index_mapping:\")\n",
        "print(comment_index_mapping)\n",
        "###revised of above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADL1E1dd0iqP"
      },
      "outputs": [],
      "source": [
        "comment_index_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibFkDcGyq-xG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles is your DataFrame from the previous steps\n",
        "# and it includes a 'comment_texts' column with lists of comment texts for each article\n",
        "\n",
        "df_articles['cleaned_comment_texts'] = df_articles['comment_texts'].apply(lambda texts: [clean_string(text) for text in texts])\n",
        "\n",
        "df_articles['comment_indices'] = df_articles['cleaned_comment_texts'].apply(lambda texts: [comment_index_mapping.get(text, -1) for text in texts])\n",
        "\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "all_indices = [index for sublist in df_articles['comment_indices'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in df_articles['comment_indices']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "df_articles['weighted_comment_sum'] = weighted_comment_sums\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AekE4NJq-xG"
      },
      "outputs": [],
      "source": [
        "code_to_theme_mapping = themes.assign(code_index=themes['code_index'].astype(float).astype(int)).set_index('code_index')['themeindex'].to_dict()\n",
        "\n",
        "\n",
        "# Step 2: Map comment_indices to theme_indices using the created mapping\n",
        "df_articles['theme_indices'] = df_articles['comment_indices'].apply(lambda indices: [code_to_theme_mapping.get(index) for index in indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eoKSYYtq-xG"
      },
      "outputs": [],
      "source": [
        "all_theme_indices = [index for sublist in df_articles['theme_indices'] for index in sublist if index is not None]  # Ensure None values are excluded\n",
        "theme_counts = pd.Series(all_theme_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3 for themes: Sum the number of comments for each article, weighted by 1/n, using theme indices\n",
        "weighted_theme_sums = []\n",
        "for theme_indices in df_articles['theme_indices']:\n",
        "    # Exclude None values to ensure only valid mappings are considered for the weighted sum\n",
        "    valid_theme_indices = [index for index in theme_indices if index is not None]\n",
        "    weighted_sum = sum(1 / theme_counts[index] for index in valid_theme_indices)\n",
        "    weighted_theme_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted theme sums to the DataFrame\n",
        "df_articles['weighted_theme_sum'] = weighted_theme_sums\n",
        "df_articles['theme_indices'] = df_articles['theme_indices'].apply(lambda indices: [0 if index is None else index for index in indices])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWG9IHvxq-xG"
      },
      "outputs": [],
      "source": [
        "merged_df=pd.read_csv('/content/drive/MyDrive/fullcodes.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3zV-D17q-xH"
      },
      "outputs": [],
      "source": [
        "!pip install pacmap\n",
        "!pip install hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82AmPh_Qq-xH"
      },
      "outputs": [],
      "source": [
        "merged_df['embeddings3'] = merged_df['embeddings3'].apply(eval)\n",
        "\n",
        "# Convert these lists to a numpy array for processing with pacmap\n",
        "embeddings_array = np.array(merged_df['embeddings3'].tolist())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVGCbE_5q-xH"
      },
      "outputs": [],
      "source": [
        "# Step 3: Apply pacmap to obtain a 2D embedding\n",
        "# Initialize PaCMAP instance and fit-transform the data to get 2D embeddings\n",
        "import pacmap\n",
        "import hdbscan\n",
        "\n",
        "pacmap_instance = pacmap.PaCMAP(n_components=6)\n",
        "embeddings_2d = pacmap_instance.fit_transform(embeddings_array)\n",
        "\n",
        "# Step 4: Apply hdbscan on the pacmap embeddings to perform clustering\n",
        "# Initialize HDBSCAN, fit it on the 2D embeddings, and obtain cluster labels\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=5,min_samples=1, gen_min_span_tree=True)\n",
        "cluster_labels = clusterer.fit_predict(embeddings_2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaNPIM31q-xH"
      },
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBK4THX4q-xH"
      },
      "outputs": [],
      "source": [
        "merged_df['cluster3'] = cluster_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1aWlHnQq-xH"
      },
      "outputs": [],
      "source": [
        "grouped_df = merged_df.groupby('fulltext').agg({\n",
        "   # 'cluster4': lambda x: list(x.unique()),  # Get unique values for 'cluster3'\n",
        "    'cluster3': lambda x: list(x.unique()),  # Get unique values for 'cluster3'\n",
        "    'codingd': lambda x: list(x.unique()),   # Get unique values for 'codingd'\n",
        "    'title': 'first'                         # Keep the first title for each group\n",
        "}).reset_index()\n",
        "\n",
        "# Add a new column 'textlen' to store the character length of the 'fulltext' strings\n",
        "grouped_df['textlen'] = grouped_df['fulltext'].apply(len)\n",
        "\n",
        "# Filter rows where the length of 'fulltext' is greater than 1200 characters\n",
        "grouped_df = grouped_df.loc[grouped_df['textlen'] > 1200]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIjllMkfq-xH"
      },
      "outputs": [],
      "source": [
        "all_theme_indices = [index for sublist in df_articles['theme_indices'] for index in sublist if index is not None]  # Ensure None values are excluded\n",
        "theme_counts = pd.Series(all_theme_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3 for themes: Sum the number of comments for each article, weighted by 1/n, using theme indices\n",
        "weighted_theme_sums = []\n",
        "for theme_indices in df_articles['theme_indices']:\n",
        "    # Exclude None values to ensure only valid mappings are considered for the weighted sum\n",
        "    valid_theme_indices = [index for index in theme_indices if index is not None]\n",
        "    weighted_sum = sum(1 / theme_counts[index] for index in valid_theme_indices)\n",
        "    weighted_theme_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted theme sums to the DataFrame\n",
        "df_articles['weighted_theme_sum'] = weighted_theme_sums\n",
        "df_articles['theme_indices'] = df_articles['theme_indices'].apply(lambda indices: [0 if index is None else index for index in indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJdC8aEBxtYJ"
      },
      "outputs": [],
      "source": [
        "corpus=pd.read_csv('/content/drive/MyDrive/AIcorpus2.csv')\n",
        "\n",
        "# Merge the 'select2' column from corpus to df_articles based on 'title'\n",
        "df_articles = df_articles.merge(corpus[['title', 'select2']], on='title', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NctbSCJ8q-xH"
      },
      "outputs": [],
      "source": [
        "result_df = pd.merge(df_articles, grouped_df[['title', 'fulltext', 'cluster3', 'codingd','textlen']], on='title', how='left')\n",
        "result_df = result_df.sort_values(by=['title', 'textlen'], ascending=[True, False])\n",
        "result_df = result_df.drop_duplicates(subset='title', keep='first')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_7wEve6hMot"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qFVm5kzq-xI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles is your DataFrame from the previous steps\n",
        "# and it includes a 'comment_texts' column with lists of comment texts for each article\n",
        "\n",
        "# Step 1: Create an index mapping for comments\n",
        "comment_index_mapping = {}\n",
        "index_counter = 0\n",
        "for comment_texts in result_df['codingd']:\n",
        "    for text in comment_texts:\n",
        "        if text not in comment_index_mapping:\n",
        "            comment_index_mapping[text] = index_counter\n",
        "            index_counter += 1\n",
        "\n",
        "result_df['AIcode_indices'] = result_df['codingd'].apply(lambda texts: [comment_index_mapping[text] for text in texts])\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "all_indices = [index for sublist in result_df['AIcode_indices'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in result_df['AIcode_indices']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "result_df['AIcodesum'] = weighted_comment_sums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmxs9FQXq-xI"
      },
      "outputs": [],
      "source": [
        "\n",
        "all_indices = [index for sublist in result_df['cluster3'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in result_df['cluster3']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "result_df['ThemesumAI'] = weighted_comment_sums\n",
        "result_df['AIcodedensity']=result_df['AIcodesum']/result_df['textlen']*1000\n",
        "result_df['AIthemedensity']=result_df['ThemesumAI']/result_df['textlen']*1000\n",
        "result_df['codedensity']=result_df['weighted_comment_sum']/result_df['textlen']*1000\n",
        "result_df['themedensity']=result_df['weighted_theme_sum']/result_df['textlen']*10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLsg1HAfq-xI"
      },
      "outputs": [],
      "source": [
        "result_df['themecodes']=(result_df['weighted_comment_sum']*result_df['weighted_theme_sum'])**0.5\n",
        "result_df['AIthemecodes']=(result_df['AIcodesum']*result_df['ThemesumAI'])**0.33"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIUIG7DDq-xI"
      },
      "outputs": [],
      "source": [
        "result_df[['AIcodesum', 'ThemesumAI', 'weighted_theme_sum', 'weighted_comment_sum','AIthemecodes','themecodes']].corr()**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iIsRBKknRv1"
      },
      "outputs": [],
      "source": [
        "result_df.to_csv('/content/drive/MyDrive/AIcorpus2results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odR6JGKrss88"
      },
      "outputs": [],
      "source": [
        "result_df=pd.read_csv('/content/drive/MyDrive/AIcorpus2results.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5infiMvHq-xI"
      },
      "outputs": [],
      "source": [
        "\tAIcodesum\tThemesumAI\tweighted_theme_sum\tweighted_comment_sum\tAIthemecodes\tthemecodes\n",
        "AIcodesum\t1.000000\t0.713444\t0.282207\t0.705462\t0.916821\t0.465143\n",
        "ThemesumAI\t0.713444\t1.000000\t0.122995\t0.409267\t0.768128\t0.222021\n",
        "weighted_theme_sum\t0.282207\t0.122995\t1.000000\t0.622006\t0.390969\t0.932037\n",
        "weighted_comment_sum\t0.705462\t0.409267\t0.622006\t1.000000\t0.835850\t0.838923\n",
        "AIthemecodes\t0.916821\t0.768128\t0.390969\t0.835850\t1.000000\t0.589395\n",
        "themecodes\t0.465143\t0.222021\t0.932037\t0.838923\t0.589395\t1.000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WquNXeBLiXP0"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zehYclCQq-xI"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'result_df' has 'textlen' and 'random' columns\n",
        "\n",
        "# Including the index and its quadratic term, as well as 'weighted_sum', as independent variables\n",
        "\n",
        "X = result_df[['select2',\"index\"]]\n",
        "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
        "\n",
        "y = result_df['codedensity']  # Dependent variable\n",
        "\n",
        "model = sm.OLS(y, X).fit()  # OLS regression\n",
        "print(model.summary())  # Prints a summary of the regression results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwNUzfwCy3fa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming result_df is your DataFrame\n",
        "# Example: import pandas as pd\n",
        "# result_df = pd.DataFrame({'AIcodedensity': [1, 2, 3], 'codedensity': [4, 5, 6]})\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(result_df['AIcodedensity'], result_df['codedensity'], alpha=0.7, edgecolors='w', s=100)\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Scatter Plot of AIcodedensity vs Codedensity')\n",
        "plt.xlabel('AIcodedensity')\n",
        "plt.ylabel('Codedensity')\n",
        "\n",
        "# Show grid\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab5JP9l1q-xJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles and df_comment_indices are already defined\n",
        "\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "df_articles['cluster'] = df_articles['cluster'].apply(lambda x: eval(x))\n",
        "\n",
        "all_indices = [index for sublist in df_articles['cluster'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in df_articles['cluster']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "df_articles['weighted_comment_sumAI'] = weighted_comment_sums\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWiHMOa4LqUx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdalwFR2dBCz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzQ0nX4cc6Pr"
      },
      "outputs": [],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqnZTYnD8Htd"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0e55WY5LsZ9"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eOk7CB98aIR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure the necessary columns are present in df_articles\n",
        "required_columns = ['AIcodedensity', 'select2']\n",
        "for column in required_columns:\n",
        "    if column not in result_df.columns:\n",
        "        raise ValueError(f\"The '{column}' column is not present in df_articles\")\n",
        "\n",
        "# Scatter plot of unique_comment_rate by select2\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(result_df['select2'], result_df['AIcodedensity'], alpha=0.5)\n",
        "plt.title('Scatter Plot of Unique Comment Rate by Select2')\n",
        "plt.xlabel('Select2')\n",
        "plt.ylabel('Unique Comment Rate')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUjTmpQB85EU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Calculate the mean and 95% CI for result_df['comment_per_1000_chars'] where result_df['select2'] = 1\n",
        "mean_1 = result_df[result_df['select2'] == 1]['comment_per_1000_chars'].mean()\n",
        "ci_1 = stats.t.interval(0.95, len(result_df[result_df['select2'] == 1]) - 1, loc=mean_1, scale=stats.sem(result_df[result_df['select2'] == 1]['comment_per_1000_chars']))\n",
        "\n",
        "# Calculate the mean and 95% CI for result_df['comment_per_1000_chars'] where result_df['select2'] = 0\n",
        "mean_0 = result_df[result_df['select2'] == 0]['comment_per_1000_chars'].mean()\n",
        "ci_0 = stats.t.interval(0.95, len(result_df[result_df['select2'] == 0]) - 1, loc=mean_0, scale=stats.sem(result_df[result_df['select2'] == 0]['comment_per_1000_chars']))\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean (select2 = 1): {mean_1:.4f}, 95% CI: {ci_1}\")\n",
        "print(f\"Mean (select2 = 0): {mean_0:.4f}, 95% CI: {ci_0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL_4xhlGtcC-"
      },
      "outputs": [],
      "source": [
        "fulllist=pd.read_csv('/content/drive/MyDrive/AIcorpussets1and2full.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcj0SSgAwgh3"
      },
      "outputs": [],
      "source": [
        "results1=pd.read_csv('/content/drive/MyDrive/codes2vscoders.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9LbLSHhwsJD"
      },
      "outputs": [],
      "source": [
        "results1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3L1jbTdtiQO"
      },
      "outputs": [],
      "source": [
        "fulllist['results1'] = fulllist['title'].isin(results1['title']).astype(int)\n",
        "\n",
        "# Add a column 'result_df' with value 1 if there's a match on the 'title' column between fulllist and result_df\n",
        "fulllist['result_df'] = fulllist['title'].isin(result_df['title']).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIiA-LJBzxEY"
      },
      "outputs": [],
      "source": [
        "fulllist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9PC2-SczATE"
      },
      "outputs": [],
      "source": [
        "fulllist.loc[:,['title','results1','result_df','select2','random2','random']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oYZjisszfA1"
      },
      "outputs": [],
      "source": [
        "subset =fulllist[(fulllist['result_df'] == 0) & (fulllist['select2'] == 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EMOCOQbxSEt"
      },
      "outputs": [],
      "source": [
        "result_df[result_df['select2']==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yZhV4fo1FMH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming results1 is your DataFrame\n",
        "results1['AIcodedensity'] = (results1['weighted_comment_sumAI'] / results1['textlen']) * 1000\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDgeISuW1gVH"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "def confidence_interval(data):\n",
        "    mean = np.mean(data)\n",
        "    std_dev = np.std(data, ddof=1)  # Use sample standard deviation\n",
        "    n = len(data)\n",
        "    t_value = stats.t.ppf(0.975, df=n-1)  # t critical value for 95% CI\n",
        "\n",
        "    lower_bound = mean - (t_value * std_dev / np.sqrt(n))\n",
        "    upper_bound = mean + (t_value * std_dev / np.sqrt(n))\n",
        "\n",
        "    return (lower_bound, mean, upper_bound)\n",
        "\n",
        "# Calculate confidence interval for AIcodedensity in results1\n",
        "confidence_interval(result_df.loc[result_df['select2']==0,'AIcodedensity'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozXLHACu2LUb"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "def confidence_interval(data):\n",
        "    mean = np.mean(data)\n",
        "    std_dev = np.std(data, ddof=1)  # Use sample standard deviation\n",
        "    n = len(data)\n",
        "    t_value = stats.t.ppf(0.975, df=n-1)  # t critical value for 95% CI\n",
        "\n",
        "    lower_bound = mean - (t_value * std_dev / np.sqrt(n))\n",
        "    upper_bound = mean + (t_value * std_dev / np.sqrt(n))\n",
        "\n",
        "    return (lower_bound, mean, upper_bound)\n",
        "\n",
        "# Calculate confidence interval for AIcodedensity in results1\n",
        "confidence_interval(results1.loc[results1['random']==1,'AIcodedensity'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no-2qABz0Rne"
      },
      "outputs": [],
      "source": [
        "results1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkjxvv0ayXTA"
      },
      "outputs": [],
      "source": [
        "results1['weighted_sum'].corr(results1['wcsr'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KmfJqQENyEI"
      },
      "source": [
        "# add thematic counts both\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppUimexrNyEN"
      },
      "outputs": [],
      "source": [
        "###round 1\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = {}\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        try:\n",
        "            with docx.open('word/comments.xml') as comments_xml:\n",
        "                tree = ET.parse(comments_xml)\n",
        "                root = tree.getroot()\n",
        "                for comment in root.findall('.//w:comment', ns):\n",
        "                    comment_id = comment.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                    comment_text = ''.join(node.text for node in comment.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    comments[comment_id] = comment_text\n",
        "        except KeyError:  # Handle case where comments.xml does not exist (no comments)\n",
        "            pass\n",
        "    return comments\n",
        "\n",
        "def parse_docx_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = parse_comments(file_path)\n",
        "    articles = []\n",
        "\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        with docx.open('word/document.xml') as document_xml:\n",
        "            tree = ET.parse(document_xml)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            current_article = {'title': None, 'comment_count': 0, 'comment_texts': []}\n",
        "            for child in root.iter():\n",
        "                if child.tag == '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p':\n",
        "                    text = ''.join(node.text for node in child.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    if text.startswith(\"TITLE:\"):\n",
        "                        if current_article['title'] is not None:  # Not the first article\n",
        "                            articles.append(current_article)\n",
        "                        current_article = {'title': text.replace(\"TITLE:\", \"\").strip(), 'comment_count': 0, 'comment_texts': []}\n",
        "                    for c in child.iter():\n",
        "                        if c.tag.endswith('commentRangeStart'):\n",
        "                            comment_id = c.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                            if comment_id in comments:\n",
        "                                current_article['comment_texts'].append(comments[comment_id])\n",
        "                                current_article['comment_count'] += 1\n",
        "\n",
        "            # Add the last article if it exists\n",
        "            if current_article['title'] is not None:\n",
        "                articles.append(current_article)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# List of docx files to process\n",
        "docx_files = ['ROUND 2 - ZAHRA.docx', 'ROUND 2 - SHARON.docx', 'ROUND 2 - AIMI.docx']\n",
        "base_path = '/content/drive/MyDrive/'\n",
        "\n",
        "# Dictionary to aggregate articles data\n",
        "articles_agg = defaultdict(lambda: {'comment_count': 0, 'comment_texts': []})\n",
        "\n",
        "for file_name in docx_files:\n",
        "    file_path = base_path + file_name\n",
        "    articles_data = parse_docx_comments(file_path)\n",
        "    for article in articles_data:\n",
        "        articles_agg[article['title']]['comment_count'] += article['comment_count']\n",
        "        articles_agg[article['title']]['comment_texts'] += article['comment_texts']\n",
        "\n",
        "# Convert the aggregated data into a list of dictionaries, then into a DataFrame\n",
        "articles_list = [{'title': title, 'comment_count': data['comment_count'], 'comment_texts': data['comment_texts'], 'Round': 1} for title, data in articles_agg.items()]\n",
        "df_articles = pd.DataFrame(articles_list)\n",
        "\n",
        "# Display the DataFrame\n",
        "#df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5OFzrsrNyEO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = {}\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        try:\n",
        "            with docx.open('word/comments.xml') as comments_xml:\n",
        "                tree = ET.parse(comments_xml)\n",
        "                root = tree.getroot()\n",
        "                for comment in root.findall('.//w:comment', ns):\n",
        "                    comment_id = comment.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                    comment_text = ''.join(node.text for node in comment.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    comments[comment_id] = comment_text\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return comments\n",
        "\n",
        "def parse_docx_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = parse_comments(file_path)\n",
        "    articles = []\n",
        "    current_article = None\n",
        "    current_text = []\n",
        "    blank_line_found = False\n",
        "    title_next = False\n",
        "\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        with docx.open('word/document.xml') as document_xml:\n",
        "            tree = ET.parse(document_xml)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            for child in root.iter():\n",
        "                if child.tag == '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p':\n",
        "                    text = ''.join(node.text for node in child.iter() if node.tag.endswith('}t') and node.text)\n",
        "\n",
        "                    if text.strip() == \"\":\n",
        "                        blank_line_found = True\n",
        "                    elif text.startswith(\"###\") and text.endswith(\"###\") and blank_line_found:\n",
        "                        if current_article is not None:\n",
        "                            current_article['content'] = ' '.join(current_text).strip()\n",
        "                            articles.append(current_article)\n",
        "                        current_article = {'title': None, 'content': '', 'comments': []}\n",
        "                        current_text = []\n",
        "                        title_next = True\n",
        "                        blank_line_found = False\n",
        "                    elif title_next:\n",
        "                        current_article['title'] = text.strip()\n",
        "                        title_next = False\n",
        "                    else:\n",
        "                        if text.startswith(\"###\") and text.endswith(\"###\"):\n",
        "                            text = text.strip(\"#\").strip()\n",
        "                        current_text.append(text)\n",
        "                        for c in child.iter():\n",
        "                            if c.tag.endswith('commentRangeStart'):\n",
        "                                comment_id = c.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                                if comment_id in comments:\n",
        "                                    current_article['comments'].append(comments[comment_id])\n",
        "\n",
        "            if current_article is not None:\n",
        "                current_article['content'] = ' '.join(current_text).strip()\n",
        "                articles.append(current_article)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Assuming df_articles is already an existing DataFrame\n",
        "# df_articles = pd.DataFrame(...)  # Your existing DataFrame initialization\n",
        "\n",
        "# List of docx files to process\n",
        "docx_files = ['Second Round Articles - Zahra.docx', 'Second Round Articles - Sharon.docx', 'Second Round Articles - Aimi.docx']\n",
        "base_path = '/content/drive/MyDrive/'\n",
        "\n",
        "# Dictionary to aggregate articles data\n",
        "articles_agg = defaultdict(lambda: {'comment_count': 0, 'comment_texts': []})\n",
        "\n",
        "for file_name in docx_files:\n",
        "    file_path = base_path + file_name\n",
        "    articles_data = parse_docx_comments(file_path)\n",
        "    for article in articles_data:\n",
        "        articles_agg[article['title']]['comment_count'] += len(article['comments'])\n",
        "        articles_agg[article['title']]['comment_texts'] += article['comments']\n",
        "\n",
        "# Convert the aggregated data into a list of dictionaries, then append to existing DataFrame\n",
        "articles_list = [{'title': title, 'comment_count': data['comment_count'], 'comment_texts': data['comment_texts'], 'Round': 2} for title, data in articles_agg.items()]\n",
        "df_new_articles = pd.DataFrame(articles_list)\n",
        "df_articles = pd.concat([df_articles, df_new_articles], ignore_index=True)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df_articles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mYGnFXSMgzU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = {}\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        try:\n",
        "            with docx.open('word/comments.xml') as comments_xml:\n",
        "                tree = ET.parse(comments_xml)\n",
        "                root = tree.getroot()\n",
        "                for comment in root.findall('.//w:comment', ns):\n",
        "                    comment_id = comment.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                    comment_text = ''.join(node.text for node in comment.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    comments[comment_id] = comment_text\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return comments\n",
        "\n",
        "def parse_docx_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = parse_comments(file_path)\n",
        "    articles = []\n",
        "    current_article = None\n",
        "    current_text = []\n",
        "    blank_line_found = False\n",
        "    title_next = False\n",
        "    char_count = 0  # Track the character count\n",
        "\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        with docx.open('word/document.xml') as document_xml:\n",
        "            tree = ET.parse(document_xml)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            for child in root.iter():\n",
        "                if child.tag == '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p':\n",
        "                    paragraph_start = char_count\n",
        "                    text = ''.join(node.text for node in child.iter() if node.tag.endswith('}t') and node.text)\n",
        "\n",
        "                    if text.strip() == \"\":\n",
        "                        blank_line_found = True\n",
        "                    elif text.startswith(\"###\") and text.endswith(\"###\") and blank_line_found:\n",
        "                        if current_article is not None:\n",
        "                            current_article['content'] = ' '.join(current_text).strip()\n",
        "                            current_article['length'] = char_count  # Set length of the article\n",
        "                            articles.append(current_article)\n",
        "                        current_article = {'title': None, 'content': '', 'comments': [], 'comment_positions': [], 'length': 0}\n",
        "                        current_text = []\n",
        "                        title_next = True\n",
        "                        blank_line_found = False\n",
        "                        char_count = 0  # Reset character count for new article\n",
        "                    elif title_next:\n",
        "                        current_article['title'] = text.strip()\n",
        "                        title_next = False\n",
        "                    else:\n",
        "                        if text.startswith(\"###\") and text.endswith(\"###\"):\n",
        "                            text = text.strip(\"#\").strip()\n",
        "                        current_text.append(text)\n",
        "                        char_count += len(text)  # Update character count\n",
        "                        for c in child.iter():\n",
        "                            if c.tag.endswith('commentRangeStart'):\n",
        "                                comment_id = c.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                                if comment_id in comments:\n",
        "                                    paragraph_end = char_count + len(text)  # Calculate end of paragraph\n",
        "                                    comment_position = (paragraph_start + paragraph_end) / 2  # Mean position\n",
        "                                    current_article['comments'].append(comments[comment_id])\n",
        "                                    current_article['comment_positions'].append(comment_position)\n",
        "\n",
        "            if current_article is not None:\n",
        "                current_article['content'] = ' '.join(current_text).strip()\n",
        "                current_article['length'] = char_count  # Set length of the last article\n",
        "                articles.append(current_article)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# List of docx files to process\n",
        "docx_files = ['Second Round Articles - Zahra.docx', 'Second Round Articles - Sharon.docx', 'Second Round Articles - Aimi.docx']\n",
        "base_path = '/content/drive/MyDrive/'\n",
        "\n",
        "# Dictionary to aggregate articles data\n",
        "articles_agg = defaultdict(lambda: {'comment_count': 0, 'comment_texts': [], 'comment_positions': [], 'length': 0})\n",
        "\n",
        "for file_name in docx_files:\n",
        "    file_path = base_path + file_name\n",
        "    articles_data = parse_docx_comments(file_path)\n",
        "    for article in articles_data:\n",
        "        articles_agg[article['title']]['comment_count'] += len(article['comments'])\n",
        "        articles_agg[article['title']]['comment_texts'] += article['comments']\n",
        "        articles_agg[article['title']]['comment_positions'] += article['comment_positions']\n",
        "        articles_agg[article['title']]['length'] += article['length']\n",
        "\n",
        "# Convert the aggregated data into a list of dictionaries, then into a DataFrame\n",
        "articles_list = [{'title': title, 'comment_count': data['comment_count'], 'comment_positions': data['comment_positions'], 'length': data['length'], 'Round': 2} for title, data in articles_agg.items()]\n",
        "df_new_articles = pd.DataFrame(articles_list)\n",
        "\n",
        "# Assuming df_articles is already an existing DataFrame\n",
        "# df_articles = pd.DataFrame(...)  # Your existing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kgn_gQYNyEO"
      },
      "outputs": [],
      "source": [
        "df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2HMzIIRNyEO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# Step 1: Create an index mapping for comments\n",
        "comment_index_mapping = {}\n",
        "index_counter = 0\n",
        "for comment_texts in df_articles['comment_texts']:\n",
        "    for text in comment_texts:\n",
        "        if text not in comment_index_mapping:\n",
        "            comment_index_mapping[text] = index_counter\n",
        "            index_counter += 1\n",
        "themes1=pd.read_csv('/content/drive/MyDrive/finalthemes.csv')\n",
        "themes1['THEMES'] = themes1['THEMES'].fillna(method='ffill')\n",
        "theme_changes = themes1['THEMES'] != themes1['THEMES'].shift(1)\n",
        "def clean_string(s):\n",
        "    # Remove all non-alphanumeric characters (except spaces)\n",
        "    cleaned = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    # Convert to lowercase\n",
        "    cleaned = cleaned.lower()\n",
        "    # Normalize spaces by removing extra spaces\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "    return cleaned\n",
        "# Convert the boolean series to integers (0 or 1) and compute the cumulative sum to create a theme index\n",
        "themes1['themeindex'] = theme_changes.cumsum()\n",
        "cleaned_comment_index_mapping = {clean_string(key): value for key, value in comment_index_mapping.items()}\n",
        "cleaned_comment_index_mapping ['former prime minister thinks it is unfair for malaysia to be on the receiving end in the conflict between refugees and their home country'] = cleaned_comment_index_mapping .pop('prime minister thinks it is unfair for malaysia to be on the receiving end in the conflict between refugees and their home country')\n",
        "\n",
        "# Preprocess the 'CODES' column in the themes DataFrame (assuming 'CODES' contains the thematic descriptions to match)\n",
        "# If 'CODES' is actually meant to be matched with another column that contains descriptions, replace 'CODES' with the correct column name\n",
        "themes1['processed_CODES'] = themes1['CODES'].apply(clean_string)\n",
        "\n",
        "# Map the cleaned 'CODES' column to the indices using the cleaned dictionary\n",
        "themes1['code_index'] = themes1['processed_CODES'].map(cleaned_comment_index_mapping)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgFBorzdNyEO"
      },
      "outputs": [],
      "source": [
        "themes1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVqWFoVpNyEO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file with the correct encoding\n",
        "try:\n",
        "    themes = pd.read_csv('/content/drive/MyDrive/codelist.csv', encoding='ISO-8859-1')  # Specify the correct encoding\n",
        "except UnicodeDecodeError as e:\n",
        "    print(f\"Error reading the CSV file: {e}\")\n",
        "    # Handle the error or re-raise it\n",
        "    raise\n",
        "\n",
        "# Drop rows with all blank values\n",
        "themes.dropna(how='all', inplace=True)\n",
        "\n",
        "# Fill missing 'Themes' values with the previous non-missing value\n",
        "themes['THEMES'] = themes['Themes'].fillna(method='ffill')\n",
        "\n",
        "# Identify theme changes\n",
        "theme_changes = themes['THEMES'] != themes['THEMES'].shift(1)\n",
        "\n",
        "# Clean a string by removing non-alphanumeric characters, converting to lowercase, and normalizing spaces\n",
        "def clean_string(s):\n",
        "    cleaned = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    cleaned = cleaned.lower()\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "    return cleaned\n",
        "\n",
        "# Compute the cumulative sum of theme changes to create a theme index\n",
        "themes['themeindex'] = theme_changes.cumsum()\n",
        "\n",
        "# Report types of values in the 'Codes' column to diagnose issues\n",
        "print(\"Diagnosing 'Codes' column values:\")\n",
        "for index, value in themes['Codes'].items():\n",
        "    if not isinstance(value, str):\n",
        "        print(f\"Non-string value found at index {index}: {value} (type: {type(value)})\")\n",
        "\n",
        "# Convert non-string values in 'Codes' to strings\n",
        "themes['Codes'] = themes['Codes'].astype(str)\n",
        "\n",
        "# Preprocess the 'Codes' column in the themes DataFrame\n",
        "themes['processed_CODES'] = themes['Codes'].apply(clean_string)\n",
        "\n",
        "# Create a simple index for each unique processed_CODE\n",
        "processed_code_index_mapping = {code: idx for idx, code in enumerate(themes['processed_CODES'].unique())}\n",
        "\n",
        "# Map the processed_CODES column to the indices using the new dictionary\n",
        "themes['code_index'] = themes['processed_CODES'].map(processed_code_index_mapping)\n",
        "\n",
        "# Additional reporting to verify the results\n",
        "print(\"Processed Codes and Indices:\")\n",
        "print(themes[['Codes', 'processed_CODES', 'code_index']].head())\n",
        "\n",
        "# Generate the crosswalk comment_index_mapping\n",
        "comment_index_mapping = {}\n",
        "for idx, row in themes.iterrows():\n",
        "    processed_code = row['processed_CODES']\n",
        "    code_index = row['code_index']\n",
        "    comment_index_mapping[processed_code] = code_index\n",
        "\n",
        "# Output the final comment_index_mapping\n",
        "print(\"Final comment_index_mapping:\")\n",
        "print(comment_index_mapping)\n",
        "# Preprocess the 'Codes' column in the themes DataFrame\n",
        "themes['processed_CODES'] = themes['Codes'].apply(clean_string)\n",
        "\n",
        "# Create a simple index for each unique processed_CODE\n",
        "processed_code_index_mapping = {code: idx for idx, code in enumerate(themes['processed_CODES'].unique())}\n",
        "\n",
        "# Map the processed_CODES column to the indices using the new dictionary\n",
        "themes['code_index'] = themes['processed_CODES'].map(processed_code_index_mapping)\n",
        "\n",
        "# Additional reporting to verify the results\n",
        "print(\"Processed Codes and Indices:\")\n",
        "print(themes[['Codes', 'processed_CODES', 'code_index']].head())\n",
        "\n",
        "# Generate the crosswalk comment_index_mapping\n",
        "comment_index_mapping = {}\n",
        "for idx, row in themes.iterrows():\n",
        "    processed_code = row['processed_CODES']\n",
        "    code_index = row['code_index']\n",
        "    comment_index_mapping[processed_code] = code_index\n",
        "\n",
        "# Output the final comment_index_mapping\n",
        "print(\"Final comment_index_mapping:\")\n",
        "print(comment_index_mapping)\n",
        "###revised of above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7h2am0POspG"
      },
      "outputs": [],
      "source": [
        "combined_codes = pd.concat([themes['processed_CODES'], themes1['processed_CODES']]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# 2. Create an index for the codes\n",
        "combined_codes_index = combined_codes.reset_index().rename(columns={'index': 'code_index'})\n",
        "\n",
        "# 3. Create a dictionary based on this index\n",
        "comment_index_mapping = pd.Series(combined_codes_index.code_index.values, index=combined_codes_index.processed_CODES).to_dict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn-nnWcoNyEP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles is your DataFrame from the previous steps\n",
        "# and it includes a 'comment_texts' column with lists of comment texts for each article\n",
        "\n",
        "df_articles['cleaned_comment_texts'] = df_articles['comment_texts'].apply(lambda texts: [clean_string(text) for text in texts])\n",
        "\n",
        "df_articles['comment_indices'] = df_articles['cleaned_comment_texts'].apply(lambda texts: [comment_index_mapping.get(text, -1) for text in texts])\n",
        "\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "all_indices = [index for sublist in df_articles['comment_indices'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in df_articles['comment_indices']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "df_articles['weighted_comment_sum'] = weighted_comment_sums\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as6-N6u5NyEP"
      },
      "outputs": [],
      "source": [
        "code_to_theme_mapping = themes.assign(code_index=themes['code_index'].astype(float).astype(int)).set_index('code_index')['themeindex'].to_dict()\n",
        "\n",
        "\n",
        "# Step 2: Map comment_indices to theme_indices using the created mapping\n",
        "df_articles['theme_indices'] = df_articles['comment_indices'].apply(lambda indices: [code_to_theme_mapping.get(index) for index in indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xweFb8l1NyEP"
      },
      "outputs": [],
      "source": [
        "all_theme_indices = [index for sublist in df_articles['theme_indices'] for index in sublist if index is not None]  # Ensure None values are excluded\n",
        "theme_counts = pd.Series(all_theme_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3 for themes: Sum the number of comments for each article, weighted by 1/n, using theme indices\n",
        "weighted_theme_sums = []\n",
        "for theme_indices in df_articles['theme_indices']:\n",
        "    # Exclude None values to ensure only valid mappings are considered for the weighted sum\n",
        "    valid_theme_indices = [index for index in theme_indices if index is not None]\n",
        "    weighted_sum = sum(1 / theme_counts[index] for index in valid_theme_indices)\n",
        "    weighted_theme_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted theme sums to the DataFrame\n",
        "df_articles['weighted_theme_sum'] = weighted_theme_sums\n",
        "df_articles['theme_indices'] = df_articles['theme_indices'].apply(lambda indices: [0 if index is None else index for index in indices])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLZYrACANyEP"
      },
      "outputs": [],
      "source": [
        "merged_df=pd.read_csv('/content/drive/MyDrive/fullcodes.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF3ShZUgNyEQ"
      },
      "outputs": [],
      "source": [
        "!pip install pacmap\n",
        "!pip install hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrmkdELCNyEQ"
      },
      "outputs": [],
      "source": [
        "merged_df['embeddings3'] = merged_df['embeddings3'].apply(eval)\n",
        "\n",
        "# Convert these lists to a numpy array for processing with pacmap\n",
        "embeddings_array = np.array(merged_df['embeddings3'].tolist())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "856jnm2ANyEQ"
      },
      "outputs": [],
      "source": [
        "# Step 3: Apply pacmap to obtain a 2D embedding\n",
        "# Initialize PaCMAP instance and fit-transform the data to get 2D embeddings\n",
        "import pacmap\n",
        "import hdbscan\n",
        "\n",
        "pacmap_instance = pacmap.PaCMAP(n_components=6)\n",
        "embeddings_2d = pacmap_instance.fit_transform(embeddings_array)\n",
        "\n",
        "# Step 4: Apply hdbscan on the pacmap embeddings to perform clustering\n",
        "# Initialize HDBSCAN, fit it on the 2D embeddings, and obtain cluster labels\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=50,min_samples=20, gen_min_span_tree=True)\n",
        "cluster_labels = clusterer.fit_predict(embeddings_2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwks_ZmGNyEQ"
      },
      "outputs": [],
      "source": [
        "# Count unique values\n",
        "unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
        "\n",
        "# Print unique values and their counts\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f'Label {label}: {count}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlPnVfcxNyEQ"
      },
      "outputs": [],
      "source": [
        "merged_df['cluster3'] = cluster_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APlBjC1ZNyEQ"
      },
      "outputs": [],
      "source": [
        "grouped_df = merged_df.groupby('fulltext').agg({\n",
        "   # 'cluster4': lambda x: list(x.unique()),  # Get unique values for 'cluster3'\n",
        "    'cluster3': lambda x: list(x.unique()),  # Get unique values for 'cluster3'\n",
        "    'codingd': lambda x: list(x.unique()),   # Get unique values for 'codingd'\n",
        "    'title': 'first'                         # Keep the first title for each group\n",
        "}).reset_index()\n",
        "\n",
        "# Add a new column 'textlen' to store the character length of the 'fulltext' strings\n",
        "grouped_df['textlen'] = grouped_df['fulltext'].apply(len)\n",
        "\n",
        "# Filter rows where the length of 'fulltext' is greater than 1200 characters\n",
        "grouped_df = grouped_df.loc[grouped_df['textlen'] > 1200]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbUL8OKUPbT4"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADhNSnYtNyER"
      },
      "outputs": [],
      "source": [
        "result_df = pd.merge(df_articles, grouped_df[['title', 'fulltext', 'cluster3', 'codingd','textlen']], on='title', how='left')\n",
        "result_df = result_df.drop_duplicates(subset=['title'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "900iIm7tNyER"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles is your DataFrame from the previous steps\n",
        "# and it includes a 'comment_texts' column with lists of comment texts for each article\n",
        "\n",
        "# Step 1: Create an index mapping for comments\n",
        "comment_index_mapping = {}\n",
        "index_counter = 0\n",
        "for comment_texts in result_df['codingd']:\n",
        "    for text in comment_texts:\n",
        "        if text not in comment_index_mapping:\n",
        "            comment_index_mapping[text] = index_counter\n",
        "            index_counter += 1\n",
        "\n",
        "result_df['AIcode_indices'] = result_df['codingd'].apply(lambda texts: [comment_index_mapping[text] for text in texts])\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "all_indices = [index for sublist in result_df['AIcode_indices'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in result_df['AIcode_indices']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "result_df['AIcodesum'] = weighted_comment_sums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YnaQKUDNyER"
      },
      "outputs": [],
      "source": [
        "\n",
        "all_indices = [index for sublist in result_df['cluster3'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in result_df['cluster3']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "result_df['AIcodedensity']=result_df['AIcodesum']/result_df['textlen']*1000\n",
        "result_df['codedensity']=result_df['weighted_comment_sum']/result_df['textlen']*1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mrp9aUbNyER"
      },
      "outputs": [],
      "source": [
        "result_df[['AIcodesum',  'weighted_comment_sum']].corr()**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkBKn7BmREO1"
      },
      "outputs": [],
      "source": [
        "fulllist=pd.read_csv('/content/drive/MyDrive/AIcorpussets1and2full.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR9mznXORJAl"
      },
      "outputs": [],
      "source": [
        "result_df=result_df.merge(fulllist[['title', 'random2', 'select2', 'random']], on='title', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i05yAsfXsb5B"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGPWwgmHS7rN"
      },
      "outputs": [],
      "source": [
        "result_df['rand']=result_df['random2']+result_df['random']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpQLQWBlSD6c"
      },
      "outputs": [],
      "source": [
        "result_df2[(result_df2['Round'] == 1) & (result_df2['select2'] == 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV4D8SydwTS4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yhi0i2IDNyER"
      },
      "outputs": [],
      "source": [
        "result_df.to_csv('/content/drive/MyDrive/AIcorpusallresults.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsOFVbXOMo-G"
      },
      "outputs": [],
      "source": [
        "result_df=pd.read_csv('/content/drive/MyDrive/AIcorpusallresults.csv')\n",
        "result_df2=pd.read_csv('/content/drive/MyDrive/AIcorpus2results.csv')\n",
        "import ast\n",
        "# Function to convert string representation of list to actual list\n",
        "def convert_to_list(string):\n",
        "    return ast.literal_eval(string)\n",
        "\n",
        "# Apply the function to the 'comment_indices' column\n",
        "result_df['comment_indices'] = result_df['comment_indices'].apply(convert_to_list)\n",
        "result_df2['theme_indices'] = result_df2['theme_indices'].apply(convert_to_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB6BUjcUdgkn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcB9tDnWtUu7"
      },
      "outputs": [],
      "source": [
        "len(result_df[result_df.select2==1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjDkVtGhSnqW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "# Filter data for random == 1 and random2 == 1\n",
        "data_random_1 = result_df[result_df['random'] == 1]['AIcodedensity']\n",
        "data_random2_1 = result_df[result_df['random2'] == 1]['AIcodedensity']\n",
        "\n",
        "# Function to calculate mean and 95% CI\n",
        "def mean_and_ci(data):\n",
        "    mean = np.mean(data)\n",
        "    ci = stats.t.interval(0.95, len(data)-1, loc=mean, scale=stats.sem(data))\n",
        "    return mean, ci\n",
        "\n",
        "# Calculate for random == 1\n",
        "mean_random_1, ci_random_1 = mean_and_ci(data_random_1)\n",
        "\n",
        "# Calculate for random2 == 1\n",
        "mean_random2_1, ci_random2_1 = mean_and_ci(data_random2_1)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean for random == 1: {mean_random_1}, 95% CI: {ci_random_1}\")\n",
        "print(f\"Mean for random2 == 1: {mean_random2_1}, 95% CI: {ci_random2_1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3PBhNMY2C8X"
      },
      "outputs": [],
      "source": [
        "###including the 3 overlap articles\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "# Subset the rows where \"random2\" > 0 or \"select2\" > 0\n",
        "filtered_df = result_df[(result_df['Round'] > 1)]\n",
        "\n",
        "# Define the dependent and independent variables\n",
        "X = filtered_df['select2']\n",
        "y = filtered_df['codedensity']\n",
        "\n",
        "# Add a constant to the independent variables\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Run the linear regression\n",
        "model1 = sm.OLS(y, X).fit()\n",
        "# Report the coefficients\n",
        "print(model1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxDnB05mUY_U"
      },
      "outputs": [],
      "source": [
        "###including the 3 overlap articles\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "# Subset the rows where \"random2\" > 0 or \"select2\" > 0\n",
        "filtered_df = result_df[(result_df['random2'] > 0) | (result_df['select2'] > 0)]\n",
        "\n",
        "# Define the dependent and independent variables\n",
        "X = filtered_df['select2']\n",
        "y = filtered_df['codedensity']\n",
        "\n",
        "# Add a constant to the independent variables\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Run the linear regression\n",
        "model2 = sm.OLS(y, X).fit()\n",
        "# Report the coefficients\n",
        "print(model2.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ttKawGL-0UZ"
      },
      "outputs": [],
      "source": [
        "result_df.textlen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoK-1R273qiH"
      },
      "outputs": [],
      "source": [
        "###including the 3 overlap articles\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "# Subset the rows where \"random2\" > 0 or \"select2\" > 0\n",
        "filtered_df = result_df[(result_df['Round'] >1)]\n",
        "\n",
        "# Define the dependent and independent variables\n",
        "X = filtered_df[['select2','index','indsq']]\n",
        "y = filtered_df['codedensity']\n",
        "\n",
        "# Add a constant to the independent variables\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Run the linear regression\n",
        "model3 = sm.OLS(y, X).fit()\n",
        "# Report the coefficients\n",
        "print(model3.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8gQvjjb9gAs"
      },
      "outputs": [],
      "source": [
        "###including the 3 overlap articles\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "# Subset the rows where \"random2\" > 0 or \"select2\" > 0\n",
        "filtered_df = result_df[(result_df['rand'] > 0) | (result_df['select2'] > 0)]\n",
        "\n",
        "# Define the dependent and independent variables\n",
        "X = filtered_df[['select2','Round']]\n",
        "y = filtered_df['codedensity']\n",
        "\n",
        "# Add a constant to the independent variables\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Run the linear regression\n",
        "model4 = sm.OLS(y, X).fit()\n",
        "# Report the coefficients\n",
        "print(model4.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9Ik9l4G94Nx"
      },
      "outputs": [],
      "source": [
        "###including the 3 overlap articles\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "# Subset the rows where \"random2\" > 0 or \"select2\" > 0\n",
        "filtered_df = result_df[(result_df['rand'] > 0) | (result_df['select2'] > 0)]\n",
        "\n",
        "# Define the dependent and independent variables\n",
        "X = filtered_df[['select2','Round','index','indsq']]\n",
        "y = filtered_df['codedensity']\n",
        "\n",
        "# Add a constant to the independent variables\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Run the linear regression\n",
        "model5 = sm.OLS(y, X).fit()\n",
        "# Report the coefficients\n",
        "print(model5.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBX8a0EK-ZXI"
      },
      "outputs": [],
      "source": [
        "###including the 3 overlap articles\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "# Subset the rows where \"random2\" > 0 or \"select2\" > 0\n",
        "filtered_df = result_df[(result_df['Round'] > 1)]\n",
        "\n",
        "# Define the dependent and independent variables\n",
        "X = filtered_df['select2']\n",
        "y = filtered_df['codedensity']\n",
        "\n",
        "# Add a constant to the independent variables\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Use the inverse of 'textlen' as weights\n",
        "weights =filtered_df['textlen']\n",
        "\n",
        "# Run the weighted least squares (WLS) regression\n",
        "model6 = sm.WLS(y, X, weights=weights).fit()\n",
        "\n",
        "# Report the coefficients\n",
        "print(model6.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fArdqIBG7oOh"
      },
      "outputs": [],
      "source": [
        "!pip install stargazer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIckxhcR5iUZ"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import HTML\n",
        "import statsmodels.api as sm\n",
        "from stargazer.stargazer import Stargazer\n",
        "# Create a stargazer object\n",
        "stargazer = Stargazer([model1, model2, model3, model4, model5, model6])\n",
        "stargazer.covariate_order(['const', 'select2','Round','index','indsq'])\n",
        "# Render the table in HTML format\n",
        "html_output = stargazer.render_html()\n",
        "# Display the HTML table in Colab\n",
        "HTML(html_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM_TNloI2yMb"
      },
      "outputs": [],
      "source": [
        "from statsmodels.iolib.summary2 import summary_col\n",
        "models = [model1, model2, model3]\n",
        "\n",
        "info_dict = {\n",
        "    'Overlap articles': { 'Model 1': 'False', 'Model 2': 'True', 'Model 3': 'False' },\n",
        "    'Order controls': { 'Model 1': 'False', 'Model 2': 'False', 'Model 3': 'True' }\n",
        "}\n",
        "results_table = summary_col(models, stars=True, model_names=['Model 1', 'Model 2', 'Model 3'], info_dict=info_dict)\n",
        "\n",
        "# Print the summary table\n",
        "print(results_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szy5A787WI38"
      },
      "outputs": [],
      "source": [
        "###unique values as more text is read, selected vs random\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "#result_df['AIcode_indices'] =result_df['AIcode_indices'].apply(ast.literal_eval)\n",
        "#result_df['comment_indices'] =result_df['comment_indices'].apply(ast.literal_eval)\n",
        "# Filter data for Round 2 and select2 == 1\n",
        "round_2_select1_df = result_df[(result_df['Round'] == 2 )& (result_df['select2'] == 1)]\n",
        "round_2_select1_df['Cumulative_Textlen'] = round_2_select1_df['textlen'].cumsum()\n",
        "\n",
        "# Calculate the running tally of unique values for select2 == 1\n",
        "unique_values_select1 = set()\n",
        "running_tally_select1 = []\n",
        "\n",
        "for indices in round_2_select1_df['comment_indices']:\n",
        "    unique_values_select1.update(indices)\n",
        "    running_tally_select1.append(len(unique_values_select1))\n",
        "\n",
        "# Add the running tally to the filtered DataFrame for select2 == 1\n",
        "round_2_select1_df['Running_Tally'] = running_tally_select1\n",
        "round_2_select1_df.reset_index(inplace=True)\n",
        "\n",
        "# Filter data for Round 2 and select2 == 0\n",
        "round_2_select0_df = result_df[(result_df['Round'] == 2 )& (result_df['select2'] == 0)]\n",
        "round_2_select0_df['Cumulative_Textlen'] = round_2_select0_df['textlen'].cumsum()\n",
        "\n",
        "# Calculate the running tally of unique values for select2 == 0\n",
        "unique_values_select0 = set()\n",
        "running_tally_select0 = []\n",
        "\n",
        "for indices in round_2_select0_df['comment_indices']:\n",
        "    unique_values_select0.update(indices)\n",
        "    running_tally_select0.append(len(unique_values_select0))\n",
        "\n",
        "# Add the running tally to the filtered DataFrame for select2 == 0\n",
        "round_2_select0_df['Running_Tally'] = running_tally_select0\n",
        "round_2_select0_df.reset_index(inplace=True)\n",
        "\n",
        "# Plot the running tally for both subsets\n",
        "plt.plot(round_2_select1_df['Cumulative_Textlen'], round_2_select1_df['Running_Tally'], marker='o', label='AI Selected')\n",
        "plt.plot(round_2_select0_df['Cumulative_Textlen'], round_2_select0_df['Running_Tally'], marker='x', label='Randomly Selected')\n",
        "\n",
        "plt.xlabel('Cumulative Corpus Length in characters')\n",
        "plt.ylabel('Cumulative Unique Codes')\n",
        "plt.title('Cumulative Unique Codes for random and AI-selected documents')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfBMkP1kqzRa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to calculate running tally of unique values\n",
        "def calculate_running_tally_and_corpus_length(df):\n",
        "    unique_values = set()\n",
        "    running_tally = []\n",
        "    cumulative_textlen = df['textlen'].cumsum().tolist()\n",
        "    for indices in df['comment_indices']:\n",
        "        unique_values.update(indices)\n",
        "        running_tally.append(len(unique_values))\n",
        "    return running_tally, cumulative_textlen\n",
        "\n",
        "# Function to perform subsampling and aggregation with adjusted variance\n",
        "def subsample_and_aggregate(df, subsample_size, n_iterations=2000):\n",
        "    all_running_tallies = []\n",
        "    all_cumulative_lengths = []\n",
        "    original_sample_size = len(df)\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        subsample_df = df.sample(n=subsample_size, replace=False).reset_index(drop=True)\n",
        "        running_tally, cumulative_length = calculate_running_tally_and_corpus_length(subsample_df)\n",
        "        all_running_tallies.append(running_tally)\n",
        "        all_cumulative_lengths.append(cumulative_length)\n",
        "\n",
        "    all_running_tallies = np.array(all_running_tallies)\n",
        "    all_cumulative_lengths = np.array(all_cumulative_lengths)\n",
        "\n",
        "    mean_tally = all_running_tallies.mean(axis=0)\n",
        "    subsample_variance = all_running_tallies.var(axis=0, ddof=1)\n",
        "\n",
        "    # Adjust variance using finite population correction (FPC) for each step\n",
        "    adjusted_variance = []\n",
        "    for i in range(subsample_size):\n",
        "        fpc = np.sqrt((original_sample_size - (i + 1)) / (original_sample_size - 1))\n",
        "        adjusted_variance.append(subsample_variance[i] / (fpc ** 2))\n",
        "\n",
        "    adjusted_variance = np.array(adjusted_variance)\n",
        "    ci_95_tally = 1.96 * np.sqrt(adjusted_variance)\n",
        "\n",
        "    mean_length = all_cumulative_lengths.mean(axis=0)\n",
        "    return mean_tally, ci_95_tally, mean_length\n",
        "\n",
        "# Filter data for Round 2\n",
        "round_2_df = result_df[result_df['Round'] == 2]\n",
        "\n",
        "# Split data into AI-selected and randomly selected\n",
        "round_2_select1_df = round_2_df[round_2_df['select2'] == 1]\n",
        "round_2_select0_df = round_2_df[round_2_df['select2'] == 0]\n",
        "\n",
        "# Determine subsample size as a proportion of the original sample size\n",
        "subsample_size_select1 = int(len(round_2_select1_df) * 0.9)  # Example: 90% of the original size\n",
        "subsample_size_select0 = int(len(round_2_select0_df) * 0.9)\n",
        "\n",
        "# Perform subsampling and aggregation\n",
        "mean_tally_select1, ci_95_tally_select1, mean_length_select1 = subsample_and_aggregate(round_2_select1_df, subsample_size_select1)\n",
        "mean_tally_select0, ci_95_tally_select0, mean_length_select0 = subsample_and_aggregate(round_2_select0_df, subsample_size_select0)\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(mean_length_select1, mean_tally_select1, marker='o', label='AI Selected')\n",
        "plt.fill_between(mean_length_select1, mean_tally_select1 - ci_95_tally_select1, mean_tally_select1 + ci_95_tally_select1, alpha=0.2)\n",
        "\n",
        "plt.plot(mean_length_select0, mean_tally_select0, marker='x', label='Randomly Selected')\n",
        "plt.fill_between(mean_length_select0, mean_tally_select0 - ci_95_tally_select0, mean_tally_select0 + ci_95_tally_select0, alpha=0.2)\n",
        "\n",
        "plt.xlabel('Mean Cumulative Corpus Length in characters')\n",
        "plt.ylabel('Mean Cumulative Unique Codes')\n",
        "plt.title('Cumulative Unique Codes for random and AI-selected documents with 95% CIs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JHaEynZT9If"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming result_df is already defined and contains the required data\n",
        "# Convert 'comment_indices' to lists if needed\n",
        "\n",
        "\n",
        "# Filter data for select2 == 1 and random2 == 1\n",
        "select2_df = result_df[result_df['select2'] == 1]\n",
        "random2_df = result_df[result_df['random2'] == 1]\n",
        "\n",
        "# Flatten the list of all indices for select2 == 1 and random2 == 1 to count frequency\n",
        "select2_all_indices = [index for indices in select2_df['comment_indices'] for index in indices]\n",
        "random2_all_indices = [index for indices in random2_df['comment_indices'] for index in indices]\n",
        "\n",
        "# Count frequency of indices\n",
        "select2_index_counts = Counter(select2_all_indices)\n",
        "random2_index_counts = Counter(random2_all_indices)\n",
        "\n",
        "# Identify high-frequency codes (3 or more instances)\n",
        "select2_high_frequency_codes = {index for index, count in select2_index_counts.items() if count >= 3}\n",
        "random2_high_frequency_codes = {index for index, count in random2_index_counts.items() if count >= 3}\n",
        "\n",
        "# Function to filter only high-frequency indices\n",
        "def filter_high_frequency_indices(indices, high_freq_codes):\n",
        "    return [index for index in indices if index in high_freq_codes]\n",
        "\n",
        "# Create the new columns with high-frequency comment indices\n",
        "result_df['hf_comment_indices_AI'] = result_df['comment_indices'].apply(lambda indices: filter_high_frequency_indices(indices, select2_high_frequency_codes))\n",
        "result_df['hf_comment_indices_r'] = result_df['comment_indices'].apply(lambda indices: filter_high_frequency_indices(indices, random2_high_frequency_codes))\n",
        "\n",
        "# Display the dataframe with the new columns\n",
        "print(result_df[['comment_indices', 'hf_comment_indices_AI', 'hf_comment_indices_r']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifxgvEr5UiwA"
      },
      "outputs": [],
      "source": [
        "###unique values as more text is read, selected vs random\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "#result_df['AIcode_indices'] =result_df['AIcode_indices'].apply(ast.literal_eval)\n",
        "#result_df['comment_indices'] =result_df['comment_indices'].apply(ast.literal_eval)\n",
        "# Filter data for Round 2 and select2 == 1\n",
        "round_2_select1_df = result_df[(result_df['Round'] == 2 )& (result_df['select2'] == 1)]\n",
        "round_2_select1_df['Cumulative_Textlen'] = round_2_select1_df['textlen'].cumsum()\n",
        "\n",
        "# Calculate the running tally of unique values for select2 == 1\n",
        "unique_values_select1 = set()\n",
        "running_tally_select1 = []\n",
        "\n",
        "for indices in round_2_select1_df['hf_comment_indices_AI']:\n",
        "    unique_values_select1.update(indices)\n",
        "    running_tally_select1.append(len(unique_values_select1))\n",
        "\n",
        "# Add the running tally to the filtered DataFrame for select2 == 1\n",
        "round_2_select1_df['Running_Tally'] = running_tally_select1\n",
        "round_2_select1_df.reset_index(inplace=True)\n",
        "\n",
        "# Filter data for Round 2 and select2 == 0\n",
        "round_2_select0_df = result_df[(result_df['Round'] == 2 )& (result_df['select2'] == 0)]\n",
        "round_2_select0_df['Cumulative_Textlen'] = round_2_select0_df['textlen'].cumsum()\n",
        "\n",
        "# Calculate the running tally of unique values for select2 == 0\n",
        "unique_values_select0 = set()\n",
        "running_tally_select0 = []\n",
        "\n",
        "for indices in round_2_select0_df['hf_comment_indices_r']:\n",
        "    unique_values_select0.update(indices)\n",
        "    running_tally_select0.append(len(unique_values_select0))\n",
        "\n",
        "# Add the running tally to the filtered DataFrame for select2 == 0\n",
        "round_2_select0_df['Running_Tally'] = running_tally_select0\n",
        "round_2_select0_df.reset_index(inplace=True)\n",
        "\n",
        "# Plot the running tally for both subsets\n",
        "plt.plot(round_2_select1_df['Cumulative_Textlen'], round_2_select1_df['Running_Tally'], marker='o', label='AI Selected')\n",
        "plt.plot(round_2_select0_df['Cumulative_Textlen'], round_2_select0_df['Running_Tally'], marker='x', label='Randomly Selected')\n",
        "\n",
        "plt.xlabel('Cumulative Corpus Length in characters')\n",
        "plt.ylabel('Cumulative Unique High Frequency Codes')\n",
        "plt.title('Cumulative Codes for Random and AI-selected Documents')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gv3iYUWV5Dy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to calculate running tally of unique values\n",
        "def calculate_running_tally_and_corpus_length(df, column):\n",
        "    unique_values = set()\n",
        "    running_tally = []\n",
        "    cumulative_textlen = df['textlen'].cumsum().tolist()\n",
        "    for indices in df[column]:\n",
        "        unique_values.update(indices)\n",
        "        running_tally.append(len(unique_values))\n",
        "    return running_tally, cumulative_textlen\n",
        "\n",
        "# Function to perform subsampling and aggregation with adjusted variance\n",
        "def subsample_and_aggregate(df, subsample_size, column, n_iterations=2000):\n",
        "    all_running_tallies = []\n",
        "    all_cumulative_lengths = []\n",
        "    original_sample_size = len(df)\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        subsample_df = df.sample(n=subsample_size, replace=False).reset_index(drop=True)\n",
        "        running_tally, cumulative_length = calculate_running_tally_and_corpus_length(subsample_df, column)\n",
        "        all_running_tallies.append(running_tally)\n",
        "        all_cumulative_lengths.append(cumulative_length)\n",
        "\n",
        "    all_running_tallies = np.array(all_running_tallies)\n",
        "    all_cumulative_lengths = np.array(all_cumulative_lengths)\n",
        "\n",
        "    mean_tally = all_running_tallies.mean(axis=0)\n",
        "    subsample_variance = all_running_tallies.var(axis=0, ddof=1)\n",
        "\n",
        "    # Adjust variance using finite population correction (FPC) for each step\n",
        "    adjusted_variance = []\n",
        "    for i in range(subsample_size):\n",
        "        fpc = np.sqrt((original_sample_size - (i + 1)) / (original_sample_size - 1))\n",
        "        adjusted_variance.append(subsample_variance[i] / (fpc ** 2))\n",
        "\n",
        "    adjusted_variance = np.array(adjusted_variance)\n",
        "    ci_95_tally = 1.96 * np.sqrt(adjusted_variance)\n",
        "\n",
        "    mean_length = all_cumulative_lengths.mean(axis=0)\n",
        "    return mean_tally, ci_95_tally, mean_length\n",
        "\n",
        "# Filter data for Round 2\n",
        "round_2_df = result_df[result_df['Round'] == 2]\n",
        "\n",
        "# Split data into AI-selected and randomly selected\n",
        "round_2_select1_df = round_2_df[round_2_df['select2'] == 1]\n",
        "round_2_select0_df = round_2_df[round_2_df['select2'] == 0]\n",
        "\n",
        "# Determine subsample size as a proportion of the original sample size\n",
        "subsample_size_select1 = int(len(round_2_select1_df) * 0.9)  # Example: 90% of the original size\n",
        "subsample_size_select0 = int(len(round_2_select0_df) * 0.9)\n",
        "\n",
        "# Perform subsampling and aggregation\n",
        "mean_tally_select1, ci_95_tally_select1, mean_length_select1 = subsample_and_aggregate(round_2_select1_df, subsample_size_select1, 'hf_comment_indices_AI')\n",
        "mean_tally_select0, ci_95_tally_select0, mean_length_select0 = subsample_and_aggregate(round_2_select0_df, subsample_size_select0, 'hf_comment_indices_r')\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(mean_length_select1, mean_tally_select1, marker='o', label='AI Selected')\n",
        "plt.fill_between(mean_length_select1, mean_tally_select1 - ci_95_tally_select1, mean_tally_select1 + ci_95_tally_select1, alpha=0.2)\n",
        "\n",
        "plt.plot(mean_length_select0, mean_tally_select0, marker='x', label='Randomly Selected')\n",
        "plt.fill_between(mean_length_select0, mean_tally_select0 - ci_95_tally_select0, mean_tally_select0 + ci_95_tally_select0, alpha=0.2)\n",
        "\n",
        "plt.xlabel('Mean Cumulative Corpus Length in characters')\n",
        "plt.ylabel('Mean Cumulative Unique High Frequency Codes')\n",
        "plt.title('Cumulative Codes for random and AI-selected documents with 95% CIs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrk5wiymyFk0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Revised function: now uses the full set from \"comment_indices\"\n",
        "def calculate_running_tally_and_corpus_length(df, code_column):\n",
        "    \"\"\"\n",
        "    Iteratively computes the cumulative count of unique codes (from the full set)\n",
        "    that have been encountered at least 3 times based solely on the documents observed so far.\n",
        "    It also computes the cumulative sum of the document lengths (from the 'textlen' column).\n",
        "\n",
        "    Parameters:\n",
        "      - df: DataFrame containing the data.\n",
        "      - code_column: Column name where each entry is a list of codes.\n",
        "\n",
        "    Returns:\n",
        "      - running_tally: List where each element is the cumulative count of codes (with 3+ occurrences)\n",
        "                       based on documents processed up to that point.\n",
        "      - cumulative_textlen: List of cumulative text lengths up to each step.\n",
        "    \"\"\"\n",
        "    running_counter = Counter()\n",
        "    running_tally = []          # Cumulative count of codes (with frequency >= 3)\n",
        "    cumulative_textlen = []     # Cumulative text length\n",
        "    current_length = 0\n",
        "\n",
        "    # Process each document sequentially\n",
        "    for codes, textlen in zip(df[code_column], df['textlen']):\n",
        "        running_counter.update(codes)   # Update counter with codes from the current document\n",
        "        current_length += textlen         # Add current document's text length\n",
        "        # Count how many unique codes have now been seen at least 3 times\n",
        "        hf_count = sum(1 for count in running_counter.values() if count >= 3)\n",
        "        running_tally.append(hf_count)\n",
        "        cumulative_textlen.append(current_length)\n",
        "\n",
        "    return running_tally, cumulative_textlen\n",
        "\n",
        "# Function to perform subsampling and aggregation using the iterative approach\n",
        "def subsample_and_aggregate(df, subsample_size, code_column, n_iterations=2000):\n",
        "    \"\"\"\n",
        "    Performs repeated subsampling (without replacement) from the DataFrame, computes the iterative\n",
        "    running tally and cumulative corpus length for each subsample, and then aggregates the results.\n",
        "    Adjusts variance with a finite population correction.\n",
        "\n",
        "    Parameters:\n",
        "      - df: DataFrame containing the data.\n",
        "      - subsample_size: Number of documents to include in each subsample.\n",
        "      - code_column: Column name where each entry is a list of codes.\n",
        "      - n_iterations: Number of subsamples to take.\n",
        "\n",
        "    Returns:\n",
        "      - mean_tally: Mean running tally (averaged over iterations).\n",
        "      - ci_95_tally: 95% confidence interval for the running tally.\n",
        "      - mean_length: Mean cumulative text length.\n",
        "    \"\"\"\n",
        "    all_running_tallies = []\n",
        "    all_cumulative_lengths = []\n",
        "    original_sample_size = len(df)\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        subsample_df = df.sample(n=subsample_size, replace=False).reset_index(drop=True)\n",
        "        running_tally, cumulative_length = calculate_running_tally_and_corpus_length(subsample_df, code_column)\n",
        "        all_running_tallies.append(running_tally)\n",
        "        all_cumulative_lengths.append(cumulative_length)\n",
        "\n",
        "    all_running_tallies = np.array(all_running_tallies)\n",
        "    all_cumulative_lengths = np.array(all_cumulative_lengths)\n",
        "\n",
        "    mean_tally = all_running_tallies.mean(axis=0)\n",
        "    subsample_variance = all_running_tallies.var(axis=0, ddof=1)\n",
        "\n",
        "    # Adjust variance using finite population correction (FPC) for each step\n",
        "    adjusted_variance = []\n",
        "    for i in range(subsample_size):\n",
        "        fpc = np.sqrt((original_sample_size - (i + 1)) / (original_sample_size - 1))\n",
        "        adjusted_variance.append(subsample_variance[i] / (fpc ** 2))\n",
        "\n",
        "    adjusted_variance = np.array(adjusted_variance)\n",
        "    ci_95_tally = 1.96 * np.sqrt(adjusted_variance)\n",
        "\n",
        "    mean_length = all_cumulative_lengths.mean(axis=0)\n",
        "    return mean_tally, ci_95_tally, mean_length\n",
        "\n",
        "# Assume that result_df contains your data with columns:\n",
        "#   - 'comment_indices': the full set of codes (as lists)\n",
        "#   - 'textlen': length in characters for each document\n",
        "#   - 'select2': indicator (1 for AI-selected, 0 otherwise)\n",
        "#   - 'Round': identifier for the round\n",
        "#\n",
        "# We now filter for Round 2, and further separate into AI-selected and randomly selected groups.\n",
        "round_2_df = result_df[result_df['Round'] == 2]\n",
        "round_2_select1_df = round_2_df[round_2_df['select2'] == 1]\n",
        "round_2_select0_df = round_2_df[round_2_df['select2'] == 0]\n",
        "\n",
        "# Determine subsample sizes as a proportion (e.g., 90%) of the group sizes\n",
        "subsample_size_select1 = int(len(round_2_select1_df) * 0.9)\n",
        "subsample_size_select0 = int(len(round_2_select0_df) * 0.9)\n",
        "\n",
        "# Use the full 'comment_indices' column (the entire set of codes) for the iterative approach.\n",
        "mean_tally_select1, ci_95_tally_select1, mean_length_select1 = subsample_and_aggregate(\n",
        "    round_2_select1_df, subsample_size_select1, 'comment_indices')\n",
        "mean_tally_select0, ci_95_tally_select0, mean_length_select0 = subsample_and_aggregate(\n",
        "    round_2_select0_df, subsample_size_select0, 'comment_indices')\n",
        "\n",
        "# Plot the aggregated iterative running tallies\n",
        "plt.figure()\n",
        "plt.plot(mean_length_select1, mean_tally_select1, marker='o', label='AI Selected')\n",
        "plt.fill_between(mean_length_select1,\n",
        "                 mean_tally_select1 - ci_95_tally_select1,\n",
        "                 mean_tally_select1 + ci_95_tally_select1,\n",
        "                 alpha=0.2)\n",
        "\n",
        "plt.plot(mean_length_select0, mean_tally_select0, marker='x', label='Randomly Selected')\n",
        "plt.fill_between(mean_length_select0,\n",
        "                 mean_tally_select0 - ci_95_tally_select0,\n",
        "                 mean_tally_select0 + ci_95_tally_select0,\n",
        "                 alpha=0.2)\n",
        "\n",
        "plt.xlabel('Mean Cumulative Corpus Length in characters')\n",
        "plt.ylabel('Mean Cumulative Unique High Frequency Codes')\n",
        "plt.title('Cumulative High Frequency Codes (Iterative Using Full Set)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfLX1jVs1u4d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_running_tally_and_corpus_length(df, code_column='comment_indices'):\n",
        "    \"\"\"\n",
        "    Iteratively computes the cumulative count of unique codes that have reached a frequency of 3 or more\n",
        "    (using only the documents encountered so far), along with the cumulative text length.\n",
        "\n",
        "    Parameters:\n",
        "      - df: DataFrame containing the data.\n",
        "      - code_column: Name of the column that contains the list of codes for each document.\n",
        "\n",
        "    Returns:\n",
        "      - running_tally: List of cumulative counts of high-frequency codes at each step.\n",
        "      - cumulative_textlen: List of cumulative text lengths (e.g., character counts) at each step.\n",
        "    \"\"\"\n",
        "    running_counter = Counter()\n",
        "    running_tally = []       # List to store cumulative count of codes seen >= 3 times.\n",
        "    cumulative_textlen = []  # List to store cumulative text length.\n",
        "    current_length = 0\n",
        "\n",
        "    for codes, textlen in zip(df[code_column], df['textlen']):\n",
        "        # Update the counter with the codes from the current document.\n",
        "        running_counter.update(codes)\n",
        "        current_length += textlen\n",
        "        # Count only those codes whose cumulative frequency is at least 3.\n",
        "        hf_count = sum(1 for count in running_counter.values() if count >= 3)\n",
        "        running_tally.append(hf_count)\n",
        "        cumulative_textlen.append(current_length)\n",
        "\n",
        "    return running_tally, cumulative_textlen\n",
        "\n",
        "def main():\n",
        "    # ----- Data Assumption -----\n",
        "    # We assume that result_df is already defined.\n",
        "    # For example, you might load it via:\n",
        "    # result_df = pd.read_csv('path_to_your_data.csv')\n",
        "    # And that the 'comment_indices' column contains lists (if stored as strings, you may need to convert them).\n",
        "\n",
        "    # For this script, we assume the DataFrame `result_df` already exists.\n",
        "    # If testing, you can create a dummy DataFrame here.\n",
        "    # ---------------------------\n",
        "\n",
        "    # Filter the DataFrame to the desired round (for example, Round == 2)\n",
        "    round_2_df = result_df[result_df['Round'] == 2].copy()\n",
        "\n",
        "    # Split the data into AI-selected (select2 == 1) and randomly selected (select2 == 0)\n",
        "    ai_selected_df = round_2_df[round_2_df['select2'] == 1].copy()\n",
        "    random_selected_df = round_2_df[round_2_df['select2'] == 0].copy()\n",
        "\n",
        "    # Calculate the iterative running tally and cumulative text length for each group.\n",
        "    ai_running_tally, ai_cum_textlen = calculate_running_tally_and_corpus_length(ai_selected_df, 'comment_indices')\n",
        "    random_running_tally, random_cum_textlen = calculate_running_tally_and_corpus_length(random_selected_df, 'comment_indices')\n",
        "\n",
        "    # Plot the resulting cumulative metrics.\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(ai_cum_textlen, ai_running_tally, marker='o', linestyle='-', label='AI Selected')\n",
        "    plt.plot(random_cum_textlen, random_running_tally, marker='x', linestyle='-', label='Randomly Selected')\n",
        "    plt.xlabel('Cumulative Corpus Length (characters)')\n",
        "    plt.ylabel('Cumulative Unique High-Frequency Codes (≥3 occurrences)')\n",
        "    plt.title('Cumulative Codes for random and AI-selected documents--Iterative Count')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE-zzi7WeCIM"
      },
      "outputs": [],
      "source": [
        "result_df2= result_df2.drop(columns=['level_0']).set_index('index').sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFyzYHlmex4Y"
      },
      "outputs": [],
      "source": [
        "result_df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_e7UfUzdsnt"
      },
      "outputs": [],
      "source": [
        "###unique values as more text is read, selected vs random\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "#result_df['AIcode_indices'] =result_df['AIcode_indices'].apply(ast.literal_eval)\n",
        "#result_df['comment_indices'] =result_df['comment_indices'].apply(ast.literal_eval)\n",
        "# Filter data for Round 2 and select2 == 1\n",
        "round_2_select1_df = result_df2[ result_df2['select2'] == 1]\n",
        "round_2_select1_df['Cumulative_Textlen'] = round_2_select1_df['textlen'].cumsum()\n",
        "\n",
        "# Calculate the running tally of unique values for select2 == 1\n",
        "unique_values_select1 = set()\n",
        "running_tally_select1 = []\n",
        "\n",
        "for indices in round_2_select1_df['theme_indices']:\n",
        "    unique_values_select1.update(indices)\n",
        "    running_tally_select1.append(len(unique_values_select1))\n",
        "\n",
        "# Add the running tally to the filtered DataFrame for select2 == 1\n",
        "round_2_select1_df['Running_Tally'] = running_tally_select1\n",
        "round_2_select1_df.reset_index(inplace=True)\n",
        "\n",
        "# Filter data for Round 2 and select2 == 0\n",
        "round_2_select0_df = result_df2[ result_df2['select2'] == 0]\n",
        "round_2_select0_df['Cumulative_Textlen'] = round_2_select0_df['textlen'].cumsum()\n",
        "\n",
        "# Calculate the running tally of unique values for select2 == 0\n",
        "unique_values_select0 = set()\n",
        "running_tally_select0 = []\n",
        "\n",
        "for indices in round_2_select0_df['theme_indices']:\n",
        "    unique_values_select0.update(indices)\n",
        "    running_tally_select0.append(len(unique_values_select0))\n",
        "\n",
        "# Add the running tally to the filtered DataFrame for select2 == 0\n",
        "round_2_select0_df['Running_Tally'] = running_tally_select0\n",
        "round_2_select0_df.reset_index(inplace=True)\n",
        "\n",
        "# Plot the running tally for both subsets\n",
        "plt.plot(round_2_select1_df['Cumulative_Textlen'], round_2_select1_df['Running_Tally'], marker='o', label='AI Selected')\n",
        "plt.plot(round_2_select0_df['Cumulative_Textlen'], round_2_select0_df['Running_Tally'], marker='x', label='Randomly Selected')\n",
        "\n",
        "plt.xlabel('Cumulative Corpus Length in characters')\n",
        "plt.ylabel('Cumulative Themes')\n",
        "plt.title('Cumulative Themes for random and AI-selected documents')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-c3r5ltgkZ2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to calculate running tally of unique values\n",
        "def calculate_running_tally_and_corpus_length(df):\n",
        "    unique_values = set()\n",
        "    running_tally = []\n",
        "    cumulative_textlen = df['textlen'].cumsum().tolist()\n",
        "    for indices in df['theme_indices']:\n",
        "        unique_values.update(indices)\n",
        "        running_tally.append(len(unique_values))\n",
        "    return running_tally, cumulative_textlen\n",
        "\n",
        "# Function to perform subsampling and aggregation with adjusted variance\n",
        "def subsample_and_aggregate(df, subsample_size, n_iterations=2000):\n",
        "    all_running_tallies = []\n",
        "    all_cumulative_lengths = []\n",
        "    original_sample_size = len(df)\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        subsample_df = df.sample(n=subsample_size, replace=False).reset_index(drop=True)\n",
        "        running_tally, cumulative_length = calculate_running_tally_and_corpus_length(subsample_df)\n",
        "        all_running_tallies.append(running_tally)\n",
        "        all_cumulative_lengths.append(cumulative_length)\n",
        "\n",
        "    all_running_tallies = np.array(all_running_tallies)\n",
        "    all_cumulative_lengths = np.array(all_cumulative_lengths)\n",
        "\n",
        "    mean_tally = all_running_tallies.mean(axis=0)\n",
        "    subsample_variance = all_running_tallies.var(axis=0, ddof=1)\n",
        "\n",
        "    # Adjust variance using finite population correction (FPC) for each step\n",
        "    adjusted_variance = []\n",
        "    for i in range(subsample_size):\n",
        "        fpc = np.sqrt((original_sample_size - (i + 1)) / (original_sample_size - 1))\n",
        "        adjusted_variance.append(subsample_variance[i] / (fpc ** 2))\n",
        "\n",
        "    adjusted_variance = np.array(adjusted_variance)\n",
        "    ci_95_tally = 1.96 * np.sqrt(adjusted_variance)\n",
        "\n",
        "    mean_length = all_cumulative_lengths.mean(axis=0)\n",
        "    return mean_tally, ci_95_tally, mean_length\n",
        "\n",
        "# Filter data for Round 2\n",
        "round_2_df = result_df2\n",
        "\n",
        "# Split data into AI-selected and randomly selected\n",
        "round_2_select1_df = round_2_df[round_2_df['select2'] == 1]\n",
        "round_2_select0_df = round_2_df[round_2_df['select2'] == 0]\n",
        "\n",
        "# Determine subsample size as a proportion of the original sample size\n",
        "subsample_size_select1 = int(len(round_2_select1_df) * 0.9)  # Example: 90% of the original size\n",
        "subsample_size_select0 = int(len(round_2_select0_df) * 0.9)\n",
        "\n",
        "# Perform subsampling and aggregation\n",
        "mean_tally_select1, ci_95_tally_select1, mean_length_select1 = subsample_and_aggregate(round_2_select1_df, subsample_size_select1)\n",
        "mean_tally_select0, ci_95_tally_select0, mean_length_select0 = subsample_and_aggregate(round_2_select0_df, subsample_size_select0)\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(mean_length_select1, mean_tally_select1, marker='o', label='AI Selected')\n",
        "plt.fill_between(mean_length_select1, mean_tally_select1 - ci_95_tally_select1, mean_tally_select1 + ci_95_tally_select1, alpha=0.2)\n",
        "\n",
        "plt.plot(mean_length_select0, mean_tally_select0, marker='x', label='Randomly Selected')\n",
        "plt.fill_between(mean_length_select0, mean_tally_select0 - ci_95_tally_select0, mean_tally_select0 + ci_95_tally_select0, alpha=0.2)\n",
        "\n",
        "plt.xlabel('Mean Cumulative Corpus Length in characters')\n",
        "plt.ylabel('Mean Cumulative Unique Themes')\n",
        "plt.title('Cumulative Themes for random and AI-selected documents with 95% CIs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBzJLfQSaMAW"
      },
      "outputs": [],
      "source": [
        "result_df=result_df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xnfeiAsaTeO"
      },
      "outputs": [],
      "source": [
        "result_df['indsq']=result_df['index']**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXS1Zntbaxvo"
      },
      "outputs": [],
      "source": [
        "result_df['indxselect']=result_df['index']*result_df['select2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFhYXbGIbEaC"
      },
      "outputs": [],
      "source": [
        "result_df['logcdense']=np.log(result_df['codedensity']*10000+1)-np.log(10001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb1v7ZkzTaVY"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Se2beeTmEO"
      },
      "outputs": [],
      "source": [
        "result_df['AIcodedensity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbdSqHwMXBTb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming result_df is already defined and loaded\n",
        "\n",
        "# Filter the DataFrame\n",
        "filtered_df = result_df[(result_df['rand'] == 1)]\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(filtered_df['textlen'], filtered_df['codedensity'], alpha=0.5)\n",
        "plt.title('Scatter Plot of Text Length vs Fecundity')\n",
        "plt.xlabel('Text Length')\n",
        "plt.ylabel(' Fecundity')\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOtWdZMGlYtt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "filtered_df = result_df[(result_df['rand']==1)]\n",
        "\n",
        "# Define the independent variables\n",
        "X = filtered_df[['AIcodedensity','textlen']]\n",
        "y = filtered_df['codedensity']\n",
        "\n",
        "# Add a constant to the independent variables\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Run the linear regression\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Report the coefficients\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdvW1iGdQ1Ox"
      },
      "outputs": [],
      "source": [
        "result_df[result_df['random2'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlIWaq--QE70"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# Subset the data\n",
        "select2_subset = result_df[result_df['select2'] == 1]['textlen']\n",
        "random2_subset = result_df[result_df['rand'] == 1]['textlen']\n",
        "\n",
        "# Create overlapping histograms\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.hist(select2_subset, bins=10, alpha=0.5, label='AI-selected')\n",
        "plt.hist(random2_subset, bins=23, alpha=0.5, label='Randomly-selected')\n",
        "\n",
        "plt.xlabel('Text Length (characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Text Length for AI-selected and Random Documents')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4M-I0Fgbu7c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "filtered_df = result_df[(result_df['rand'] == 1)]\n",
        "\n",
        "# Define the independent variables\n",
        "X = filtered_df[['AIcodedensity']]\n",
        "y = filtered_df['codedensity']\n",
        "\n",
        "# Add a constant to the independent variables\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Run the linear regression\n",
        "model4 = sm.OLS(y, X).fit()\n",
        "\n",
        "# Report the coefficients\n",
        "print(model4.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jukVBzJ7bs_E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "filtered_df = result_df[(result_df['Round']==2)]\n",
        "\n",
        "# Define the independent variables\n",
        "X = filtered_df[['select2','index','indsq']]\n",
        "y = filtered_df['codedensity']\n",
        "\n",
        "# Add a constant to the independent variables\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Run the linear regression\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Report the coefficients\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oaF7WD8V7v5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "filtered_df = result_df[(result_df['rand'] == 1) | (result_df['select2'] == 1)]\n",
        "\n",
        "# Define the independent variables\n",
        "X = filtered_df[['select2', 'Round','index','indsq']]\n",
        "y = filtered_df['codedensity']\n",
        "\n",
        "# Add a constant to the independent variables\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Run the linear regression\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Report the coefficients\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC6exHmVGi6P"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import HTML\n",
        "import statsmodels.api as sm\n",
        "from stargazer.stargazer import Stargazer\n",
        "# Create a stargazer object\n",
        "stargazer = Stargazer([second_stage_model])\n",
        "stargazer.significant_digits(3)  # Adjust the number to set the number of decimal places\n",
        "\n",
        "#stargazer.covariate_order(['const', 'select2','Round','index','indsq'])\n",
        "# Render the table in HTML format\n",
        "html_output = stargazer.render_html()\n",
        "# Display the HTML table in Colab\n",
        "HTML(html_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsA8r5SJNyES"
      },
      "outputs": [],
      "source": [
        "#Effect of textlen on codedensity independent of AIcodedensity\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "filtered_df = result_df[(result_df['rand'] == 1) | (result_df['select2'] == 1)]\n",
        "# Assuming you have a DataFrame `filtered_df` with columns 'codedensity', 'textlen', 'AIcodedensity', and 'select2'\n",
        "\n",
        "# Step 1: Regress textlen on AIcodedensity to get the residuals\n",
        "X = filtered_df[['AIcodedensity']]\n",
        "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
        "y = filtered_df['textlen']/1000\n",
        "\n",
        "first_stage_model = sm.OLS(y, X).fit()\n",
        "filtered_df['textlen_residuals'] = first_stage_model.resid\n",
        "\n",
        "# Step 2: Use the residuals in the main regression\n",
        "X_main = filtered_df[['textlen_residuals', 'select2','index','indsq', 'Round']]\n",
        "X_main = sm.add_constant(X_main)  # Adds a constant term to the predictor\n",
        "y_main = filtered_df['codedensity']\n",
        "\n",
        "second_stage_model = sm.OLS(y_main, X_main).fit()\n",
        "\n",
        "# Summary of the Second Stage Regression\n",
        "print(second_stage_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--81o5e5NyES"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming result_df is your DataFrame\n",
        "# Example: import pandas as pd\n",
        "# result_df = pd.DataFrame({'AIcodedensity': [1, 2, 3], 'codedensity': [4, 5, 6]})\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(result_df['AIcodedensity'], result_df['codedensity'], alpha=0.7, edgecolors='w', s=100)\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Scatter Plot of AIcodedensity vs Codedensity')\n",
        "plt.xlabel('AIcodedensity')\n",
        "plt.ylabel('Codedensity')\n",
        "\n",
        "# Show grid\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAOpRTthNyES"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df_articles and df_comment_indices are already defined\n",
        "\n",
        "\n",
        "# Step 2: Count the number of instances for each unique index2\n",
        "df_articles['cluster'] = df_articles['cluster'].apply(lambda x: eval(x))\n",
        "\n",
        "all_indices = [index for sublist in df_articles['cluster'] for index in sublist]\n",
        "comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "# Step 3: Sum the number of comments for each article, weighted by 1/n, using index2\n",
        "weighted_comment_sums = []\n",
        "for indices in df_articles['cluster']:\n",
        "    weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "    weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "# Add the weighted sums to the DataFrame\n",
        "df_articles['weighted_comment_sumAI'] = weighted_comment_sums\n",
        "\n",
        "# Display the updated DataFrame\n",
        "df_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zJiORBFNyET"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure the necessary columns are present in df_articles\n",
        "required_columns = ['AIcodedensity', 'select2']\n",
        "for column in required_columns:\n",
        "    if column not in result_df.columns:\n",
        "        raise ValueError(f\"The '{column}' column is not present in df_articles\")\n",
        "\n",
        "# Scatter plot of unique_comment_rate by select2\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(result_df['select2'], result_df['AIcodedensity'], alpha=0.5)\n",
        "plt.title('Scatter Plot of Unique Comment Rate by Select2')\n",
        "plt.xlabel('Select2')\n",
        "plt.ylabel('Unique Comment Rate')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ELYP1Z_NyET"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Calculate the mean and 95% CI for result_df['comment_per_1000_chars'] where result_df['select2'] = 1\n",
        "mean_1 = result_df[result_df['select2'] == 1]['comment_per_1000_chars'].mean()\n",
        "ci_1 = stats.t.interval(0.95, len(result_df[result_df['select2'] == 1]) - 1, loc=mean_1, scale=stats.sem(result_df[result_df['select2'] == 1]['comment_per_1000_chars']))\n",
        "\n",
        "# Calculate the mean and 95% CI for result_df['comment_per_1000_chars'] where result_df['select2'] = 0\n",
        "mean_0 = result_df[result_df['select2'] == 0]['comment_per_1000_chars'].mean()\n",
        "ci_0 = stats.t.interval(0.95, len(result_df[result_df['select2'] == 0]) - 1, loc=mean_0, scale=stats.sem(result_df[result_df['select2'] == 0]['comment_per_1000_chars']))\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean (select2 = 1): {mean_1:.4f}, 95% CI: {ci_1}\")\n",
        "print(f\"Mean (select2 = 0): {mean_0:.4f}, 95% CI: {ci_0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMs4sa-uNyEU"
      },
      "outputs": [],
      "source": [
        "results1=pd.read_csv('/content/drive/MyDrive/codes2vscoders.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-Ew6iiGNyEU"
      },
      "outputs": [],
      "source": [
        "fulllist['results1'] = fulllist['title'].isin(results1['title']).astype(int)\n",
        "\n",
        "# Add a column 'result_df' with value 1 if there's a match on the 'title' column between fulllist and result_df\n",
        "fulllist['result_df'] = fulllist['title'].isin(result_df['title']).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Keo1FG_NyEV"
      },
      "outputs": [],
      "source": [
        "subset =fulllist[(fulllist['result_df'] == 0) & (fulllist['select2'] == 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IfFjBbmNyEV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming results1 is your DataFrame\n",
        "results1['AIcodedensity'] = (results1['weighted_comment_sumAI'] / results1['textlen']) * 1000\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G4Em7_B8Ok9"
      },
      "source": [
        "Superset generation for Figure 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIp8vkKewpAW"
      },
      "outputs": [],
      "source": [
        "from apricot import FeatureBasedSelection\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def convert_to_incidence_matrix(grouped_df, cluster_column):\n",
        "    # Identify all unique clusters across all articles\n",
        "    unique_clusters = set(cluster for clusters in grouped_df[cluster_column] for cluster in clusters)\n",
        "\n",
        "    # Create a binary incidence matrix\n",
        "    incidence_matrix = pd.DataFrame(0, index=grouped_df.index, columns=sorted(unique_clusters))\n",
        "\n",
        "    # Fill the incidence matrix with 1's where the article has the cluster\n",
        "    for index, row in grouped_df.iterrows():\n",
        "        incidence_matrix.loc[index, row[cluster_column]] = 1\n",
        "\n",
        "    return incidence_matrix\n",
        "\n",
        "def select_articles(grouped_df, n_values, k):\n",
        "    all_results = {}\n",
        "\n",
        "    all_article_lengths = grouped_df['fulltext'].apply(len).values  # Calculate all article lengths once\n",
        "\n",
        "    for n in n_values:\n",
        "        results = []\n",
        "\n",
        "        for _ in range(k):\n",
        "            # Draw a random sample of n articles without replacement from grouped_df\n",
        "            sampled_df = grouped_df.sample(n=n, replace=False)\n",
        "\n",
        "            # Convert to incidence matrix\n",
        "            incidence_matrix = convert_to_incidence_matrix(sampled_df, 'codingd')\n",
        "\n",
        "            # Calculate the length of each article\n",
        "            article_lengths = sampled_df['fulltext'].apply(len).values\n",
        "\n",
        "            # Normalize the sample costs so that the mean is approximately 1\n",
        "            normalized_costs = article_lengths / all_article_lengths.mean()\n",
        "\n",
        "            # Initialize the FeatureBasedSelection object with the 'sqrt' concave function\n",
        "            selector = FeatureBasedSelection(n_samples=20, concave_func='sqrt', verbose=True)\n",
        "\n",
        "            # Fit the model on the binary incidence matrix to find the subset that maximizes the submodular function\n",
        "            # using the normalized costs as a knapsack constraint\n",
        "            selector.fit(incidence_matrix.values, sample_cost=normalized_costs)\n",
        "\n",
        "            # Extract the subset using the transform method\n",
        "            selected_features = selector.transform(incidence_matrix.values)\n",
        "\n",
        "            # Get the indices of the selected articles\n",
        "            selected_indices = selector.ranking\n",
        "\n",
        "            # Use the indices to extract the corresponding rows from the original DataFrame\n",
        "            selected_articles = sampled_df.iloc[selected_indices]\n",
        "\n",
        "            # Step 1: Create an index mapping for comments\n",
        "            comment_index_mapping = {}\n",
        "            index_counter = 0\n",
        "            for comment_texts in selected_articles['codingd']:\n",
        "                for text in comment_texts:\n",
        "                    if text not in comment_index_mapping:\n",
        "                        comment_index_mapping[text] = index_counter\n",
        "                        index_counter += 1\n",
        "\n",
        "            selected_articles['AIcode_indices'] = selected_articles['codingd'].apply(lambda texts: [comment_index_mapping[text] for text in texts])\n",
        "\n",
        "            # Step 2: Count the number of instances for each unique index\n",
        "            all_indices = [index for sublist in selected_articles['AIcode_indices'] for index in sublist]\n",
        "            comment_counts = pd.Series(all_indices).value_counts().sort_index()\n",
        "\n",
        "            # Step 3: Sum the number of comments for each article, weighted by 1/n, using index\n",
        "            weighted_comment_sums = []\n",
        "            for indices in selected_articles['AIcode_indices']:\n",
        "                weighted_sum = sum(1 / comment_counts[index] for index in indices)\n",
        "                weighted_comment_sums.append(weighted_sum)\n",
        "\n",
        "            # Add the weighted sums to the DataFrame\n",
        "            selected_articles['AIcodesum'] = weighted_comment_sums\n",
        "            selected_articles['AIcodedensity'] = selected_articles['AIcodesum'] / selected_articles['textlen'] * 1000\n",
        "\n",
        "            # Step 2: Calculate the sum of AIcodesum and textlen\n",
        "            sum_weighted_comment_sum = selected_articles['AIcodesum'].sum()\n",
        "            sum_textlen = selected_articles['textlen'].sum()\n",
        "\n",
        "            # Step 3: Divide the sum of AIcodesum by the sum of textlen\n",
        "            result = sum_weighted_comment_sum / sum_textlen * 1000\n",
        "\n",
        "            # Store the result for this iteration\n",
        "            results.append(result)\n",
        "\n",
        "        # Calculate the mean result over all iterations for the current n\n",
        "        mean_result = np.mean(results)\n",
        "\n",
        "        # Store the mean result for the current n\n",
        "        all_results[n] = mean_result\n",
        "\n",
        "    # Output the mean results for all n values\n",
        "    for n, mean_result in all_results.items():\n",
        "        print(f\"Mean Result for n={n}: {mean_result}\")\n",
        "\n",
        "# Parameters\n",
        "n_values = [50, 100, 250, 500, 1000]  # List of different n values\n",
        "k = 10  # Number of iterations\n",
        "\n",
        "# Assuming grouped_df is already defined\n",
        "select_articles(grouped_df, n_values, k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8si5H4Ey3Xx"
      },
      "outputs": [],
      "source": [
        "result_df['AIsq']=result_df['AIcodedensity']**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QeWzQWY8Boq"
      },
      "source": [
        "Figure 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTRESWnT2bux"
      },
      "outputs": [],
      "source": [
        "result_df.loc[result_df['random2']==1,'AIcodedensity'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1JlH7DJ2BIZ"
      },
      "outputs": [],
      "source": [
        "result_df.loc[result_df['random']==1,'AIcodedensity'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzQ6_B0tyyzp"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'result_df' has 'textlen' and 'random' columns\n",
        "\n",
        "# Including the index and its quadratic term, as well as 'weighted_sum', as independent variables\n",
        "\n",
        "X = result_df[['AIcodedensity',\"AIsq\"]]\n",
        "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
        "\n",
        "y = result_df['codedensity']  # Dependent variable\n",
        "\n",
        "model = sm.OLS(y, X).fit()  # OLS regression\n",
        "print(model.summary())  # Prints a summary of the regression results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDsZldsAxPGl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Given AIcodedensity values and sample sizes\n",
        "data = [\n",
        "    (2530, 5.631718406012233),\n",
        "    (1000, 5.426492850326782),\n",
        "    (500, 5.1972600604093255),\n",
        "    (250, 4.9214512232780425),\n",
        "    (100, 4.316129003704333),\n",
        "    (50, 3.576394769507362),\n",
        "    (20, 1.9364524673889225)  # Random sample size set to 20\n",
        "]\n",
        "\n",
        "# Regression coefficients\n",
        "const = -0.0698\n",
        "AIcodedensity_coeff = 0.6911\n",
        "AIdens2_coeff = -0.0323\n",
        "\n",
        "# Function to predict codedensity\n",
        "def predict_codedensity(AIcodedensity):\n",
        "    return const + AIcodedensity_coeff * AIcodedensity + AIdens2_coeff * (AIcodedensity ** 2)\n",
        "\n",
        "# Calculate predicted codedensity values\n",
        "predicted_codedensity = [predict_codedensity(AIcodedensity) for _, AIcodedensity in data]\n",
        "\n",
        "# Normalize the predicted codedensity values so that the \"Random\" value (at sample size 20) is 100%\n",
        "random_codedensity = predicted_codedensity[-1]\n",
        "normalized_codedensity = [(value / random_codedensity) * 100 for value in predicted_codedensity]\n",
        "\n",
        "# Extract sample sizes\n",
        "sample_sizes = [sample for sample, _ in data]\n",
        "\n",
        "# Reverse the order of data for plotting\n",
        "sample_sizes = sample_sizes[::-1]\n",
        "normalized_codedensity = normalized_codedensity[::-1]\n",
        "\n",
        "# Plot normalized predicted codedensity against sample size\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(sample_sizes, normalized_codedensity, marker='o', label='Normalized Predicted codedensity', color='r')\n",
        "plt.xscale('log')\n",
        "plt.xticks([20, 50, 100, 250, 500, 1000, 2530], ['20 (Random)', '50', '100', '250', '500', '1000', '2530'])\n",
        "plt.xlabel('Sample Size')\n",
        "plt.ylabel('Normalized codedensity (%)')\n",
        "plt.title('Normalized Estimated Fecundity vs Sample Size (Log Scale)')\n",
        "plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDUTFpcL3Qdp"
      },
      "source": [
        "# Comment Locations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX0W2OhA6_Jj"
      },
      "source": [
        "Figure 2 and Table 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fiks7kSNkGwl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = {}\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        try:\n",
        "            with docx.open('word/comments.xml') as comments_xml:\n",
        "                tree = ET.parse(comments_xml)\n",
        "                root = tree.getroot()\n",
        "                for comment in root.findall('.//w:comment', ns):\n",
        "                    comment_id = comment.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                    comment_text = ''.join(node.text for node in comment.iter() if node.tag.endswith('}t') and node.text)\n",
        "                    comments[comment_id] = comment_text\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return comments\n",
        "\n",
        "def parse_docx_comments(file_path):\n",
        "    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    comments = parse_comments(file_path)\n",
        "    articles = []\n",
        "    current_article = None\n",
        "    current_text = []\n",
        "    blank_line_found = False\n",
        "    title_next = False\n",
        "    char_count = 0\n",
        "\n",
        "    with zipfile.ZipFile(file_path, 'r') as docx:\n",
        "        with docx.open('word/document.xml') as document_xml:\n",
        "            tree = ET.parse(document_xml)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            for child in root.iter():\n",
        "                if child.tag == '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}p':\n",
        "                    paragraph_start = char_count\n",
        "                    text = ''.join(node.text for node in child.iter() if node.tag.endswith('}t') and node.text)\n",
        "\n",
        "                    if text.strip() == \"\":\n",
        "                        blank_line_found = True\n",
        "                    elif text.startswith(\"###\") and text.endswith(\"###\") and blank_line_found:\n",
        "                        if current_article is not None:\n",
        "                            current_article['content'] = ' '.join(current_text).strip()\n",
        "                            articles.append(current_article)\n",
        "                        current_article = {'title': None, 'content': '', 'comments': [], 'comment_positions': [], 'length': 0}\n",
        "                        current_text = []\n",
        "                        title_next = True\n",
        "                        blank_line_found = False\n",
        "                        char_count = 0  # Reset char count for new article\n",
        "                    elif title_next:\n",
        "                        current_article['title'] = text.strip()\n",
        "                        current_article['length'] += len(text)  # Include title length\n",
        "                        title_next = False\n",
        "                    else:\n",
        "                        if text.startswith(\"###\") and text.endswith(\"###\"):\n",
        "                            text = text.strip(\"#\").strip()\n",
        "                        current_text.append(text)\n",
        "                        current_article['length'] += len(text)  # Update article length\n",
        "                        for c in child.iter():\n",
        "                            if c.tag.endswith('commentRangeStart'):\n",
        "                                comment_id = c.attrib['{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id']\n",
        "                                if comment_id in comments:\n",
        "                                    paragraph_end = char_count + len(text)\n",
        "                                    comment_position = (paragraph_start + paragraph_end) / 2  # Mean position\n",
        "                                    current_article['comments'].append(comments[comment_id])\n",
        "                                    current_article['comment_positions'].append(comment_position)\n",
        "                        for run in child.findall('.//w:t', ns):\n",
        "                            char_count += len(run.text or '')\n",
        "\n",
        "            if current_article is not None:\n",
        "                current_article['content'] = ' '.join(current_text).strip()\n",
        "                articles.append(current_article)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Assuming df_articles is already an existing DataFrame\n",
        "# df_articles = pd.DataFrame(...)  # Your existing DataFrame initialization\n",
        "\n",
        "# List of docx files to process\n",
        "docx_files = ['Second Round Articles - Zahra.docx', 'Second Round Articles - Sharon.docx', 'Second Round Articles - Aimi.docx']\n",
        "base_path = '/content/drive/MyDrive/'\n",
        "\n",
        "# Dictionary to aggregate articles data, tracking lengths only once per title\n",
        "articles_agg = defaultdict(lambda: {'comment_positions': [], 'length': 0})\n",
        "processed_titles = set()\n",
        "\n",
        "for file_name in docx_files:\n",
        "    file_path = base_path + file_name\n",
        "    articles_data = parse_docx_comments(file_path)\n",
        "    for article in articles_data:\n",
        "        if article['title'] not in processed_titles:\n",
        "            articles_agg[article['title']]['length'] = article['length']\n",
        "            processed_titles.add(article['title'])\n",
        "        articles_agg[article['title']]['comment_positions'] += article['comment_positions']\n",
        "\n",
        "# Convert the aggregated data into a list of dictionaries, then append to existing DataFrame\n",
        "articles_list = [{'title': title, 'comment_positions': data['comment_positions'], 'length': data['length'], 'Round': 2} for title, data in articles_agg.items()]\n",
        "df_new_articles = pd.DataFrame(articles_list)\n",
        "df_articles = pd.concat([df_articles, df_new_articles], ignore_index=True)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df_articles)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDp_77fzLh9E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Calculate the 75th percentile of comment_positions divided by length\n",
        "df_articles['upper_quartile'] = df_articles['comment_positions'].apply(lambda x: pd.Series(x).quantile(0.5)) / df_articles['length']\n",
        "# Assuming df_articles already has the 'upper_quartile' column\n",
        "\n",
        "# Drop rows where upper_quartile is NaN\n",
        "df_articles_cleaned = df_articles.dropna(subset=['upper_quartile'])\n",
        "\n",
        "# Sort the DataFrame by length to ensure the moving average is calculated correctly\n",
        "df_articles_sorted = df_articles_cleaned.sort_values('length')\n",
        "\n",
        "# Calculate the moving average of upper_quartile with a window size of 2\n",
        "df_articles_sorted['moving_average'] = df_articles_sorted['upper_quartile'].rolling(window=5).mean()\n",
        "\n",
        "# Scatter plot of upper quartile against length\n",
        "plt.scatter(df_articles_sorted['length'], df_articles_sorted['upper_quartile'])\n",
        "\n",
        "# Plot the moving average\n",
        "plt.plot(df_articles_sorted['length'], df_articles_sorted['moving_average'], color='red', label='Moving Average (Mean of 5 Values)')\n",
        "\n",
        "plt.xlabel('Length (Characters)')\n",
        "plt.ylabel('Median comment position / length')\n",
        "plt.title('Scatter Plot of Median Comment Positions with Moving Average')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqCnU0eWoUDB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Helper function to calculate summary statistics\n",
        "def calculate_summary(data):\n",
        "    mean = data.mean()\n",
        "    ci = stats.t.interval(0.95, len(data)-1, loc=mean, scale=stats.sem(data))\n",
        "    n = len(data)\n",
        "    percentile_25 = data.quantile(0.25)\n",
        "    percentile_75 = data.quantile(0.75)\n",
        "\n",
        "    return {\n",
        "        'Mean': mean,\n",
        "        'N': n,\n",
        "        '95% CI Lower': ci[0],\n",
        "        '95% CI Upper': ci[1],\n",
        "        '25th Percentile': percentile_25,\n",
        "        '75th Percentile': percentile_75\n",
        "    }\n",
        "\n",
        "# Calculate summary statistics for each subset\n",
        "summary_data = {\n",
        "    'Round 1 - Randomly Selected (random)': result_df[result_df['random'] == 1][['AIcodedensity', 'textlen']].apply(calculate_summary).T,\n",
        "    'Round 1 - AI Selected (null)': result_df[(result_df['random'] == 0) &\n",
        "                                              (result_df['random2'] == 0) &\n",
        "                                              (result_df['select2'] == 0)][['AIcodedensity', 'textlen']].apply(calculate_summary).T,\n",
        "    'Round 2 - Randomly Selected (random2)': result_df[result_df['random2'] == 1][['AIcodedensity', 'textlen']].apply(calculate_summary).T,\n",
        "    'Round 2 - AI Selected (select2)': result_df[result_df['select2'] == 1][['AIcodedensity', 'textlen']].apply(calculate_summary).T\n",
        "}\n",
        "\n",
        "# Convert the summary data to a DataFrame\n",
        "summary_df = pd.concat(summary_data, axis=1)\n",
        "\n",
        "# Use tabulate to create a publication-quality table\n",
        "formatted_table = tabulate(summary_df, headers='keys', tablefmt='grid', floatfmt=\".3f\")\n",
        "\n",
        "# Display the formatted table\n",
        "print(formatted_table)\n",
        "\n",
        "# Optionally, save it to a text file\n",
        "with open('summary_statistics.txt', 'w') as f:\n",
        "    f.write(formatted_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Yw4Y-Noq36D"
      },
      "outputs": [],
      "source": [
        "# Function to format numbers according to the requirement\n",
        "def format_number(num):\n",
        "    if num > 999:\n",
        "        return f\"{int(num):,}\"\n",
        "    elif isinstance(num, int):\n",
        "        return f\"{num}\"\n",
        "    else:\n",
        "        return f\"{num:.2f}\"\n",
        "\n",
        "# Initialize a list to store the summary rows\n",
        "summary_rows = []\n",
        "\n",
        "# Define the conditions and labels, including the full dataset\n",
        "conditions = {\n",
        "    'Round 1 - Randomly Selected': result_df[result_df['random'] == 1],\n",
        "    'Round 1 - AI Selected': result_df[(result_df['random'] == 0) &\n",
        "                                       (result_df['random2'] == 0) &\n",
        "                                       (result_df['select2'] == 0)],\n",
        "    'Round 2 - Randomly Selected': result_df[result_df['random2'] == 1],\n",
        "    'Round 2 - AI Selected': result_df[result_df['select2'] == 1],\n",
        "    'Full Dataset': grouped_df  # Add full dataset\n",
        "}\n",
        "\n",
        "# Calculate summary statistics for each condition and variable\n",
        "for label, df in conditions.items():\n",
        "    for variable in ['AIcodedensity', 'textlen'] if label != 'Full Dataset' else ['textlen']:\n",
        "        summary = calculate_summary(df[variable])\n",
        "        summary_rows.append([\n",
        "            label,\n",
        "            variable,\n",
        "            format_number(summary['Mean']),\n",
        "            format_number(summary['N']),  # Ensure N is an integer\n",
        "            format_number(summary['95% CI Lower']),\n",
        "            format_number(summary['95% CI Upper']),\n",
        "            format_number(summary['25th Percentile']),\n",
        "            format_number(summary['75th Percentile'])\n",
        "        ])\n",
        "\n",
        "# Create the HTML table from the summary rows\n",
        "html_table = \"\"\"\n",
        "<table border=\"1\" cellpadding=\"5\" cellspacing=\"0\" style=\"border-collapse: collapse; width: 100%;\">\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th>Condition</th>\n",
        "            <th>Variable</th>\n",
        "            <th>Mean</th>\n",
        "            <th>N</th>\n",
        "            <th>95% CI Lower</th>\n",
        "            <th>95% CI Upper</th>\n",
        "            <th>25th Percentile</th>\n",
        "            <th>75th Percentile</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "\"\"\"\n",
        "\n",
        "# Populate the table with rows\n",
        "for row in summary_rows:\n",
        "    html_table += f\"\"\"\n",
        "    <tr>\n",
        "        <td>{row[0]}</td>\n",
        "        <td>{row[1]}</td>\n",
        "        <td>{row[2]}</td>\n",
        "        <td>{row[3]}</td>\n",
        "        <td>{row[4]}</td>\n",
        "        <td>{row[5]}</td>\n",
        "        <td>{row[6]}</td>\n",
        "        <td>{row[7]}</td>\n",
        "    </tr>\n",
        "    \"\"\"\n",
        "\n",
        "html_table += \"\"\"\n",
        "    </tbody>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "# Display the HTML table\n",
        "display(HTML(html_table))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9EYp59_etTeQ",
        "45KtgiA-RkzO",
        "4Rb4Ajx4UFqp",
        "78ejUsS4Isz_",
        "CX0U9h1EtsYZ",
        "Cz1I9PMeBpRC",
        "objcz_vmnrEr",
        "MGEx7NZv8jAW",
        "L_718_LLhH_9",
        "xvBbWgsC3-ig",
        "MYbW8ejt_HXw",
        "58Hq7UqU1u2Q",
        "GnIUHdCNq-xF"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
